{"id": "9992c43b8c42caa65a4c20ca9aff12d5cb01959c", "text": "Hello\n\nApples and Bananas\n\nRead this first\n\nGreek Yogurt", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/ambiguous.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 1, "total-input-tokens": 1752, "total-output-tokens": 46, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 56, 1]], "primary_language": ["en"], "is_rotation_valid": [true], "rotation_correction": [0], "is_table": [false], "is_diagram": [true]}}
{"id": "2d485d1b2c5a4813e5f536ada63dbd6c7c468d2e", "text": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs\n\nAshesh Jain\\textsuperscript{1,2}, Amir R. Zamir\\textsuperscript{2}, Silvio Savarese\\textsuperscript{2}, and Ashutosh Saxena\\textsuperscript{3}\n\nCornell University\\textsuperscript{1}, Stanford University\\textsuperscript{2}, Brain Of Things Inc.\\textsuperscript{3}\nashesh@cs.cornell.edu, \\{zamir,ssilvio,asaxena\\}@cs.stanford.edu\n\nAbstract\n\nDeep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks (RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.\n\nLinks: \ud83c\udf10Web\n\n1. Introduction\n\nThe world we live in is inherently structured. It is comprised of components that interact with each other in space and time, leading to a spatio-temporal composition. Utilizing such structures in problem formulation allows domain-experts to inject their high-level knowledge in learning frameworks. This has been the incentive for many efforts in computer vision and machine learning, such as Logic Networks [46], Graphical Models [28], and Structured SVMs [26]. Structures that span over both space and time (spatio-temporal) are of particular interest to computer vision and robotics communities. Primarily, interactions between humans and environment in real world are inherently spatio-temporal in nature. For example, during a cooking activity, humans interact with multiple objects both in space and through time. Similarly, parts of human body (arms, legs, etc.) have individual functions but work with each other in concert to generate physically sensible motions. Hence, bringing high-level spatio-temporal structures and rich sequence modeling capabilities together is of particular importance for many applications.\n\nThe notable success of RNNs has proven their capability on many end-to-end learning tasks [19, 14, 10, 66]. However, they lack a high-level and intuitive spatio-temporal structure though they have been shown to be successful at modeling long sequences [49, 43, 52]. Therefore, augmenting a high-level structure with learning capability of RNNs leads to a powerful tool that has the best of both worlds. Spatio-temporal graphs (st-graphs) are a popular [39, 37, 4, 11, 32, 65, 22] general tool for representing such high-level spatio-temporal structures. The nodes of the graph typically represent the problem components, and the edges capture their spatio-temporal interactions. To achieve\nthe above goal, we develop a generic tool for transforming an arbitrary st-graph into a feedforward mixture of RNNs, named structural-RNN (S-RNN). Figure 1 schematically illustrates this process, where a sample spatio-temporal problem is shown at the bottom, the corresponding st-graph representation is shown in the middle, and our RNN mixture counterpart of the st-graph is shown at the top.\n\nIn high-level steps, given an arbitrary st-graph, we first roll it out in time and decompose it into a set of contributing factor components. The factors identify the independent components that collectively determine one decision and are derived from both edges and nodes of the st-graph. We then semantically group the factor components and represent each group using one RNN, which results in the desired RNN mixture. The main challenges of this transformation problem are: 1) making the RNN mixture as rich as possible to enable learning complex functions, yet 2) keeping the RNN mixture scalable with respect to size of the input st-graph. In order to make the resulting RNN mixture rich, we liberally represent each spatio-temporal factor (including node factors, temporal edge factors, and spatio-temporal edge factors) using one RNN. On the other hand, to keep the overall mixture scalable but not lose the essential learning capacity, we utilize \u201cfactor sharing\u201d (aka clique templates [54, 42, 53]) and allow the factors with similar semantic functions to share an RNN. This results in a rich and scalable feedforward mixture of RNNs that is equivalent to the provided st-graph in terms of input, output, and spatio-temporal relationships. The mixture is also fully differentiable, and therefore, can be trained jointly as one entity.\n\nThe proposed method is principled and generic as the transformation is based on a set of well defined steps and it is applicable to any problem that can be formulated as st-graphs (as defined in Section 3). Several previous works have attempted solving specific problems using a collection of RNNs [49, 12, 61, 10, 5], but they are almost unanimously task-specific. They also do not utilize mechanisms similar to factorization or factor sharing in devising their architecture to ensure richness and scalability.\n\nS-RNN is also modular, as it is enjoying an underlying high-level structure. This enables easy high-level manipulations which are basically not possible in unstructured (plain-vanilla) RNNs (e.g., we will experimentally show forming a feasible hybrid human motion by mixing parts of different motion styles - Sec 4.2). We evaluate the proposed approach on a diverse set of spatio-temporal problems (human pose modeling and forecasting, human-object interaction, and driver decision making), and show significant improvements over the state of the art on each problem. We also study complexity and convergence properties of S-RNN and provide further experimental insights by visualizing its memory cells that reveals some cells interestingly represent certain semantic operations. The code of the entire framework that accepts a st-graph as the input and yields the output RNN mixture is available at the http://asheshjain.org/srnn.\n\nThe contribution of this paper are: 1) a generic method for casting an arbitrary st-graph as a rich, scalable, and jointly trainable RNN mixture, 2) in defence of structured approaches, we show S-RNN significantly outperforms its unstructured (plain-vanilla) RNN counterparts, 3) in defence of RNNs, we show on several diverse spatio-temporal problems that modeling structure with S-RNN outperforms the non-deep learning based structured counterparts.\n\n2. Related Work\n\nWe give a categorized overview of the related literature. In general, three main characteristics differentiate our work from existing techniques: being generic and not restricted to a specific problem, providing a principled method for transforming a st-graph into a scalable rich feedforward RNN mixture, and being jointly trainable.\n\nSpatio-temporal problems. Problems that require spatial and temporal reasoning are very common in robotics and computer vision. Examples include human activity recognition and segmentation from videos [50, 47, 62, 60, 8, 25, 37, 36], context-rich human-object interactions [39, 33, 29, 20, 30], modeling human motion [14, 57, 56] etc. Spatio-temporal reasoning also finds application in assistive robots, driver understanding, and object recognition [65, 22, 44, 23, 11]. In fact most of our daily activities are spatio-temporal in nature. With growing interests in rich interactions and robotics, this form of reasoning will become even more important. We evaluate our generic method on three context-rich spatio-temporal problems: (i) Human motion modeling [14]; (ii) Human-object interaction understanding [33]; and (iii) Driver maneuver anticipation [22].\n\nMixtures of deep architectures. Several previous works build multiple networks and wire them together in order to capture some complex structure (or interactions) in the problem with promising results on applications such as activity detection, scene labeling, image captioning, and object detection [12, 5, 9, 16, 49, 61]. However, such architectures are mostly hand designed for specific problems, though they demonstrate the benefit in using a modular deep architecture. Recursive Neural Networks [17] are, on the other hand, generic feedforward architectures, but for problems with recursive structure such as parsing of natural sentences and scenes [48]. Our work is a remedy for problems expressed as spatio-temporal graphs. For a new spatio-temporal problem in hand, all a practitioner needs to do is to express their intuition about the problem as an st-graph.\n\nDeep learning with graphical models. Many works have addressed deep networks with graphical models for structured prediction tasks. Bengio et al. [1] combined CNNs with HMM for hand writing recognition. Tompson et al. [58] jointly train CNN and MRF for human pose estimation. Chen et al. [7] use a similar approach for image classification with general MRF. Recently several works\nhave addressed end-to-end image segmentation with fully connected CRF [66, 41, 15, 40]. Several works follow a two-stage approach and decouple the deep network from CRF. They have been applied to multiple problems including image segmentation, pose estimation, document processing [64, 6, 38, 3] etc. All of these works advocate and well demonstrate the benefit in exploiting the structure in the problem together with rich deep architectures. However, they largely do not address spatio-temporal problems and the proposed architectures are task-specific.\n\n**Conditional Random Fields (CRF)** model dependencies between the outputs by learning a joint distribution over them. They have been applied to many applications [34, 13, 45] including st-graphs which are commonly modeled as spatio-temporal CRF [39, 33, 65, 11]. In our approach, we adopt st-graphs as a general graph representation and embody it using an RNN mixture architecture. Unlike CRF, our approach is not probabilistic and is not meant to model the joint distribution over the outputs. S-RNN instead learns the dependencies between the outputs via structural sharing of RNNs between the outputs.\n\n### 3. Structural-RNN architectures\n\nIn this section we describe our approach for building structural-RNN (S-RNN) architectures. We start with a st-graph, decompose it into a set of factor components, then represent each factor using an RNN. The RNNs are interconnected in a way that the resulting architecture captures the structure and interactions of the st-graph.\n\n#### 3.1. Representation of spatio-temporal graphs\n\nMany applications that require spatial and temporal reasoning are modeled using st-graphs [4, 11, 33, 65, 22]. We represent a st-graph with $G = (\\mathcal{V}, \\mathcal{E}_S, \\mathcal{E}_T)$, whose structure $(\\mathcal{V}, \\mathcal{E}_S)$ unrolls over time through edges $\\mathcal{E}_T$. Figure 2a shows an example st-graph capturing human-object interactions during an activity. The nodes $v \\in \\mathcal{V}$ and edges $e \\in \\mathcal{E}_S \\cup \\mathcal{E}_T$ of the st-graph repeats over time. In particular, Figure 2b shows the same st-graph unrolled through time. In the unrolled st-graph, the nodes at a given time step $t$ are connected with undirected *spatio-temporal* edge $e = (u, v) \\in \\mathcal{E}_S$, and the nodes at adjacent time steps (say the node $u$ at time $t$ and the node $v$ at time $t + 1$) are connected with undirected *temporal* edge iff $(u, v) \\in \\mathcal{E}_T$.\\(^1\\)\n\nGiven a st-graph and the feature vectors associated with the nodes $x^t_u$, and edges $x^t_e$, as shown in Figure 2b, the goal is to predict the node labels (or real value vectors) $y^t_v$ at each time step $t$. For instance, in human-object interaction, the node features can represent the human and object poses, and edge features can their relative orientation; the node labels represent the human activity and object affordance. Label $y^t_v$ is affected by both its node and its interactions with other nodes (edges), leading to an overall complex system. Such interactions are commonly parameterized with a factor graph that conveys how a (complicated) function over the st-graph factorizes into simpler functions [35]. We derive our S-RNN architecture from the factor graph representation of the st-graph. Our factor graph representation has a factor function $\\Psi_v(y_v, x_v)$ for each node and a pairwise factor $\\Psi_e(y_{e(1)}, y_{e(2)}, x_e)$ for each edge. Figure 2c shows the factor graph corresponding to the st-graph in 2a.\\(^2\\)\n\n#### Sharing factors between nodes\n\nEach factor in the st-graph has parameters that needs to be learned. Instead of learning a distinct factor for each node, semantically similar nodes can optionally share factors. For example, all \u201cobject nodes\u201d $\\{u, w\\}$ in the st-graph can share the same node factor and parameters. This modeling choice allows enforcing parameter sharing between similar nodes. It further gives the flexibility to handle st-graphs with more nodes without increasing the number of parameters. For this purpose, we partition the nodes as $\\mathcal{C}_V = \\{V_1, ..., V_P\\}$ where $V_p$ is a set of semantically similar nodes, and they all use the same node factor $\\Psi_{V_p}$. In Figure 3a we re-draw the st-graph and assign same color to the nodes sharing node factors.\n\nPartitioning nodes on their semantic meanings leads to a natural semantic partition of the edges, $\\mathcal{C}_E = \\{E_1, ..., E_M\\}$, where $E_m$ is a set of edges whose nodes form a semantic pair. Therefore, all edges in the set $E_m$ share the same edge factor $\\Psi_{E_m}$. For example all \u201chuman-object\u201d edges...\n\n---\n\n\\(^1\\)For simplicity, the example st-graph in Figure 2a considers temporal edges of the form $(v, v) \\in \\mathcal{E}_T$.\n\n\\(^2\\)Note that we adopted factor graph as a tool for capturing interactions and not modeling the overall function. Factor graphs are commonly used in probabilistic graphical models for factorizing joint probability distributions. We consider them for general st-graphs and do not establish relations to its probabilistic and function decomposition properties.\nedges\u201d \\{(v, u), (v, w)\\} are modeled with the same edge factor. Sharing factors based on semantic meaning makes the overall parametrization compact. In fact, sharing parameters is necessary to address applications where the number of nodes depends on the context. For example, in human-object interaction the number of object nodes vary with the environment. Therefore, without sharing parameters between the object nodes, the model cannot generalize to new environments with more objects. For modeling flexibility, the edge factors are not shared across the edges in \\(E_S\\) and \\(E_T\\). Hence, in Figure 3a, object-object \\((w, w) \\in E_T\\) temporal edge is colored differently from object-object \\((u, w) \\in E_S\\) spatio-temporal edge.\n\nIn order to predict the label of node \\(v \\in V_p\\), we consider its node factor \\(\\Psi_{V_p}\\), and the edge factors connected to \\(v\\) in the factor graph. We define a node factor and an edge factor as neighbors if they jointly affect the label of some node in the st-graph. More formally, the node factor \\(\\Psi_{V_p}\\) and edge factor \\(\\Psi_{E_m}\\) are neighbors, if there exist a node \\(v \\in V_p\\) such that it connects to both \\(\\Psi_{V_p}\\) and \\(\\Psi_{E_m}\\) in the factor graph. We will use this definition in building S-RNN architecture such that it captures the interactions in the st-graph.\n\n### 3.2. Structural-RNN from spatio-temporal graphs\n\nWe derive our S-RNN architecture from the factor graph representation of the st-graph. The factors in the st-graph operate in a temporal manner, where at each time step the factors observe (node & edge) features and perform some computation on those features. In S-RNN, we represent each factor with an RNN. We refer the RNNs obtained from the node factors as nodeRNNs and the RNNs obtained from the edge factors as edgeRNNs. The interactions represented by the st-graph are captured through connections between the nodeRNNs and the edgeRNNs.\n\nWe denote the RNNs corresponding to the node factor \\(\\Psi_{V_p}\\) and the edge factor \\(\\Psi_{E_m}\\) as \\(R_{V_p}\\) and \\(R_{E_m}\\) respectively. In order to obtain a feedforward network, we connect the edgeRNNs and nodeRNNs to form a bipartite graph \\(G_R = (\\{R_{E_m}\\}, \\{R_{V_p}\\}, E_R)\\). In particular, the edgeRNN \\(R_{E_m}\\) is connected to the nodeRNN \\(R_{V_p}\\) iff the factors \\(\\Psi_{E_m}\\) and \\(\\Psi_{V_p}\\) are neighbors in the st-graph, i.e. they jointly affect the label of some node in the st-graph. To summarize, in Algorithm 1 we show the steps for constructing S-RNN architecture. Figure 3b shows the S-RNN for the human activity represented in Figure 3a. The nodeRNNs combine the outputs of the edgeRNNs they are connected to (i.e. its neighbors in the factor graph), and predict the node labels. The predictions of nodeRNNs (eg. \\(R_{V_1}\\) and \\(R_{V_2}\\)) interact through the edgeRNNs (eg. \\(R_{E_1}\\)). Each edgeRNN handles a specific semantic interaction between the nodes connected in the st-graph and models how the interactions evolve over time. In the next section, we explain the inputs, outputs, and the training procedure of S-RNN.\n\n### 3.3. Training structural-RNN architecture\n\nIn order to train the S-RNN architecture, for each node of the st-graph the features associated with the node are fed into the architecture. In the forward-pass for node \\(v \\in V_p\\), the input into edgeRNN \\(R_{E_m}\\) is the temporal sequence of edge features \\(x_e^t\\) on the edge \\(e \\in E_m\\), where edge \\(e\\) is incident to node \\(v\\) in the st-graph. The nodeRNN \\(R_{V_p}\\) at each time step concatenates the node feature \\(x_v^t\\) and the outputs of edgeRNNs it is connected to, and predicts the node label. At the time of training, the errors in prediction are back-propagated through the nodeRNN and edgeRNNs involved during the forward-pass. That way, S-RNN non-linearly combines the node and edge features associated with the nodes in order to predict the node labels.\n\nFigure 3c shows the forward-pass through S-RNN for the human node. Figure 4 shows a detailed architecture lay-\nout of the same forward-pass. The forward-pass involves the edgeRNNs \\( R_{E_1} \\) (human-object edge) and \\( R_{E_2} \\) (human-human edge). Since the human node \\( v \\) interacts with two object nodes \\( \\{u, w\\} \\), we pass the summation of the two edge features as input to \\( R_{E_1} \\). The summation of features, as opposed to concatenation, is important to handle variable number of object nodes with a fixed architecture. Since the object count varies with environment, it is challenging to represent variable context with a fixed length feature vector. Empirically, adding features works better than mean pooling. We conjecture that addition retains the object count and the structure of the st-graph, while mean pooling averages out the number of edges. The nodeRNN \\( R_{V_1} \\) concatenates the (human) node features with the outputs from edgeRNNs, and predicts the activity at each time step.\n\n**Parameter sharing and structured feature space.** An important aspect of S-RNN is sharing of parameters across the node labels. Parameter sharing between node labels happen when an RNN is common in their forward-pass. For example in Figure 3c and 3d, the edgeRNN \\( R_{E_1} \\) is common in the forward-pass for the human node and the object nodes. Furthermore, the parameters of \\( R_{E_1} \\) gets updated through back-propagated gradients from both the object and human nodeRNNs. In this way, \\( R_{E_1} \\) affects both the human and object node labels.\n\nSince the human node is connected to multiple object nodes, the input into edgeRNN \\( R_{E_1} \\) is always a linear combination of human-object edge features. This imposes an structure on the features processed by \\( R_{E_1} \\). More formally, the input into \\( R_{E_1} \\) is the inner product \\( s^T F \\), where \\( F \\) is the feature matrix storing the edge features \\( x_e \\) s.t. \\( e \\in E_1 \\). Vector \\( s \\) captures the structured feature space. Its entries are in \\( \\{0,1\\} \\) depending on the node being forward-passed. In the example above \\( F = [x_{v,u}, x_{v,w}]^T \\). For the human node \\( v \\), \\( s = [1 1]^T \\), while for the object node \\( u \\), \\( s = [1 0]^T \\).\n\n### 4. Experiment\n\nWe present results on three diverse spatio-temporal problems to ensure generic applicability of S-RNN, shown in Figure 5. The applications include: (i) modeling human motion [14] from motion capture data [21]; (ii) human activity detection and anticipation [29, 31]; and (iii) maneuver anticipation from real-world driving data [22].\n\n#### 4.1. Human motion modeling and forecasting\n\nHuman body is a good example of separate but well related components. Its motion involves complex spatio-temporal interactions between the components (arms, legs, spine), resulting in sensible motion styles like walking, eating etc. In this experiment, we represent the complex motion of humans over st-graphs and learn to model them with S-RNN. We show that our structured approach outperforms the state-of-the-art unstructured deep architecture [14] on motion forecasting from motion capture (mocap) data. Several approaches based on Gaussian processes [59, 63], Restricted Boltzmann Machines (RBMs) [57, 56, 51], and RNNs [14] have been proposed to model human motion. Recently, Fragkiadaki et al. [14] proposed an encoder-RNN-decoder (ERD) which gets state-of-the-art forecasting results on H3.6m mocap data set [21].\n\n**S-RNN architecture for human motion.** Our S-RNN architecture follows the st-graph shown in Figure 5a. According to the st-graph, the spine interacts with all the body parts, and the arms and legs interact with each other. The st-graph is automatically transformed to S-RNN following Section 3.2. The resulting S-RNN have three nodeRNNs, one for each type of body part (spine, arm, and leg), four edgeRNNs for modeling the spatio-temporal interactions between them, and three edgeRNNs for their temporal connections. For edgeRNNs and nodeRNNs we use FC(256)-FC(256)-LSTM(512) and LSTM(512)-FC(256)-FC(100)-FC(-) architectures, respectively, with skip input and output connections [18]. The outputs of nodeRNNs are skeleton joints of different body parts, which are concatenated to reconstruct the complete skeleton. In order to model human motion, we train S-RNN to predict the mocap frame at time \\( t + 1 \\) given the frame at time \\( t \\). Similar to [14], we gradually add noise to the mocap frames during training. This simulates curriculum learning [2] and helps in keeping the forecasted motion close to the manifold of human motion. As node features we use the raw joint values expressed as exponential map [14], and edge features are concatenation...\nof the node features. We train all RNNs jointly to minimize the Euclidean loss between the predicted mocap frame and the ground truth. See supplementary material on the project web page [24] for training details.\n\n**Evaluation setup.** We compare S-RNN with the state-of-the-art ERD architecture [14] on H3.6m mocap data set [21]. We also compare with a 3 layer LSTM architecture (LSTM-3LR) which [14] use as a baseline. For motion forecasting we follow the experimental setup of [14]. We downsample H3.6m by two and train on 6 subjects and test on subject S5. To forecast, we first feed the architectures with (50) seed mocap frames, and then forecast the future (100) frames. Following [14], we consider walking, eating, and smoking activities. In addition to these three, we also consider discussion activity.\n\nForecasting is specially challenging on activities with complex aperiodic human motion. In H3.6m data set, significant parts of eating, smoking, and discussion activities are aperiodic, while walking activity is mostly periodic. Our evaluation demonstrates the benefits of having an underlying structure in three important ways: (i) We present visualizations and quantitative results on complex aperiodic activities ([14] evaluates only on periodic walking motion); (ii) We forecast human motion for almost twice longer than state-of-the-art [14]. This is very challenging for aperiodic activities; and finally (iii) We show S-RNN interestingly learns semantic concepts, and demonstrate its modularity by generating hybrid human motion. Unstructured deep architectures like [14] does not offer such modularity.\n\n**Qualitative results on motion forecasting.** Figure 6 shows forecasting 1000ms of human motion on \u201ceating\u201d activity \u2013 the subject drinks while walking. S-RNN stays close to the ground-truth in the short-term and generates human like motion in the long-term. On removing edgeRNNs, the parts of human body become independent and stops interacting through parameters. Hence without edgeRNNs the skeleton freezes to some mean position. LSTM-3LR suffers with a drifting problem. On many test examples it drifts to the mean position of walking human ([14] made similar observations about LSTM-3LR). The motion generated by ERD [14] stays human-like in the short-term but it drifts away to non-human like motion in the long-term. This was a common outcome of ERD on complex aperiodic activities, unlike S-RNN. Furthermore, ERD produced human motion was non-smooth on many test examples. See the video on the project web page for more examples [24].\n\n**Quantitative evaluation.** We follow the evaluation metric of Fragkiadaki et al. [14] and present the 3D angle error between the forecasted mocap frame and the ground truth in Table 1. Qualitatively, ERD models human motion better than LSTM-3LR. However, in the short-term, it does not mimic the ground-truth as well as LSTM-3LR. Fragkiadaki et al. [14] also note this trade-off between ERD and LSTM-3LR. On the other hand, S-RNN outperforms both LSTM-3LR and ERD on short-term motion forecasting on all activities. S-RNN therefore mimics the ground truth in the short-term and generates human like motion in the long term. In this way it well handles both short and long term forecasting. Due to stochasticity in human motion, long-term forecasts (> 500ms) can significantly differ from the ground truth but still depict human-like motion. For this reason, the long-term forecast numbers in Table 1 are not a fair representative of algorithms modeling capabilities. We also observe that discussion is one of the most challenging\n\n---\n\n**Table 1:** Motion forecasting angle error. {80, 160, 320, 560, 1000} msecs after the seed motion. The results are averaged over 8 seed motion sequences for each activity on the test subject.\n\n| Methods | Short-term forecast | Long-term forecast |\n|---------|---------------------|--------------------|\n|         | 80ms    | 160ms   | 320ms   | 560ms   | 1000ms  |\n| Walking activity |\n| ERD [14] | 1.30    | 1.56    | 1.87    | 2.00    | 2.38    |\n| LSTM-3LR | 1.18    | 1.50    | 1.67    | 1.81    | 2.20    |\n| S-RNN    | 1.08    | 1.54    | 1.60    | 1.90    | 2.13    |\n| Eating activity |\n| ERD [14] | 1.66    | 1.93    | 2.28    | 2.36    | 2.41    |\n| LSTM-3LR | 1.36    | 1.79    | 2.29    | 2.49    | 2.82    |\n| S-RNN    | 1.35    | 1.71    | 2.12    | 2.28    | 2.58    |\n| Smoking activity |\n| ERD [14] | 2.34    | 2.74    | 3.73    | 3.68    | 3.82    |\n| LSTM-3LR | 2.05    | 2.34    | 3.10    | 3.24    | 3.42    |\n| S-RNN    | 1.90    | 2.30    | 2.90    | 3.21    | 3.23    |\n| Discussion activity |\n| ERD [14] | 2.67    | 2.97    | 3.23    | 3.47    | 2.92    |\n| LSTM-3LR | 2.25    | 2.33    | 2.45    | 2.48    | 2.93    |\n| S-RNN    | 1.67    | 2.03    | 2.20    | 2.39    | 2.43    |\n\n---\n\n\\*We reproduce ERD and LSTM-3LR architectures following [14]. The authors implementation were not available at the time of submission.\naperiodic activity for all algorithms.\n\n**User study.** We asked users to rate the motions on a Likert scale of 1 to 3. S-RNN performed best according to the user study. See supplementary material for the results.\n\nTo summarize, unstructured approaches like LSTM-3LR and ERD struggles to model long-term human motion on complex activities. S-RNN\u2019s good performance is attributed to its structural modeling of human motion through the underlying st-graph. S-RNN models each body part separately with nodeRNNs and captures interactions between them with edgeRNNs in order to produce coherent motions.\n\n### 4.2. Going deeper into structural-RNN\n\nWe now present several insights into S-RNN architecture and demonstrate the modularity of the architecture which enables it to generate hybrid human motions.\n\n**Visualization of memory cells.** We investigated if S-RNN memory cells represent meaningful semantic sub-motions. Semantic cells were earlier studied on other problems [27], we are the first to present it on a vision task and human motion. In Figure 7 (left) we show a cell in the leg nodeRNN learns the semantic motion of *moving the leg forward*. The cell fires positive (red color) on the forward movement of the leg and negative (blue color) on its backward movement. As the subject walks, the cell alternatively fires for the right and the left leg. Longer activations in the right leg corresponds to the longer steps taken by the subject with the right leg. Similarly, a cell in the arm nodeRNN learns the concept of *moving hand close to the face*, as shown in Figure 7 (right). The same cell fires whenever the subject moves the hand closer to the face during eating or smoking. The cell remains active as long as the hand stays close to the face. See the video [24].\n\n**Generating hybrid human motion.** We now demonstrate the flexibility of our modular architecture by generating novel yet meaningful motions which are not in the data set. Such modularity is of interest and has been explored to generate diverse motion styles [55]. As a result of having an underlying high-level structure, our approach allows us to exchange RNNs between the S-RNN architectures trained on different motion styles. We leverage this to create a novel S-RNN architecture which generates a hybrid motion of a *human jumping forward on one leg*, as shown in Figure 8 (Left). For this experiment we modeled the left and right leg with different nodeRNNs. We trained two independent S-RNN models \u2013 a slower human and a faster human (by down sampling data) \u2013 and swapped the left leg nodeRNN of the trained models. The resulting faster human, with a slower left leg, jumps forward on the left leg to keep up with its twice faster right leg.\\(^4\\) Unstructured architectures like ERD [14] does not offer this kind of flexibility.\n\nFigure 8 (Right) examines the test and train error with iterations. Both S-RNN and ERD converge to similar training error, however S-RNN generalizes better with a smaller test error for next step prediction. Discussion in supplementary.\n\n### 4.3. Human activity detection and anticipation\n\nIn this section we present S-RNN for modeling human activities. We consider the CAD-120 [29] data set where the activities involve rich human-object interactions. Each activity consist of a sequence of sub-activities (e.g. moving, drinking etc.) and objects affordance (e.g., reachable, drinkable etc.), which evolves as the activity progresses. Detecting and anticipating the sub-activities and affordance enables personal robots to assist humans. However, the problem is challenging as it involves complex interactions \u2013 humans interact with multiple objects during an activity, and objects also interact with each other (e.g. pouring water from \u201cglass\u201d into a \u201ccontainer\u201d), which makes it a particularly good fit for evaluating our method. Koppula et al. [31, 29] represents such rich spatio-temporal interactions with the st-graph shown in Figure 5b, and models it with a spatio-temporal CRF. In this experiment, we show that modeling the same st-graph with S-RNN yields superior results. We use the node and edges features from [29].\n\nFigure 3b shows our S-RNN architecture to model the st-graph. Since the number of objects varies with environment, factor sharing between the object nodes and the human-object edges becomes crucial. In S-RNN, \\(R_{V_2}\\) and \\(R_{E_1}\\) handles all the object nodes and the human-object edges respectively. This allows our fixed S-RNN architecture to handle varying size st-graphs. For edgeRNNs we use a single layer LSTM of size 128, and for nodeRNNs we use LSTM(256)-softmax(\\(\\cdot\\)). At each time step, the human nodeRNN outputs the sub-activity label (10 classes), and the object nodeRNN outputs the affordance (12 classes).\n\n---\n\n\\(^4\\)Imagine your motion forward if someone holds your right leg and runs!\nTable 2: Maneuver Anticipation on 1100 miles of real-world driving data. S-RNN is derived from the st-graph shown in Figure 5c. Jain et al. [22] use the same st-graph but models it in a probabilistic frame with AIO-HMM. The table shows average precision, recall and time-to-maneuver. Time-to-maneuver is the interval between the time of algorithm\u2019s prediction and the start of the maneuver. Algorithms are compared on the features from [22].\n\n| Method          | Turns | Lane change | All maneuvers |\n|-----------------|-------|-------------|---------------|\n|                 | Pr (%) | Re (%)      | Time-to-maneuver (s) | Pr (%) | Re (%) | Time-to-maneuver (s) | Pr (%) | Re (%) | Time-to-maneuver (s) |\n| SVM             | 64.7   | 47.2        | 2.40           | 73.7   | 57.8   | 2.40           | 43.7   | 37.7   | 1.20           |\n| AIO-HMM [22]    | 80.8   | 75.2        | 4.16           | 83.8   | 79.2   | 3.80           | 77.4   | 71.2   | 3.53           |\n| S-RNN w/o edgeRNN | 75.2   | 75.3        | 3.68           | 85.4   | 86.0   | 3.53           | 78.0   | 71.1   | 3.15           |\n| S-RNN           | 81.2   | 78.6        | 3.94           | 92.7   | 84.4   | 3.46           | 82.2   | 75.9   | 3.75           |\n\nTable 3: Results on CAD-120 [29]. S-RNN architecture derived from the st-graph in Figure 5b outperforms Koppula et al. [31, 29] which models the same st-graph in a probabilistic framework. S-RNN in multi-task setting (joint detection and anticipation) further improves the performance.\n\n| Method          | Detection F1-score | Anticipation F1-score |\n|-----------------|--------------------|-----------------------|\n|                 | Sub-activity (%)  | Object Affordance (%) | Sub-activity (%)  | Object Affordance (%) |\n| Koppula et al. [31, 29] | 80.4               | 81.5                  | 37.9               | 36.7                  |\n| S-RNN w/o edgeRNN | 82.2               | 82.1                  | 64.8               | 72.4                  |\n| S-RNN            | 83.2               | 88.7                  | 62.3               | 80.7                  |\n| S-RNN (multi-task)| 82.4               | 91.1                  | 65.6               | 80.9                  |\n\nHaving observed the st-graph upto time $t$, the goal is to detect the sub-activity and affordance labels at the current time $t$, and also anticipate their future labels of the time step $t+1$. For detection we train S-RNN on the labels of the current time step. For anticipation we train the architecture to predict the labels of the next time step, given the observations upto the current time. We also train a multi-task version of S-RNN, where we add two softmax layers to each nodeRNN and jointly train for anticipation and detection.\n\nTable 3 shows the detection and anticipation F1-scores averaged over all the classes. S-RNN significantly improves over Koppula et al. on both anticipation [31] and detection [29]. On anticipating object affordance S-RNN F1-score is 44% more than [31], and 7% more on detection. S-RNN does not have any Markov assumptions like spatio-temporal CRF, and therefore, it better models the long-time dependencies needed for anticipation. The table also shows the importance of edgeRNNs in handling spatio-temporal components. EdgeRNN transfers the information from the human to objects, which helps is predicting the object labels. Therefore, S-RNN without the edgeRNNs poorly models the objects. This signifies the importance of edgeRNNs and also validates our design. Finally, training S-RNN in a multi-task manner works best in majority of the cases. In Figure 9 we show the visualization of an eating activity. We show one representative frame from each sub-activity and our corresponding predictions.\n\nS-RNN complexity. In terms of complexity, we discuss two aspects as a function of the underlying st-graph: (i) the number of RNNs in the mixture; and (ii) the complexity of forward-pass. The number of RNNs depends on the number of semantically similar nodes in the st-graph. The overall S-RNN architecture is compact because the edgeRNNs are shared between the nodeRNNs, and the number of semantic categories are usually few in context-rich applications. Furthermore, because of factor sharing the number of RNNs does not increase if more semantically similar nodes are added to the st-graph. The forward-pass complexity depends on the number of RNNs. Since the forward-pass through all edgeRNNs and nodeRNNs can happen in parallel, in practice, the complexity only depends on the cascade of two neural networks (edgeRNN followed by nodeRNN).\n\n4.4. Driver maneuver anticipation\n\nWe finally present S-RNN for another application which involves anticipating maneuvers several seconds before they happen. Jain et al. [22] represent this problem with the st-graph shown in Figure 5c. They model the st-graph as a probabilistic Bayesian network (AIO-HMM [22]). The st-graph represents the interactions between the observations outside the vehicle (eg. the road features), the driver\u2019s maneuvers, and the observations inside the vehicle (eg. the driver\u2019s facial features). We model the same st-graph with S-RNN architecture using the node and edge features from Jain et al. [22]. Table 2 shows the performance of different algorithms on this task. S-RNN performs better than the state-of-the-art AIO-HMM [22] in every setting. See supplementary material for the discussion and details [24].\n\n5. Conclusion\n\nWe proposed a generic and principled approach for combining high-level spatio-temporal graphs with sequence modeling success of RNNs. We make use of factor graph, and factor sharing in order to obtain an RNN mixture that is scalable and applicable to any problem expressed over st-graphs. Our RNN mixture captures the rich interactions in the underlying st-graph. We demonstrated significant improvements with S-RNN on three diverse spatio-temporal problems including: (i) human motion modeling; (ii) human-object interaction; and (iii) driver maneuver anticipation. By visualizing the memory cells we showed that S-RNN learns certain semantic sub-motions, and demonstrated its modularity by generating novel human motion.\n\n---\n\n5We acknowledge NRI #1426452, ONR-N00014-14-1-0156, MURI-WF911NF-15-1-0479 and Panasonic Center grant #122282.\nReferences\n\n[1] Y. Bengio, Y. LeCun, and D. Henderson. Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden markov models. *NIPS*, 1994.\n\n[2] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In *ICML*, 2009.\n\n[3] L. Bottou, Y. Bengio, and Y. LeCun. Global training of document processing systems using graph transformer networks. In *CVPR*, 1997.\n\n[4] W. Brendel and S. Todorovic. Learning spatiotemporal graphs of human activities. In *ICCV*, 2011.\n\n[5] W. Byeon, T. Breuel, F. Raue, and M. Liwicki. Scene labeling with lstm recurrent neural networks. In *CVPR*, 2015.\n\n[6] L. C. Chen, G. Papandreou, I. Kokkinos, and K. M. A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. *arXiv:1412.7062*, 2014.\n\n[7] L. C. Chen, A. Schwing, A. L. Yuille, and R. Urtasun. Learning deep structured models. In *ICML*, 2015.\n\n[8] M. Chen and A. Hauptmann. Mosift: Recognizing human actions in surveillance videos. 2009.\n\n[9] X. Chen and C. L. Zitnick. Mind\u2019s eye: A recurrent visual representation for image caption generation. In *CVPR*, 2015.\n\n[10] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In *CVPR*, 2015.\n\n[11] B. Douillard, D. Fox, and F. Ramos. A spatio-temporal probabilistic model for multi-sensor multi-class object recognition. In *Robotics Research*, 2011.\n\n[12] Y. Du, W. Wang, and L. Wang. Hierarchical recurrent neural network for skeleton based action recognition. In *CVPR*, 2015.\n\n[13] P. Felzenszwalb, D. McAllester, and D. Ramanan. A discriminatively trained, multiscale, deformable part model. In *CVPR*, 2008.\n\n[14] K. Fragkiadaki, S. Levine, P. Felsen, and J. Malik. Recurrent network models for human dynamics. In *ICCV*, 2015.\n\n[15] A. G. S. G and R. Urtasun. Fully connected deep structured networks. *arXiv:1503.02351*, 2015.\n\n[16] R. Girshick. Fast r-cnn. In *ICCV*, 2015.\n\n[17] C. Goller and A. Kuchler. Learning task-dependent distributed representations by backpropagation through structure. In *Neural Networks, IEEE*, volume 1, 1996.\n\n[18] A. Graves. Generating sequences with recurrent neural networks. *arXiv:1308.0850*, 2013.\n\n[19] A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In *ICML*, 2014.\n\n[20] A. Gupta, A. Kembhavi, and L. S. Davis. Observing human-object interactions: Using spatial and functional compatibility for recognition. *IEEE PAMI*, 31(10), 2009.\n\n[21] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. *IEEE PAMI*, 36(7), 2014.\n\n[22] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena. Car that knows before you do: Anticipating maneuvers via learning temporal driving models. In *ICCV*, 2015.\n\n[23] A. Jain, S. Sharma, and A. Saxena. Beyond geometric path planning: Learning context-driven user preferences via sub-optimal feedback. In *ISRR*, 2013.\n\n[24] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. S-rnn supplementary video and material. http://asheshjain.org/srnn.\n\n[25] M. Jain, J. C. van Gemert, T. Mensink, and C. Snoek. Objects2action: Classifying and localizing actions without any video example. In *ICCV*, 2015.\n\n[26] T. Joachims, T. Finley, and C.-N. J. Yu. Cutting-plane training of structural svms. *Machine Learning*, 77(1):27\u201359, 2009.\n\n[27] A. Karpathy, J. Johnson, and F. F. Li. Visualizing and understanding recurrent networks. *arXiv:1506.02078*, 2015.\n\n[28] D. Koller and N. Friedman. *Probabilistic graphical models: principles and techniques*. MIT press, 2009.\n\n[29] H. Koppula, R. Gupta, and A. Saxena. Learning human activities and object affordances from rgb-d videos. *IJRR*, 32(8), 2013.\n\n[30] H. Koppula, A. Jain, and A. Saxena. Anticipatory planning for humanrobot teams. In *ISER*, 2014.\n\n[31] H. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic response. In *RSS*, 2013.\n\n[32] H. Koppula and A. Saxena. Learning spatio-temporal structure from rgb-d videos for human activity detection and anticipation. In *ICML*, 2013.\n\n[33] H. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic response. *IEEE PAMI*, 2015.\n\n[34] P. Kr\u00e4henb\u00fchl and V. Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. *arXiv:1210.5644*, 2012.\n\n[35] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor graphs and the sum-product algorithm. *Information Theory, IEEE Trans.*, 47(2), 2001.\n\n[36] I. Laptev, M. Marsza\u0142ek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In *CVPR*, 2008.\n\n[37] J. Lezama, K. Alahari, J. Sivic, and I. Laptev. Track to the future: Spatio-temporal video segmentation with long-range motion cues. In *CVPR*, 2011.\n\n[38] S. Li, W. Zhang, and A. B. Chan. Maximum-margin structured learning with deep networks for 3d human pose estimation. In *ICCV*, 2015.\n\n[39] Y. Li and R. Nevatia. Key object driven multi-category object recognition, localization and tracking using spatio-temporal context. In *Proc. ECCV*, 2008.\n\n[40] G. Lin, C. Shen, I. Reid, et al. Efficient piecewise training of deep structured models for semantic segmentation. *arXiv:1504.01013*, 2015.\n\n[41] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic image segmentation via deep parsing network. In *ICCV*, 2015.\n\n[42] A. McCallum, K. Schultz, and S. Singh. Factorie: Probabilistic programming via imperatively defined factor graphs. In *NIPS*, 2009.\n\n[43] T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. A. Ranzato. Learning longer memory in recurrent neural networks. *arXiv:1412.7753*, 2014.\n\n[44] A. Pieropan, C. H. Ek, and H. Kjellstr\u00f6m. Recognizing object affordances in terms of spatio-temporal object-object relationships. In *IEEE-RAS Intl. Conf. on Humanoid Robots*, 2014.\n\n[45] A. Quattoni, M. Collins, and T. Darrell. Conditional random fields for object recognition. In *NIPS*, 2004.\n\n[46] M. Richardson and P. Domingos. Markov logic networks. *ML*, 62(1-2), 2006.\n[47] Q. Shi, L. Cheng, L. Wang, and A. Smola. Human action segmentation and recognition using discriminative semi-markov models. *IJCV*, 93(1), 2011.\n\n[48] R. Socher, C. C. Lin, A. Y. Ng, and C. D. Manning. Parsing Natural Scenes and Natural Language with Recursive Neural Networks. In *ICML*, 2011.\n\n[49] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using lstms. In *ICML*, 2015.\n\n[50] C. Sun and R. Nevatia. Active: Activity concept transitions in video event classification. In *ICCV*, 2013.\n\n[51] I. Sutskever, G. Hinton, and G. Taylor. The recurrent temporal restricted boltzmann machine. In *NIPS*, 2009.\n\n[52] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In *NIPS*, 2014.\n\n[53] C. Sutton and A. McCallum. An introduction to conditional random fields. *Machine Learning*, 4(4), 2011.\n\n[54] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In *UAI*, 2002.\n\n[55] G. Taylor and G. E. Hinton. Factored conditional restricted boltzmann machines for modeling motion style. In *ICML*, 2009.\n\n[56] G. Taylor, L. Sigal, D. J. Fleet, and G. E. Hinton. Dynamical binary latent variable models for 3d human pose tracking. In *CVPR*, 2010.\n\n[57] G. W. Taylor, G. E. Hinton, and S. T. Roweis. Modeling human motion using binary latent variables. In *NIPS*, 2006.\n\n[58] J. Tompson, M. Stein, Y. Lecun, and K. Perlin. Real-time continuous pose recovery of human hands using convolutional networks. *ACM TOG*, 33(5), 2014.\n\n[59] R. Urtasun, D. J. Fleet, A. Geiger, J. Popovi\u0107, T. J. Darrell, and N. D. Lawrence. Topologically-constrained latent variable models. In *ICML*, 2008.\n\n[60] D. L. Vail, M. M. Veloso, and J. D. Lafferty. Conditional random fields for activity recognition. In *AAMAS*, 2007.\n\n[61] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko. Translating videos to natural language using deep recurrent neural networks. *arXiv:1412.4729*, 2014.\n\n[62] H. Wang and C. Schmid. Action recognition with improved trajectories. In *CVPR*, 2013.\n\n[63] J. M. Wang, D. J. Fleet, and A. Hertzmann. Gaussian process dynamical models for human motion. *IEEE PAMI*, 30(2), 2008.\n\n[64] N. Zhang, M. Paluri, M. A. Ranzato, T. Darrell, and L. Bourdev. Panda: Pose aligned networks for deep attribute modeling. In *CVPR*, 2014.\n\n[65] X. Zhang, P. Jiang, and F. Wang. Overtaking vehicle detection using a spatio-temporal crf. In *IVS, IEEE*, 2014.\n\n[66] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr. Conditional random fields as recurrent neural networks. In *ICCV*, 2015.", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/badlines.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 10, "total-input-tokens": 17520, "total-output-tokens": 13152, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 3475, 1], [3475, 9579, 2], [9579, 14711, 3], [14711, 18768, 4], [18768, 23408, 5], [23408, 28366, 6], [28366, 33230, 7], [33230, 39522, 8], [39522, 45752, 9], [45752, 48422, 10]], "primary_language": ["en", "en", "en", "en", "en", "en", "en", "en", "en", "en"], "is_rotation_valid": [true, true, true, true, true, true, true, true, true, true], "rotation_correction": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "is_table": [false, false, false, false, false, false, false, false, false, false], "is_diagram": [false, false, true, false, false, false, false, false, false, false]}}
{"id": "a7e49b6d8ff2064efeeb99c16cb479c996ca5784", "text": "Chapter 2\n\nMathematical Induction:\n\u201cAnd so on . . .\u201d\n\n2.1 Introduction\n\nThis chapter marks our first big step toward investigating mathematical proofs more thoroughly and learning to construct our own. It is also an introduction to the first significant proof technique we will see. As we describe below, this chapter is meant to be an appetizer, a first taste, of what mathematical induction is and how to use it. A couple of chapters from now, we will be able to rigorously define induction and prove that this technique is mathematically valid. That\u2019s right, we\u2019ll actually prove how and why it works! For now, though, we\u2019ll continue our investigation of some interesting mathematical puzzles, with these particular problems hand-picked by us for their use of inductive techniques.\n\n2.1.1 Objectives\n\nThe following short sections in this introduction will show you how this chapter fits into the scheme of the book. They will describe how our previous work will be helpful, they will motivate why we would care to investigate the topics that appear in this chapter, and they will tell you our goals and what you should keep in mind while reading along to achieve those goals. Right now, we will summarize the main objectives of this chapter for you via a series of statements. These describe the skills and knowledge you should have gained by the conclusion of this chapter. The following sections will reiterate these ideas in more detail, but this will provide you with a brief list for future reference. When you finish working through this chapter, return to this list and see if you understand all of these objectives. Do you see why we outlined them here as being important? Can you define all the terminology we use? Can you apply the techniques we describe?\nBy the end of this chapter, you should be able to . . .\n\n\u2022 Define what an inductive argument is, as well as classify a presented argument as an inductive one or not.\n\n\u2022 Decide when to use an inductive argument, depending on the structure of the problem you are solving.\n\n\u2022 Heuristically describe mathematical induction via an analogy.\n\n\u2022 Identify and describe different kinds of inductive arguments by comparing and contrasting them, as well as identify the underlying structures of the corresponding problems that would yield these similarities and differences.\n\n2.1.2 Segue from previous chapter\n\nAs in the previous chapter, we won\u2019t assume any familiarity with more advanced mathematics beyond basic algebra and arithmetic, and perhaps some visual, geometric intuition. We will, however, make use of summation and product notation fairly often, so if you feel like your notational skills are, go back and review Section 1.3.5.\n\n2.1.3 Motivation\n\nLook back at the Puzzle in Section 1.4.3, where we proved that the sum of the first $n$ odd natural numbers is exactly $n^2$. We first observed this pattern geometrically, by arranging the terms of the sums (odd integers) as successively larger \u201ccorner pieces\u201d of a square. The first way we proved the result, though, didn\u2019t seem to depend on that observation and merely utilized a previous result (about sums of even and odd integers) in an algebraic way; that is, we did some tricky manipulation of some equations (multiplying and subtracting and what have you) and then\u2014voil\u00e0!\u2014out popped the result we expected. What did you think about that approach? Did it feel satisfying? In a way, it didn\u2019t quite match the geometric interpretation we had, at first, so it might be surprising that it worked out so nicely. (Perhaps there is a different geometric interpretation of this approach. Can you find one?)\n\nOur second approach was to model that initial geometric observation. We transformed visual pieces into algebraic pieces; specifically, a sum was related to the area of a square, and the terms of the sum were related to particular pieces of that square. We established a correspondence between different interpretations of the same problem, finding a way to relate one to the other so that we could work with either interpretation and know that we were proving something about the overall result. The benefit of the visual interpretation is that it allowed us to take advantage of a general proof strategy known as mathematical induction, or sometimes just induction, for short. (The word induction has some non-mathematical meanings, as well, such as in electromagnetism or in philosophical arguments, but within the context of this book, when we say induction, we mean\nmathematical induction.) What exactly is induction? How does it work? When can we use this strategy? How do we adapt the strategy to a particular puzzle? Are there variations of the strategy that are more useful in certain situations? These are all questions that we hope to answer in this chapter.\n\nThe first topic we\u2019d like to address is a question that we didn\u2019t just ask in the last paragraph, namely, \u201cWhy induction? Why bother with it?\u201d Based on that puzzle in Section 1.4.3, it would seem that mathematical induction isn\u2019t entirely necessary since there might be other ways of proving something, instead of by induction. Depending on the context, this very well may be true, but the point we\u2019d like to make clear from the beginning is that induction is incredibly useful! There are many situations where a proof by induction is the most concise and clear approach, and it is a well-known general strategy that can be applied in a variety of such situations. Furthermore, applying induction to a problem requires there to be a certain structure to the problem, a dependence of one \u201cpart\u201d of the result on a \u201cprevious part\u201d. (The \u201cparts\u201d and the \u201cdependence\u201d will depend on the context, of course.) Recognizing that induction applies, and actually going through the subsequent proof process, will usually teach us something about the inherent structure of the problem. This is true even when induction fails! Perhaps there\u2019s a particular part of a problem that \u201cruins\u201d the induction process, and identifying that particular part can be helpful and insightful.\n\nWe hope to motivate these points through some illustrative examples first, after which we will provide a reasonably thorough definition of mathematical induction that will show how the method works, in generality. (A completely rigorous definition will have to be put off until a little bit later, after we have defined and investigated some relevant concepts, like set theory and logical statements and implications. For now, though, the definition we give will suffice to work on some interesting puzzles and allow us to discuss induction as a general proof strategy.)\n\n2.1.4 Goals and Warnings for the Reader\n\nDo keep in mind that we are still building towards our goal of mathematical rigor, or as much as is possible within the scope and timing of this book and course. Some of the claims we make in this Chapter will be clarified and technically proven later on, once we have properly discussed the natural numbers and some basic mathematical logic. All in due time!\n\nThat said, this chapter is still very important, since we are continuing to introduce you to the process of solving mathematical problems, applying our existing knowledge and techniques to discover new facts and explain them to others. In addition, mathematical induction is a fundamental proof technique that will likely appear in every other mathematics course you take! This is because of its usefulness and the prevalence of inductive properties throughout the mathematical world.\n2.2 Examples and Discussion\n\n2.2.1 Turning Cubes Into Bigger Cubes\n\nTo motivate the overall method of mathematical induction, let\u2019s examine a geometric puzzle and solve it together. This example has been chosen carefully to illustrate how mathematical induction is relevant when a puzzle has a particular type of structure; specifically, some truth or fact or observation depends or relies or can be derived from a \u201cprevious\u201d fact. This dependence on a previous case (or cases) is what makes a process inductive, and when we observe this phenomenon, applying induction is almost always a good idea.\n\n1-Cube into a 2-Cube\n\nLet\u2019s examine cubic numbers and, specifically, let\u2019s try to describe a cubic number in terms of the previous cubic number. Imagine a $1 \\times 1 \\times 1$ cube, just one building block. How can we build the \u201cnext biggest\u201d cube, of size $2 \\times 2 \\times 2$, by adding $1 \\times 1 \\times 1$ building blocks? How many do we need to add? Arithmetically, we know the answer: $2^3 = 8$ and $1^3 = 1$, so we need to add 7 blocks to have the correct volume. Okay, that\u2019s a specific answer, but it doesn\u2019t quite tell us how to arrange those 7 blocks to make a cube, nor does it give us any insight into how to answer this question for larger cubes. Ultimately, we would like to say how many blocks are required to build a $100 \\times 100 \\times 100$ cube into a $101 \\times 101 \\times 101$ cube without having to perform a lot of tedious arithmetic; that is, we are hoping to eventually find an answer to the question: given an $n \\times n \\times n$ cube, how many blocks must we add to build it into a $(n + 1) \\times (n + 1) \\times (n + 1)$ cube? With that in mind, let\u2019s think carefully about this initial case and try to answer it with a general argument.\n\nGiven that single building block, and knowing we have to add 7 blocks to it, let\u2019s try to identify exactly where those 7 blocks should be placed to make a $2 \\times 2 \\times 2$ cube. (For simplicity, we will refer to a cube of size $n \\times n \\times n$ as an $n$-cube, for any value of $n$. We will only need to use values of $n$ that are natural numbers, i.e. non-negative whole numbers, in this example.) Look at the pictures of the 1-cube and 2-cube below and try to come up with an explanation of constructing one from the other.\n\nHere\u2019s one reasonable explanation that we want to use because it will guide us in the general explanation of building an $(n + 1)$-cube from an $n$-cube, and because\nit is a mathematically elegant and simple explanation. Start with the 1-cube positioned as it is above and \u201cenlarge\u201d the 3 exposed faces by the appropriate amount, in this case by one block. This accounts for 3 of the 7 blocks, thus far: $2^3 = 1^3 + 3 + \\_\\_$. Where are there \u201choles\u201d now?\n\nThe blocks we just added have created \u201cgaps\u201d between each pair of them, and each of those \u201cgaps\u201d can be filled with one block. This accounts for 3 more of the 7 total blocks: $2^3 = 1^3 + 3 + 3 + \\_\\_$. Now what?\n\nThere is just one block left to be filled, and it\u2019s the very top corner. Adding this block completes the 2-cube and tells us how to mathematically describe our construction process with the following picture and equation:\n\n$$2^3 = 1 + 3 + 3 + 1$$\n\n**2-Cube into a 3-Cube**\n\nOkay, we might now have a better idea of how to describe this process in general, but let\u2019s examine another case or two just to make sure we have the full idea.\nLet\u2019s start with a 2-cube and construct a 3-cube from it. (You can even try this out by hand if you happen to own various sizes of Rubik\u2019s Cubes!) We can follow a process similar to the steps we used in the previous case and just change the numbers appropriately. Starting with a similar picture\n\nwe see that we need to \u201cenlarge\u201d the three exposed faces of the 2-cube but, in this case, the amount by which we need to enlarge them is different than before (with the 1-cube) since we are working with a larger initial cube. Specifically, each face must be enlarged by a $2 \\times 2$ square of blocks (whereas, in the previous case, we added a $1 \\times 1$ square of blocks). Thus, an equation to account for this addition is\n\n$$3^3 = 2^3 + 3 \\cdot 2^2 + ___$$\n\nAfter we do this, we see that we need to fill in the gaps between those enlarged faces with $2 \\times 1$ of blocks (whereas, in the previous case, we added $1 \\times 1$ rows of blocks). An equation to account for the additions thus far is\n\n$$3^3 = 2^3 + 3 \\cdot 2^2 + 3 \\cdot 2 + ___$$\nAfter we do this, we see that there is only the top corner left to fill in. Accordingly, we can depict our construction process and its corresponding equation:\n\n\\[\n3^3 = 2^3 + 3 \\cdot 2^2 + 3 \\cdot 2 + 1\n\\]\n\n**n-Cube into an (n + 1)-Cube**\n\nDo you see now how this process will generalize? What if we started with an \\( n \\)-cube? How could we construct an \\((n + 1)\\)-cube? Let\u2019s follow the same steps we used in the previous two cases. First, we would enlarge the three exposed faces by adding three squares of blocks. How big is each square? Well, we want each square to be the same size as the exposed faces, so they will be \\( n \\times n \\) squares, accounting for \\( n^2 \\) blocks for each face:\nNext, we would fill in the gaps between these enlarged faces with rows of blocks. How long are those rows? Well, they each lie along the edges of the squares of blocks we just added, so they will each be of size $n \\times 1$, accounting for $n$ blocks for each gap:\n\n\\[(n + 1)^3 = n^3 + 3n^2 + ___\\]\n\nFinally, there will only be the top corner left to fill in! Therefore,\n\n\\[(n + 1)^3 = n^3 + 3n^2 + 3n + 1\\]\n\n\u201cWait a minute!\u201d you might say, abruptly. \u201cWe already knew that, right?\u201d In a way, yes; the equation above is an algebraic identity that we can also easily see by just expanding the product on the left and collecting terms:\n\n\\[(n + 1)^3 = (n + 1) \\cdot (n + 1)^2 \\\\\n= (n + 1) \\cdot (n^2 + 2n + 1) \\\\\n= (n^3 + 2n^2 + n) + (n^2 + 2n + 1) \\\\\n= n^3 + 3n^2 + 3n + 1\\]\n\nSo what have we really accomplished? Well, the main point behind deriving this identity in this geometric and visual way is that it exhibits how this identity represents some kind of *inductive* process. We sought to explain how to derive one \u201cfact\u201d (a cubic number, $(n + 1)^3$) from a previously known \u201cfact\u201d (the next smallest cubic number, $n^3$) and properly explained how to do just that. Compare this to one of the methods we used to investigate the fact that the sums of odd integers yield perfect squares, too. That observation also belies an\ninductive process and, although we didn\u2019t describe it as such at the time, we encourage you to think about that now. Look back at our discussion and try to write out how you could write \\((n + 1)^2\\) in terms of \\(n^2\\) by looking at squares of blocks. Does it look anything like an \u201cobvious\u201d algebraic identity? (If you\u2019re feeling ambitious, think about what happens with writing \\((n + 1)^4\\) in terms of \\(n^4\\). Is there any geometric intuition behind this? What about higher powers?)\n\nThe benefit of the method we\u2019ve used is that we now know how to describe cubic numbers in terms of smaller cubic numbers, all the way down to 1; that is, any time we see a cubic number in an expression, we know precisely how to write that value in terms of a smaller cubic number and some leftover terms. Furthermore, each of those expressions and leftover terms have an inherent structure to them that depends on the cubic number in question. Thus, by iteratively replacing any cubic number, like \\((n + 1)^3\\), with an expression like the one we derived above, and continuing until we can\u2019t replace any more, should produce an equation that has some built in symmetry. This idea is best illustrated by actually doing it, so let\u2019s see what happens. Let\u2019s start with the expression we derived, for some arbitrary value of \\(n\\),\n\n\\[\n(n + 1)^3 = n^3 + 3n^2 + 3n + 1\n\\]\n\nand then recognize that we now know a similar expression\n\n\\[\nn^3 = (n - 1)^3 + 3(n - 1)^2 + 3(n - 1) + 1\n\\]\n\nWe proved that this equation holds when we gave a general argument for the expression above for \\(n^3\\), since that only relied on the fact that \\(n \\geq 1\\). We can follow the same logical steps, throughout replacing \\(n\\) with \\(n - 1\\), and end up with the second expression above, for \\((n - 1)^3\\). (Does this keep going, for any value of \\(n\\)? Think about this for a minute. Does our argument make any sense when \\(n \\leq 0\\)? Would it make physical sense to talk about, say, constructing a \\((-2) \\times (-2) \\times (-2)\\) cube from a different cube?)\n\nTherefore, we can replace the \\(n^3\\) term in the line above\n\n\\[\n(n + 1)^3 = n^3 + 3n^2 + 3n + 1\n\\]\n\nThis is also an algebraic identity, but it\u2019s certainly not one that we would easily think to write down just by expanding the product on the left-hand side and grouping terms. Here, we are taking advantage of the structure of our result to apply it over and over and obtain new expressions that we wouldn\u2019t have otherwise thought to write down. Let\u2019s continue with this substitution process and see where it takes us! Next, we replace \\((n - 1)^3\\) with the corresponding expression and find\n\n\\[\n(n + 1)^3 = n^3 + 3n^2 + 3n + 1\n\\]\n\n\\[\n+ (n - 1)^3 + 3(n - 1)^2 + 3(n - 1) + 1\n\\]\n\n\\[\n+ (n - 2)^3 + 3(n - 2)^2 + 3(n - 2) + 1\n\\]\nPerhaps you see where this is going? We can do this substitution process over and over, and the columns that we\u2019ve arranged above will continue to grow, showing us that there is something deep and mathematically symmetric going on here. But where does this process stop? We want to write down a concise version of this iterative process and be able to explain all of the terms that arise, so we need to know where it ends. Remember the very first step in our investigation of the cubic numbers? We figured out how to write $2^3 = 1^3 + 3 + 3 + 1$. Since this was our first step in building this inductive process, it should be the last step we apply when building backwards, as we are now. Accordingly, we can write\n\n$$(n + 1)^3 = \\begin{array}{cccc}\n3n^2 & + & 3n & + 1 \\\\\n+ & 3(n - 1)^2 & + & 3(n - 1) & + 1 \\\\\n+ & 3(n - 2)^2 & + & 3(n - 2) & + 1 \\\\\n+ & 3(n - 3)^2 & + & 3(n - 3) & + 1 \\\\\n\\vdots & + & \\vdots & + & \\vdots \\\\\n+ & 3 \\cdot 2^2 & + & 3 \\cdot 2 & + 1 \\\\\n+ & 1^3 & + & 3 \\cdot 1^2 & + & 3 \\cdot 1 & + 1\n\\end{array}$$\n\nThis is definitely an identity we wouldn\u2019t have come up with off the top of our heads! In addition to being relatively pretty-looking on the page like this, it also allows us to apply some of our previous knowledge and simplify the expression. To see how we can do that, let\u2019s apply summation notation to the columns above and collect a bunch of terms into some simple expressions:\n\n$$(n + 1)^3 = 1^3 + 3 \\sum_{k=1}^{n} k^2 + 3 \\sum_{k=1}^{n} k + \\sum_{k=1}^{n} 1$$\n\nIn the last chapter, we saw a couple of different proofs that told us\n\n$$\\sum_{k=1}^{n} k = \\frac{n(n + 1)}{2}$$\n\nLet\u2019s use that fact in the line above, and also simplify the term on the far right, to write\n\n$$(n + 1)^3 = 1 + 3 \\sum_{k=1}^{n} k^2 + \\frac{3n(n + 1)}{2} + n$$\n\nWhat does this tell us? What have we accomplished after all this algebraic manipulation? Well, we previously proved a result about the sum of the first $n$ natural numbers, so a natural question to ask after that is: What is the sum of the first $n$ natural numbers squared? How could we begin to answer that? That\u2019s a trick question, because we already have! Let\u2019s do one or two more algebraic steps with the equation above by isolating the summation term and\nthen dividing:\n\n\\[\n(n + 1)^3 - 1 - n - \\frac{3n(n + 1)}{2} = 3 \\cdot \\sum_{k=1}^{n} k^2\n\\]\n\n\\[\n\\frac{1}{3}(n + 1)^3 - \\frac{1}{3}(n + 1) - \\frac{n(n + 1)}{2} = \\sum_{k=1}^{n} k^2\n\\]\n\nThis is what we\u2019ve accomplished: we\u2019ve derived a formula for the sum of the first \\( n \\) square natural numbers! Of course, the expression on the left in the line above isn\u2019t particularly nice looking and we could perform some further simplification, and we will leave it to you to verify that this yields the expression below:\n\n\\[\n\\sum_{k=1}^{n} k^2 = \\frac{1}{6} n(n + 1)(2n + 1)\n\\]\n\n\u201cAnd so on\u201d is not rigorous!\n\nThere are a couple of \u201cmorals\u201d that we\u2019d like to point out, based on all of this work. The first moral is that generalizing an argument is a good method for discovering new and interesting mathematical ideas and results. Did you think about how this puzzle is related to the sums of odd natural numbers? If not, we encourage you strongly to try that now, as well as think about generalizing this even further to four or five dimensional \u201ccubes\u201d and so on. In addition to giving you some other interesting results, it will also be incredibly instructive for learning to think abstractly and apply inductive processes. The second moral is more like an admission: we have not technically proven the formula above for the sum of the first \\( n \\) square natural numbers. It seems like our derivation is valid and tells us the \u201ccorrect answer\u201d but there is a glaring issue: ellipses!\n\nIn expanding the equation for \\((n + 1)^3\\) and obtaining those columns of terms that we collected into particular sums, writing : in the middle of those columns was helpful in guiding our intuition, but this is not a mathematically rigorous technique. How do we know that all of the terms in the middle are exactly what we\u2019d expect them to be? How can we be so sure that all of our pictures of cubes translate perfectly into the mathematical expressions we wrote down? What do we really mean by \u201cand keep going all the way down to 1\u201d?\n\nAs an example, consider this:\n\n\\[1, 2, 3, 4, \\ldots, 100\\]\n\nWhat is that list of numbers? You probably interpreted it as \u201call the natural numbers between 1 and 100, inclusive\u201d. That seems reasonable. But what if we actually meant this list?\n\n\\[1, 2, 3, 4, 7, 10, 11, 12, 14, \\ldots, 100\\]\n\nWhy, of course, we meant the list of natural numbers from 1 to 100 that don\u2019t have an \u201ci\u201d in their English spelling! Wasn\u2019t it obvious?\nThe point is this: when talking with a friend, and verbalizing some ideas, it might be okay to write \u201c1, 2, 3, . . . , 100\u201d and ensure that whoever is listening knows exactly what you mean. In general, though, we can\u2019t assume that a reader would just naturally intuit whatever we were trying to convey; we should be as explicit and rigorous as possible.\n\nIt may seem to you now like we\u2019re nit-picking, but the larger point is that there is a mathematical way of making this argument more precise, so that it constitutes a completely valid proof. Everything we have done so far is useful in guiding our intuition, but we will have to do a little more work to be sure our arguments are completely convincing. There are a few other concepts required to make this type of argument rigorous, in general, and we will investigate those in the next chapter and return to this subject immediately after that. However, in the meantime, let\u2019s examine one more example to practice this intuitive argument style and recognizing when induction is an applicable technique.\n\n### 2.2.2 Lines On The Plane\n\nTake a clean sheet of paper and a pen and a ruler. How many regions are on your sheet? Just one, right? Draw a line all the way across the paper. Now there are two regions. Draw another straight line that intersects your first line. How many regions are there? You should count four in total. Draw a third line that intersects both of the first two, but not at the point where the first two intersect. (That is, there should be three intersection points, in total.) How many regions are there? Can you predict the answer before counting? What happens when there are 4 lines? Or 5? Or 100? How do we approach this puzzle and, ultimately, solve it? Let\u2019s give a more formal statement to be sure we\u2019re thinking the same way:\n\nConsider $n$ lines on an infinite plane (two-dimensional surface) such that no two lines are parallel and no more than two lines intersect at one point. How many distinct regions do the lines create?\n\nWe can draw a few examples by hand when $n$ is small (up to, say, $n = 5$ is reasonable), and let\u2019s use this to guide our intuition into making a general argument for an arbitrary value of $n$. (Notice that this strategy is very similar to what we did in the previous puzzle: identify a pattern with small cases, identify the relevant components of those cases that can generalize, then abstract to an arbitrary case.) Specifically, we want to attempt to identify how the number of regions in one drawing depends on the number of regions in a drawing with fewer lines. What happens when we draw a new line? Can we determine how this changes the already existing regions? Can we somehow count how many regions this creates? Do some investigation of this puzzle on your own before reading on. If you figure out some results, compare your work to the steps we follow below.\n\nLet\u2019s start with a small case, say $n = 2$. We know one line divides a plane into 2 regions; what happens when we add a second line? We know we get 4 regions, because we can just look and count them:\nHowever, we are only looking at one specific case of two intersecting lines. How do we know that we will always find four regions, no matter how we draw those two lines appropriately? That is, can we describe how this happens in a way that somehow incorporates the fact that the number of lines is $n = 2$? Think about it!\n\nHere's our approach. Notice that each of the already existing regions is split into two when we add a second line, and that this is true no matter how you choose to draw the lines; as long as we make sure the two lines aren't parallel, they will always behave this way. That is, if we take one line that splits the plane into two regions,\n\nthen adding a new line will split each of those existing regions in two. This adds two new regions to the whole plane, giving four regions in total:\n\nWhat about when $n = 3$? In this case, we want to think about adding a third line to a diagram with two lines and four regions. We want to make an argument that doesn\u2019t depend on a particular arrangement of the lines, so eventually the only facts we should use are that no lines are parallel and any point of intersection only lies on two lines (not three or more). For now, though,\nit helps to look at a particular arrangement of lines so that we are talking about the same diagram; we can use our observations of this specific diagram to guide our general argument. Let\u2019s start with a two-line diagram, on the left below, and add a third line, but let\u2019s choose the third line so that all of the intersection points are \u201cnearby\u201d or within the scope of the diagram, so that we don\u2019t have to rescale the picture:\n\nWe certainly have 7 regions now, but we made the third line a separate color so that we can identify where the \u201cnew\u201d regions appear: one region (the lower quadrant, Region 4) remains unchanged, but the three other regions are split in two and each of those \u201csplits\u201d adds 1 to our count (where there was 1 region, now there are 2). What if we had placed the line differently?\n\nThe same phenomenon occurs, where one quadrant remains untouched but the other three are split in two. (How do we know there aren\u2019t any other regions not depicted within the scale of our diagram? This is not as easy a question to answer as you might think at first blush, and it\u2019s worth thinking about.) Experiment with other arrangements of the three lines and try to convince yourself that this always happens; also, think about why this is the case and how we could explain that this must happen. Before giving a general explanation, though, let\u2019s examine another small case.\n\nWhen $n = 4$, we start with three lines and 7 regions and add a fourth line that is not parallel to any of the existing lines and doesn\u2019t pass through any existing intersection points. Again, we will want to make an argument that isn\u2019t tied to a specific arrangement of the lines, but looking at the following specific diagram will help guide our intuition into making that argument:\nNotice that three of the original regions remain unchanged (Regions 3 and 5 and 7), and the other four are split in two. Do you notice a pattern here? It seems like for every \\( n \\) we\u2019ve examined, adding the \\( n \\)-th line leaves exactly \\( n - 1 \\) regions unchanged while the rest are split in two. Let\u2019s try to explain why this happens. Remember that we\u2019re trying to identify how many regions appear when we draw \\( n \\) lines, so let\u2019s assign that value a \u201cname\u201d so we can refer to it; let\u2019s say \\( R(n) \\) represents the number of regions created by drawing \\( n \\) lines on the plane so that no two lines are parallel and no intersection point belongs to more than two lines. In these examples we\u2019ve considered for small values of \\( n \\), we\u2019ve looked at what changes when we add a new line; that is, we\u2019ve figured out what \\( R(n) \\) is by already knowing \\( R(n-1) \\). Let\u2019s try to adapt our observations so that they apply to any arbitrary value of \\( n \\).\n\nAssume that we know \\( R(n) \\) already. (Why can we do this? Do we know any particular value of \\( R(n) \\) for sure, for some specific \\( n \\)? Which? How?) Say we have an arbitrary diagram of \\( n \\) lines on the plane that satisfy the two conditions given in the puzzle statement above. How many regions do these lines create? Yes, exactly \\( R(n) \\). Now, what happens when we add the \\((n+1)\\)-th line? What can we say for sure about this line and how it alters the diagram? Well, the only information we really have is that (a) this new line is not parallel to any of the existing \\( n \\) lines and (b) this new line does not intersect any of the already existing intersection points. Now, condition (a) tells us that this new line must intersect all of the existing \\( n \\) lines; parallel lines don\u2019t intersect, and non-parallel lines must intersect somewhere. Thus, we must create \\( n \\) new intersection points on the diagram. Can any of those intersection points coincide with any existing intersection points? No! This is precisely what condition (b) tells us. These two pieces of information together tell us that, no matter how we draw this new line, as long as it satisfies the requirements of the puzzle, we must be able to identify \\( n \\) \u201cspecial\u201d points along that line. Those special points are precisely the points where the new line intersects an existing line.\n\nWe\u2019d now like to take these special points and use them to identify new regions in the diagram. Look back to the cases we examined above: identify the new intersection points and see if you can associate them with new regions. Perhaps it would help to label those intersections with a large dot and mark the new regions with an \\( X \\) to make them all stand out. We\u2019ll show you one example below, where \\( n = 4 \\). What do you notice? Can you use these dots to\nhelp identify how many new regions are created with the addition of that \\( n \\)-th line? Think about this for a minute and then read on.\n\nExactly! Between any two of the new intersection points, we have a line segment that splits a region in two! All that remains is to identify how many new such segments we\u2019ve created. Since each one corresponds to exactly one existing region split in two, this will tell us exactly how many new regions we\u2019ve created. We\u2019ve already figured out that this \\((n + 1)\\)-th line creates \\( n \\) new intersection points. Think about how these points are arranged on the line. Any two \u201cconsecutive\u201d points create a segment, but the extreme points also create infinite segments (that continue past those extreme points forever). How many are there in total? Exactly \\( n + 1 \\). (Look at the diagram above, for \\( n = 3 \\). We see that there are 3 new intersection points and 4 new segments, with two of them being infinite rays.) This means there are \\( n + 1 \\) line segments that split regions in two, so we have created exactly \\( n + 1 \\) new regions! This allows us to say that\n\n\\[\nR(n + 1) = R(n) + n + 1\n\\]\n\nPhew, what an observation! It took a bit of playing around with examples and making some geometric arguments, but here we are. We\u2019ve identified some inductive structure to this puzzle; we\u2019ve found how one case depends on another one. That is, we\u2019ve found how \\( R(n+1) \\) depends on \\( R(n) \\). This hasn\u2019t completely solved the puzzle, but we are now much closer. All that remains is to replace \\( R(n) \\) with a similar expression, and continually do this until we reach a value we know, \\( R(1) = 2 \\). Observe:\n\n\\[\n\\begin{align*}\nR(n + 1) &= \\\\\n&= R(n) + n + 1 \\\\\n&= R(n-1) + (n - 1) + n + n + 1 \\\\\n&= \\cdots \\\\\n&= R(1) + 2 + 3 + \\cdots + n + n + 1\n\\end{align*}\n\\]\n\nSince we know \\( R(1) = 2 \\), we can say\n\n\\[\nR(n + 1) = 2 + (2 + 3 + \\cdots + n + (n + 1)) = 2 + \\left( \\sum_{k=1}^{n+1} k \\right) - 1 = 1 + \\sum_{k=1}^{n+1} k\n\\]\nand this is a sum we have investigated before! (Also notice that we had to subtract 1 because of the missing first term of the sum in parentheses.) Recall that \\( \\sum_{k=1}^{n} k = \\frac{n(n+1)}{2} \\), and to represent the sum we have in the equation above, we just replace \\( n \\) with \\( n + 1 \\). Therefore,\n\n\\[\nR(n + 1) = 1 + \\frac{(n + 1)(n + 2)}{2}\n\\]\n\nOne final simplification we would like to make is to replace \\( n + 1 \\) with \\( n \\) throughout the equation, because it makes more sense to have an expression for \\( R(n) \\) (For what values of \\( n \\) is this valid?)\n\n\\[\nR(n) = 1 + \\frac{n(n + 1)}{2}\n\\]\n\nFinally, we have arrived at an answer to the originally-posed puzzle! In so doing, we employed an inductive technique: we explained how one \u201cfact\u201d, namely the value of \\( R(n + 1) \\), depends on the value of a \u201cprevious fact\u201d, namely \\( R(n) \\), and used these iterative dependencies to work backwards until we reached a particular, known value, namely \\( R(1) \\).\n\nWe want to point out, again, that the derivation we followed and the observations we made in this section have guided our intuition into an answer, but this has not rigorously proven anything. The issue is with the \u201c...\u201d, which is not a concrete, \u201cofficially\u201d mathematical way of capturing the inductive process underlying our technique. Furthermore, our method with the \u201clines in the plane\u201d problem had us starting with a diagram of \\( n - 1 \\) lines and building a new diagram with \\( n \\) lines; is this okay? Why does this actually tell us anything about an arbitrary diagram of \\( n \\) lines? Do all such diagrams come from a smaller diagram with one fewer line?\n\nWe will, in the next two chapters, learn the necessary tools to fully describe a rigorous way of doing what we have done so far, and in the chapter after that, we will employ those tools to make mathematical induction officially rigorous. For now, though, we want to give a heuristic definition of induction and continue to examine interesting puzzles and observations that rely on inductive techniques. Practicing with these types of puzzles\u2014learning when to recognize an inductive process, how to work with it, how to use that structure to solve a problem, and so on\u2014will be extremely helpful in the future, and we have no need to delve into technical mathematical detail. (At least, not just yet!)\n\n### 2.2.3 Questions & Exercises\n\n**Remind Yourself**\n\nAnswering the following questions briefly, either out loud or in writing. These are all based on the section you just read, so if you can\u2019t recall a specific definition or concept or example, go back and reread that part. Making sure you can confidently answer these before moving on will help your understanding and memory!\n(1) What properties characterize an *inductive* process?\n\n(2) How did we prove that \\( \\sum_{k=1}^{n} k = \\frac{n(n + 1)}{2} \\) is correct? How was our method inductive? (Reread Section 1.4.2 if you forget!)\n\n(3) Why can we take the sum formula mentioned in the previous question and \u201creplace\u201d \\( n \\) with \\( n + 1 \\) and know that it still holds true? Can we also replace \\( n \\) with \\( n - 1 \\)?\n\n(4) Work through the algebraic steps to obtain our final expression for the sum of the first \\( n \\) squares; that is, verify that\n\n\\[\n\\frac{1}{3}(n + 1)^3 - \\frac{1}{3}(n + 1) - \\frac{n(n + 1)}{2} = \\frac{1}{6}n(n + 1)(2n + 1)\n\\]\n\n(5) Try to recall the argument that adding the \\((n + 1)\\)-th line on the plane created *exactly* \\( n + 1 \\) new regions. Can you work through the argument for a friend and convince him/her that it is valid?\n\n(6) To find the sum of the first \\( n \\) squares, why couldn\u2019t we just square the formula for the sum of the first \\( n \\) numbers? Why is that wrong?\n\n**Try It**\n\nTry answering the following short-answer questions. They require you to actually write something down, or describe something out loud (to a friend/classmate, perhaps). The goal is to get you to practice working with new concepts, definitions, and notation. They are meant to be easy, though; making sure you can work through them will help you!\n\n(1) Draw 5 lines on the plane (that satisfy the two conditions of the puzzle) and verify that there are 16 regions. Can you also verify that 6 lines yield 22 regions?\n\n(2) Come up with another description of a sequence that goes 1, 2, 3, 4, . . . , 100 that is *not* simply all of the numbers from 1 to 100. (Recall the example we gave: all the numbers from 1 to 100 with no \u201ci\u201d in their English spelling.)\n\n(3) Come up with an algebraic expression that relates \\((n + 1)^4\\) to \\(n^4\\), like we did with cubes.\n\n*(Challenge:* Can you come up with a geometric interpretation for the expression you just derived?)\n\n(4) **Challenge:** Let\u2019s bump the \u201clines in the plane\u201d puzzle up one dimension! Think about having \\( n \\) planes in three-dimensional space. How many regions are created? Assume that no two planes are parallel, and no three of them intersect in one line. (Think about how these two conditions are directly analogous to the specified conditions for the \u201clines\u201d puzzle.)\n2.3 Defining Induction\n\nTo properly motivate the forthcoming definition of mathematical induction as a proof technique, we want to emphasize that the above examples used some intuitive notions of the structure of the puzzle to develop a \u201csolution\u201d, where we use quotation marks around solution to indicate that we haven\u2019t officially proven it yet. In that sense, we ask the following question: What if we had been given the formula that we derived and asked to verify it? What if we had not gone through any intuitive steps to derive the formula and someone just told us that it is correct? How could we check their claim? The reason we ask this is because we are really facing that situation now, except the person telling us the formula is . . . the very same intuitive argument we discovered above!\n\nPretend you have a skeptical friend who says, \u201cHey, I heard about this formula for the sum of the first \\( n \\) natural numbers squared. Somebody told me that they add up to \\( \\frac{1}{6}n(n+1)(2n+1) \\). I checked the first two natural numbers, and it worked, so it\u2019s gotta be right. Pass it on!\u201d Being a logical thinker, but also a good friend, you nod along and say, \u201cI did hear that, but let\u2019s make sure it\u2019s correct for every number.\u201d How would you proceed? Your friend is right that the first few values \u201cwork out\u201d nicely:\n\n\\[\n1^2 = 1 = \\frac{1}{6}(1)(2)(3)\n\\]\n\n\\[\n1^2 + 2^2 = 5 = \\frac{1}{6}(2)(3)(5)\n\\]\n\n\\[\n1^2 + 2^2 + 3^2 = 14 = \\frac{1}{6}(3)(4)(7)\n\\]\n\n\\[\n1^2 + 2^2 + 3^2 + 4^2 = 30 = \\frac{1}{6}(4)(5)(9)\n\\]\n\nand so on. We could even check, by hand, a large value of \\( n \\), if we wanted to:\n\n\\[\n1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 + 7^2 + 8^2 + 9^2 + 10^2 = 385 = \\frac{1}{6}(10)(11)(21)\n\\]\n\nRemember, though, that this formula is claimed to be valid for any value of \\( n \\). Checking individual results by hand would take forever, because there are an infinite number of natural numbers. No matter how many individual values of \\( n \\) we check, there will always be larger values, and how do we know that the formula doesn\u2019t break down for some large value? We need a far more efficient procedure, mathematically and temporally speaking, to somehow verify the formula for all values of \\( n \\) in just a few steps. We have an idea in mind, of course (it\u2019s the upcoming rigorous version of mathematical induction), and here we will explain how the procedure works, in a broad sense.\n\n2.3.1 The Domino Analogy\n\nPretend that we have a set of dominoes. This is a special set of dominoes because we have an infinite number of them (!) and we can imagine anything we want\nwritten on them, instead of the standard array of dots. Let\u2019s also pretend that they are set up in an infinite line along an infinite tabletop, and we are viewing the dominos from the side and we can see a label under each one so that we know where we are in the line:\n\nFor this particular example, to verify the formula\n\n\\[\n\\sum_{k=1}^{n} k^2 = \\frac{1}{6} n(n + 1)(2n + 1)\n\\]\n\nwe will imagine a particular \u201cfact\u201d written on each domino. Specifically, we will imagine that the 1st domino has the expression\n\n\\[\n\\sum_{k=1}^{1} k^2 = \\frac{1}{6}(1)(2)(3)\n\\]\n\nwritten on it, and the 2nd domino has the expression\n\n\\[\n\\sum_{k=1}^{2} k^2 = \\frac{1}{6}(2)(3)(5)\n\\]\n\nwritten on it. In general, we imagine that the \\(n\\)-th domino in the infinite line has the following \u201cfact\u201d written on it:\n\n\\[\n\\sum_{k=1}^{n} k^2 = \\frac{1}{6} n(n + 1)(2n + 1)\n\\]\n\nSince we\u2019re dealing with dominos that are meant to fall into each other and knock each other over, let\u2019s pretend that whenever a domino falls, that means the corresponding \u201cfact\u201d written on it is a true statement. This is how we will relate our physical interpretation of the dominos to the mathematical interpretation of the validity of the formula we derived.\n\nWe did check the sum for \\(n = 1\\) by hand: \\(1^2 = \\frac{1}{6}(1)(2)(3)\\). Thus, the fact written on the first domino is a true statement, so we know that the first domino will, indeed, fall over. We also checked the sum for \\(n = 2\\) by hand, so we know that the second domino will fall over:\nHowever, continuing like this brings us back to the same problem as before: we don\u2019t want to check every individual domino to make sure it falls. We would really like to encapsulate our physical notion of the line of dominos\u2014namely, that when a domino falls it will topple into the next one and knock that over, and so on\u2014and somehow relate the \u201cfacts\u201d that are written on adjacent dominos.\n\nLet\u2019s look at this situation for the first two dominos. Knowing that Domino 1 falls, can we guarantee that Domino 2 falls without rewriting all of the terms of the sum? How are the statements written on the two dominos related? Each statement is a sum of squared natural numbers, and the one on the second domino has exactly one more term. Thus, knowing already that Domino 1 has fallen, we can use the true statement written on Domino 1 to verify the truth of the statement written on Domino 2:\n\n\\[\n\\sum_{k=1}^{2} k^2 = 1^2 + 2^2 = 1 + 2^2 = 5 = \\frac{1}{6}(2)(3)(5)\n\\]\n\nNow, this may seem a little silly because the only \u201cwork\u201d we have saved is not having to \u201cdo the arithmetic\u201d to write \\(1^2 = 1\\). Let\u2019s use this procedure on a case with larger numbers so we can more convincingly illustrate the benefit of this method. Let\u2019s assume that Domino 10 has fallen. (In case you are worried about this assumption, we wrote the full sum a few paragraphs ago and you can verify it there.) This means we know that\n\n\\[\n\\sum_{k=1}^{10} k^2 = \\frac{1}{6}(10)(11)(21) = 385\n\\]\n\nis a true statement. Let\u2019s use this to verify the statement written on Domino 11, which is\n\n\\[\n\\sum_{k=1}^{11} k^2 = \\frac{1}{6}(11)(12)(23)\n\\]\n\nThe sum written on Domino 11 has 11 terms, and the first 10 are exactly the sum written on Domino 10! Since we know something about that sum, let\u2019s just separate that 11th term from the sum and apply our knowledge of the other\nterms:\n\n\\[\n\\sum_{k=1}^{11} k^2 = (1^2 + 2^2 + \\cdots + 10^2) + 11^2 \\\\\n= \\left( \\sum_{k=1}^{10} k^2 \\right) + 11^2 \\\\\n= 385 + 121 \\\\\n= 506 \\\\\n= \\frac{1}{6} \\cdot 3036 = \\frac{1}{6}(11)(12)(23)\n\\]\n\nLook at all of the effort we saved! Why bother reading the first 10 terms of the sum if we know something about them already?\n\nNow, imagine if we could do this procedure for all values of \\( n \\), simultaneously! That is, imagine that we could prove that any time Domino \\( n \\) falls, we are guaranteed that Domino \\( (n + 1) \\) falls. What would this tell us? Well, think about the infinite line of dominos again. We know Domino 1 will fall, because we checked that value by hand. Then, because we verified the \u201cDomino \\( n \\) knocks over Domino \\( (n + 1) \\)\u201d step for all values of \\( n \\), we know Domino 1 will fall into Domino 2, which in turn falls into Domino 3, which in turns falls into Domino 4, which . . . The entire line of dominos will fall! In essence, we could collapse the whole line of dominos falling down into just two steps:\n\n(a) Make sure the first domino topples;\n\n(b) Make sure every domino knocks over the one immediately after it in line.\n\nWith only these two steps, we can guarantee every domino falls and, therefore, prove that all of the facts written on them are true. This will prove that the formula we derived is valid for every natural number \\( n \\).\n\nWe have already accomplished step (a), so now we have to complete step (b). We have done this for specific cases in the previous paragraphs (Domino 1 topples Domino 2, and Domino 10 topples Domino 11), so let\u2019s try to follow along with the steps of those cases and generalize to an arbitrary value of \\( n \\). We assume, for some specific but arbitrary value of \\( n \\), that Domino \\( n \\) falls, which tells us that the equation\n\n\\[\n\\sum_{k=1}^{n} k^2 = \\frac{1}{6} n(n + 1)(2n + 1)\n\\]\n\nis a true statement. Now, we want to relate this to the statement written on Domino \\( (n + 1) \\) and apply the knowledge given in the equation above. Let\u2019s do what we did before and write a sum of \\( n + 1 \\) terms as a sum of \\( n \\) terms plus the last term:\n\n\\[\n\\sum_{k=1}^{n+1} k^2 = 1^2 + 2^2 + \\cdots + n^2 + (n + 1)^2 = \\left( \\sum_{k=1}^{n} k^2 \\right) + (n + 1)^2\n\\]\nNext, we can apply our assumption that Domino $n$ has fallen (which tells us that the fact written on it is true) and write\n\n$$\\sum_{k=1}^{n+1} k^2 = \\frac{1}{6} n(n + 1)(2n + 1) + (n + 1)^2$$\n\nIs this the same as the fact written on Domino $(n + 1)$? Let\u2019s look at what that is, first, and then compare. The \u201cfact\u201d on Domino $(n + 1)$ is similar to the fact on Domino $n$, except everywhere we see \u201c$n$\u201d we replace it with \u201c$n + 1$\u201d:\n\n$$\\sum_{k=1}^{n+1} k^2 = \\frac{1}{6} (n + 1)((n + 1) + 1)(2(n + 1) + 1) = \\frac{1}{6} (n + 1)(n + 2)(2n + 3)$$\n\nIt is not clear yet whether the expression we have derived thus far is actually equal to this. We could attempt to simplify the expression we\u2019ve derived and factor it to make it \u201clook like\u201d this new expression, but it might be easier to just expand both expressions and compare all the terms. (This is motivated by the general idea that expanding a factored polynomial is far easier than recognizing a polynomial can be factored.) For the first expression, we get\n\n$$\\frac{1}{6} n(n + 1)(2n + 1) + (n + 1)^2 = \\frac{1}{6} (2n^2 + 3n + 1) + (n^2 + 2n + 1)$$\n\n$$= \\frac{1}{3} n^3 + \\frac{1}{2} n^2 + \\frac{1}{6} n + n^2 + 2n + 1$$\n\n$$= \\frac{1}{3} n^3 + \\frac{3}{2} n^2 + \\frac{13}{6} n + 1$$\n\nand for the second expression, we get\n\n$$\\frac{1}{6} (n + 1)(n + 2)(2n + 3) = \\frac{1}{6} (n + 1)(2n^2 + 7n + 6)$$\n\n$$= \\frac{1}{6} \\left[(2n^3 + 7n^2 + 6n) + (2n^2 + 7n + 6)\\right]$$\n\n$$= \\frac{1}{3} n^3 + \\frac{3}{2} n^2 + \\frac{13}{6} n + 1$$\n\nLook at that; they\u2019re identical! Also, notice how much easier this was than trying to rearrange one of the expressions and \u201cmorph\u201d it into the other. We proved they were identical by manipulating them both and finding the same expression, ultimately. Now, let\u2019s look back and assess what we have accomplished:\n\n1. We likened proving the validity of the formula\n\n$$\\sum_{k=1}^{n} k^2 = \\frac{1}{6} n(n + 1)(2n + 1)$$\n\nfor all values of $n$ to knocking over an infinite line of dominos.\n2. We verified that Domino 1 will fall by checking the formula corresponding to that case by hand.\n\n3. We proved that Domino \\( n \\) will fall into Domino \\((n+1)\\) and knock it over by assuming the fact written on Domino \\( n \\) is true and using that knowledge to show that the fact written on Domino \\((n+1)\\) must also be true.\n\n4. This guarantees that all dominos will fall, so the formula is true for all values of \\( n! \\)\n\nAre you convinced by this technique? Do you think we\u2019ve rigorously proven that the formula is valid for all natural numbers \\( n \\)? What if there were a value of \\( n \\) for which the formula didn\u2019t hold? What would that mean in terms of our domino scheme?\n\nRemember that this domino analogy is a good intuitive guide for how induction works, but it is not built on mathematically rigorous foundations. That will be the goal of the next couple of chapters. For now, let\u2019s revisit the other example we\u2019ve examined in this section: lines in the plane. Again, the use of ellipses in our derivation of the formula \\( R(n) \\) is troublesome and we want to avoid it. Let\u2019s try to follow along with the domino scheme in the context of this puzzle.\n\nImagine that we have defined the expression \\( R(n) \\) to represent the number of distinct regions in the plane created by \\( n \\) lines, where no two lines are parallel and no three intersect at one point. Also, imagine that on Domino \\( n \\) we have written the \u201cfact\u201d that \\( R(n) = 1 + \\frac{n(n+1)}{2} \\). Can we follow the same steps as above and verify that all the dominos will fall?\n\nFirst, we need to check that Domino 1 does, indeed fall. This amounts to verifying the statement: \\( R(1) = 1 + \\frac{1(2)}{2} = 1 + 1 = 2 \\). Is this a true statement? Yes, of course, we saw this before; one line divides the plane into two regions.\n\nSecond, we need to prove that Domino \\( n \\) will topple into Domino \\((n+1)\\) for any arbitrary value of \\( n \\). That is, let\u2019s assume that \\( R(n) = 1 + \\frac{n(n+1)}{2} \\) is a true statement for some value of \\( n \\) and show that \\( R(n+1) = 1 + \\frac{(n+1)(n+2)}{2} \\) must also be a true statement. How can we do this? Well, let\u2019s follow along with the argument we used before to relate \\( R(n+1) \\) to \\( R(n) \\). By considering the geometric consequences of adding an extra line to any diagram with \\( n \\) lines (that also fit our rules about the lines) we proved that \\( R(n+1) = R(n) + n + 1 \\).\n\nUsing this knowledge and our assumption about Domino \\( n \\) falling, we can say that\n\n\\[\nR(n+1) = R(n) + n + 1 = 1 + \\frac{n(n+1)}{2} + n + 1\n\\]\n\nIs this the same expression as what is written on Domino \\((n+1)\\)? Again, let\u2019s simplify both expressions to verify they are the same. We have\n\n\\[\n1 + \\frac{n(n+1)}{2} + n + 1 = 2 + n + \\frac{n^2 + n}{2} = \\frac{1}{2}n^2 + \\frac{3}{2}n + 2\n\\]\n\nand\n\n\\[\n1 + \\frac{(n+1)(n+2)}{2} = 1 + \\frac{n^2 + 3n + 2}{2} = \\frac{1}{2}n^2 + \\frac{3}{2}n + 2\n\\]\nLook at that; they\u2019re identical! Thus, we have shown that Domino $n$ is guaranteed to fall into Domino $(n + 1)$, for any value of $n$. Accordingly, we can declare that all dominos will fall!\n\nThink about the differences between what we have done with this \u201cdomino technique\u201d and what we did before to derive the expressions we just proved. Did we use any ellipses in this section? Why is it better to prove a formula this way? Could we have used the domino induction technique to derive the formulas themselves?\n\n### 2.3.2 Other Analogies\n\nThe Domino Analogy is quite popular, but it\u2019s not the only description of how induction works. Depending on what you read, or who you talk to, you might learn of a different analogy, or some other kind of description altogether. Here, we\u2019ll describe a couple that we\u2019ve heard of before. It will help solidify your understanding of induction (at least as far as we\u2019ve developed it) to think about how these analogies are all equivalent, fundamentally.\n\n#### Mojo the Magical, Mathematical Monkey\n\nImagine an infinite ladder, heading straight upwards into the sky. There are infinitely-many rungs on this ladder, numbered in order: 1, 2, 3, and so on going upwards. Our friend Mojo happens to be standing next to this ladder. He is an intelligent monkey, very interested in mathematics but also a little bit magical, because he can actually climb up this infinite ladder!\n\nIf Mojo makes it to a certain rung on the ladder, that means the fact corresponding to that number is True. How can we make sure he climbs up the entire ladder? It would be inefficient to check each rung individually. Imagine that: we would have to stand on the ground and make sure he got to Rung 1, then we would have to look up a bit and make sure he got to Rung 2, and then Rung 3, and so on . . . Instead, let\u2019s just confirm two details with Mojo before he starts climbing. Is he going to start climbing? That is, is he going to make it to Rung 1? If so, great! Also, are the rungs close enough together so that he can always reach the next one, no matter where he is? If so, even greater! These are exactly like the conditions established in our Domino Analogy. To ensure that Mojo gets to every rung, we just need to know he gets to the first one and that he can always get to the next one.\n\n#### Doug the Induction Duck\n\nMeet Doug. He\u2019s a duck. He also loves bread, and he\u2019s going to go searching through everyone\u2019s yards to find more bread. These yards are all along Induction Street in Math Town, where the houses are numbered 1, 2, 3, and so on, all in a row.\n\nDoug starts in the yard of house #1, looking for bread. He doesn\u2019t find any, so he\u2019s still hungry. Where else can he look? The house next door, #2, has a\nbackyard, too! Doug heads that way, his tummy rumbling. He doesn\u2019t find any bread there, either, so he has to keep looking. He already knows house #1 has no bread, so the only place to go is next door to house #3. We think you see where this is going . . .\n\nIf we were keeping track of Doug\u2019s progress, we might wonder whether he eventually gets to every yard. Let\u2019s say we also knew ahead of time that nobody has any bread. This means that whenever he\u2019s in someone\u2019s yard, he will definitely go to the next house, still searching for a meal. This means that he will definitely get to every house! That is, no matter which house we live in, no matter how large the number on our front door might be, at some point we will see Doug wandering around our backyard. (Unfortunately, he will go hungry all this time, though! Poor Doug.)\n\n2.3.3 Summary\n\nLet\u2019s reconsider what we\u2019ve accomplished with the two example puzzles we\u2019ve seen thus far, and the analogies we\u2019ve given. In our initial consideration of each puzzle, we identified some aspect of the structure of the puzzle where a \u201cfact\u201d depended on a \u201cprevious fact\u201d. In the case of the cubic numbers, we found a way to express \\((n + 1)^3\\) in terms of \\(n^3\\); in the case of the lines in the plane, we described how many regions were added when an extra line was added to a diagram with \\(n\\) lines. From these observations, we applied this encapsulated knowledge over and over until we arrived at a \u201cfact\u201d that we knew, for a \u201csmall\u201d value of \\(n\\) (in both cases, here, \\(n = 1\\)). This allowed us to derive a formula or equation or expression for a general fact that should hold for any value of \\(n\\).\n\nThis work was interesting and essential for deriving these expressions, but it was not enough to prove the validity of the expressions. In doing the work described above, we identified the presence of an inductive process and utilized its structure to derive the expressions in question. This was beneficial in two ways, really; we actually found the expressions we wanted to prove and, by recognizing the inductive behavior of the puzzle, we realized that proving the expressions by mathematical induction would be prudent and efficient.\n\nFor the actual \u201cproof by induction\u201d, we followed two main steps. First, we identified a \u201cstarting value\u201d for which we could check the formula/equation by hand. Second, we assumed that one particular value of \\(n\\) made the corresponding formula hold true, and then used this knowledge to show that the corresponding formula for the value \\(n + 1\\) must also hold true. Between those two steps, we could rest assured that \u201call dominos will fall\u201d and, therefore, the formulas would hold true for all relevant values of \\(n\\).\n\nOne Concern: What\u2019s at the \u201ctop\u201d of the ladder?\n\nYou might be worried about something, and we\u2019ll try to anticipate your question here. (We only bring this up because it\u2019s a not uncommon observation to make. If you weren\u2019t thinking about this, try to imagine where the idea would come from.) You might say, \u201cHey now, I think I see how Mojo is climbing the ladder,\nbut how can he actually get all the way to the top? It\u2019s an infinite ladder, right?\nAnd he never gets there . . . right?\u201d\n\nIn a way, you would be right. Since this magical ladder really does go on forever, then there is truly no end to it and Mojo will never get \u201cthere\u201d. However, that isn\u2019t the point; we don\u2019t care about any \u201cend\u201d of the ladder (and not just because there isn\u2019t one). We just need to know that Mojo actually gets to every possible rung. He doesn\u2019t have to surpass all of them and stand at the top of the ladder, looking down at where he came from. That wasn\u2019t the goal!\n\nThink of it this way: pretend you have a vested interest in some particular fact that we\u2019re proving. Let\u2019s say it\u2019s Fact #18,458,789,572,311,000,574,003. (Some huge number. It doesn\u2019t matter, really.) Its corresponding rung is waaayyyyyy up there on the ladder, and all you care about is whether or not Mojo gets there on his journey. Does he? . . . You bet he does! It might take a long time (how many steps will it take?), but in this magical world of monkeys and ladders, who cares about time anyway! You know that he\u2019ll eventually get there, and that makes you happy. Now, just imagine that for each fact, there\u2019s somebody out there in that magical world that cares about only that fact. Surely, everyone will be happy with the knowledge that Mojo will get to their rung on his journey. Nobody cares about whether he gets to the top; that isn\u2019t their concern. Meanwhile, out here in our regular, non-magical world, we are extremely happy with the fact that everyone in that world will eventually be happy. That entire infinite process of ladder-climbing was condensed into just two steps, and with only those two steps, we can rest assured that every rung on that ladder will be touched. Every numbered fact is true.\n\nThink about this in terms of the Domino Analogy, as well. Do we care whether or not there is some \u201cending point\u201d of the line of dominoes, so that they all fall into a wall somewhere? Of course not; the line goes on forever. Every domino will eventually fall over, and we don\u2019t even care how \u201clong\u201d that takes. Likewise, we know Doug will get to everyone\u2019s yard; we don\u2019t care \u201cwhen\u201d he gets to any individual yard, just that he gets to all of them.\n\n### 2.3.4 Questions & Exercises\n\n**Remind Yourself**\n\nAnswering the following questions briefly, either out loud or in writing. These are all based on the section you just read, so if you can\u2019t recall a specific definition or concept or example, go back and reread that part. Making sure you can confidently answer these before moving on will help your understanding and memory!\n\n1. How are the Domino, Mojo, and Doug analogies all equivalent? Can you come up with some \u201cfunction\u201d that describes their relationship, that converts one analogy into another?\n\n2. Find a friend who hasn\u2019t studied mathematical induction before, and try to describe it. Do you find yourself using one of the analogies? Was it helpful?\n(3) Why is it the case that our work with the cubes didn\u2019t prove the summation formula? Why did we still need to go through all that work?\n\n(4) Think about the Domino Analogy. Is it a problem that the line of dominoes goes on forever? Does this mean that there are some dominoes that will never fall down? Try to describe what this means in terms of the analogy.\n\nTry It\n\nTry answering the following short-answer questions. They require you to actually write something down, or describe something out loud (to a friend/classmate, perhaps). The goal is to get you to practice working with new concepts, definitions, and notation. They are meant to be easy, though; making sure you can work through them will help you!\n\n(1) Work through the inductive steps to prove the formula\n\n\\[\n\\sum_{k=1}^{n} k = \\frac{n(n + 1)}{2}\n\\]\n\n(2) Work through the inductive steps to prove the formula\n\n\\[\n\\sum_{k=1}^{n} (2k - 1) = n^2\n\\]\n\n(3) Work through the inductive steps to prove the formula\n\n\\[\n\\sum_{k=1}^{n} k^3 = \\left(\\frac{n(n + 1)}{2}\\right)^2\n\\]\n\n(4) Suppose we have a series of facts that are indexed by natural numbers. Let\u2019s use the expression \u201c\\(P(n)\\)\u201d to represent the \\(n\\)-th fact.\n\n(a) If we want to prove every instance is True, for every natural number \\(n\\), how can we do this?\n\n(b) What if we want to prove that only every even value of \\(n\\) makes a True statement? Can we do this? Can you come up with a modification of one of the analogies we gave that would describe your method?\n\n(c) What if we want to prove that only every value of \\(n\\) greater than or equal to 4 makes a True statement? Can we do this? Can you come up with a modification of one of the analogies we gave that would describe your method?\n2.4 Two More (Different) Examples\n\nThis short section serves a few purposes. For one, we don\u2019t want you to get the idea, right away, that induction is all about proving a numerical formula with numbers and polynomials. Induction is so much more useful than that! One of the following examples, in particular, will be about proving some abstract property is true for any \u201csize\u201d of the given situation. You will see how it still falls under the umbrella of \u201cinduction\u201d, but you will also notice how it is different from the previous examples. Furthermore, these examples will illustrate that sometimes we need to know \u201cmore information\u201d to knock over some dominoes. In the previous examples, we only needed to know that Domino $n$ fell to guarantee that Domino $n + 1$ will fall. Here, though, we might have to know about several previous dominoes. After these two examples, we will summarize how this differs from the domino definition given above, and preview a broader definition of the technique of induction, as it applies to these examples.\n\n2.4.1 Dominos and Tilings\n\nThis next example is a little more complicated than the first two. We will still end up proving a certain numerical formula, but the problem is decidedly more visual than just manipulating algebraic expressions. Furthermore, we\u2019ll notice an interesting \u201ckink\u201d in the starting steps, where we have to solve a couple of \u201csmall cases\u201d before being able to generalize our approach. This will be our first consideration of how the technique of induction can be generalized and adapted to other situations.\n\nThe question we want to answer is nicely stated as follows:\n\nGiven a $2 \\times n$ array of squares, how many different ways can we tile the array with dominoes? A tiling must have every square covered by one\u2014and only one\u2014domino.\n\nFor example, the following are proper tilings\n\nwhereas the following are not proper tilings\nAs before, let\u2019s examine the first few cases\u2014where \\( n = 1, 2, 3, \\) and so on\u2014and see if we notice any patterns. Try working with the problem yourself, before reading on, even!\n\nWhen \\( n = 1, \\) we have an array that is exactly the shape of one domino, so surely there is only one way to do this. Let\u2019s use the notation \\( T(n) \\) to represent the number of tilings on a \\( 2 \\times n \\) array. Thus, \\( T(1) = 1. \\)\n\nWhen \\( n = 2, \\) we have a \\( 2 \\times 2 \\) array. Since the orientation of the array matters, we have each of the following distinct tilings. Thus, \\( T(2) = 2. \\)\n\nWhat about when \\( n = 3? \\) Again, we can enumerate these tilings by hand and be sure that we aren\u2019t missing any. We see that \\( T(3) = 3. \\)\n\nOkay, one more case, when \\( n = 4. \\) We see that \\( T(4) = 5. \\)\nCan we start to find a pattern now? Writing out larger arrays will just be tiresome! Let\u2019s think about how we could have used the fact that $T(1) = 1$ to deduce something about $T(2)$ . . . Well, wait a minute . . . We couldn\u2019t, right? There was something fundamentally different about those two cases. Specifically, because dominoes are $2 \\times 1$ in size, the fact that we only added one row to the array didn\u2019t help us.\n\nAlright, let\u2019s consider $n = 3$, then. Could we use the fact that $T(2) = 2$ at all? In this case, yes! Knowing there were two tilings of the $2 \\times 2$ array, we could immediately build two tilings of the $2 \\times 3$ array without much thought. Specifically, we can just append a vertical domino to each of those previous tilings. But we know now that $T(3) = 3$. Where did the third tiling come from? Look at that tiling again and how it compares to the other two. In that third tiling, the dominoes on the right side are horizontal, as opposed to the vertical one in the other two tilings. If we remove those two parallel, horizontal dominoes, we are left with precisely the situation when $n = 1$. Put another way, we can build a tiling of a $2 \\times 3$ array by appending a square of two horizontal dominoes to the right side. In total, then, we have described all of the tilings of a $2 \\times 3$ board in terms of boards of smaller sizes, namely $2 \\times 2$ and $2 \\times 1$:\n\n$$T(2) = 2$$\n\n$$T(1) = 1$$\n\n$$T(3) = 3 = 2 + 1 = T(2) + T(1)$$\n\nNow you might see how the pattern develops! Let\u2019s show you what happens when $n = 4$, how we can construct all of the tillings that make up $T(4)$ by appending a vertical domino to each of the tilings that make up $T(3)$, or by appending two horizontal dominoes to each of the tilings that make up $T(2)$:\nNotice, as well, that no tiling for the $2 \\times 4$ array was produced twice in this way. (Think carefully about why this is true. How can we characterize the two types of tilings in a way that will guarantee they don\u2019t coincide at all?) With this information, we can immediately conclude that $T(4) = T(3) + T(2)$.\n\nFurthermore, we can generalize this argument; nothing was special about $n = 4$, right? For any particular $n$, we can just consider all possible tilings, and look at what happens on the far right-hand side of the array: either we have one vertical domino (which means the tiling came from a $2 \\times (n - 1)$ array) or two horizontal dominoes (which means the tiling came from a $2 \\times (n - 2)$ array). With confidence in this argument, we can conclude that\n\n$$T(n) = T(n - 1) + T(n - 2)$$\n\nfor all of the values of $n$ for which this expression makes sense. What values are those? Remember that we had to identify $T(1)$ and $T(2)$ separately; this argument doesn\u2019t apply to those values. Accordingly, we have to add the restriction $n \\geq 3$ for the equation above to hold true.\n\nWith this information, we can then easily figure out $T(n)$ for any value of $n$, given enough time. We could write a computer program fairly easily, even. It was this inductive argument, though\u2014the pattern that we noticed and our thorough description of why it occurs\u2014that allowed us to make the conclusion in the first place. In this case, too, it just so happens that the value of every term, $T(n)$, depends on the value of two previous terms, $T(n - 1)$ and $T(n - 2)$. This did not happen in our previous examples in this chapter, and it hints at something deeper going on here. Do you see how our previous definition of induction, and the domino analogy, doesn\u2019t exactly apply here anymore? How might you try to amend our analogy to explain this kind of situation? Think\nabout these issues for a bit and then read on. We\u2019ll talk about them more in-depth after the next example.\n\nBy the way, did you notice something interesting about our solution to this example? Do you know any other sequences of numbers that behave similarly? Think about it . . .\n\n2.4.2 Winning Strategies\n\nThis example will be our first induction puzzle that doesn\u2019t prove a numerical formula! It might seem strange to think about that, but it\u2019s true, as you\u2019ll see. This is actually more common in mathematics than you might think, too: a problem or mathematical object might have some underlying inductive structure without depending on something algebraic or arithmetic.\n\nIn fact, we will be discussing a game. It\u2019s a game in the usual sense\u2014there are rules to be followed by two players and there is a clear winner and loser\u2014but it\u2019s also a game in the mathematical sense, where we can formulate the rules and playing situations using mathematical notation and discuss formal strategies in an abstract way. We can even solve the game. This is very different than say, the game of baseball.\n\nLet\u2019s discuss the rules for this game, which we shall call \u201cTakeaway\u201d, for now. There are two players, called $P_1$ and $P_2$. The player $P_1$ goes first every time. The players start with two piles of stones in the middle of a table, each pile containing exactly $n$ stones, where $n$ is some natural number. (To distinguish the different versions of the game, we will say the players are \u201cplaying $T_n$\u201d when there are $n$ stones per pile.) On each player\u2019s move, they are allowed to remove any number of stones from either pile. It is illegal, though, to remove stones from both piles at once. The player who removes the final stone from the piles is the winner.\n\nTry playing Takeaway with some friends. Use pennies or candies or penny candy as stones. Try it for different values of $n$. Try switching roles so you are $P_1$ and then $P_2$. Try to come up with a winning strategy, a method of playing that maximizes your chances of winning. Try to make a conjecture for what happens for different values of $n$. Who is \u201csupposed\u201d to win? Can you prove your claim? Seriously, play around with this game and attempt to prove something before reading on for our analysis thereof. You might be surprised by what you can accomplish!\nAs with the other examples, let\u2019s use some small values of \\( n \\) to figure out what\u2019s really going on, then try to generalize. When \\( n = 1 \\), this game is rather silly. \\( P_1 \\) must empty one pile of its only stone, then \\( P_2 \\) gets the only remaining stone in the other pile. Thus, \\( P_2 \\) wins. (Notice that it doesn\u2019t matter which of the two piles \\( P_1 \\) picks from, \\( P_2 \\) will always get the other one. We might say that \\( P_1 \\) picks the pile on the left \u201cwithout loss of generality\u201d because it doesn\u2019t matter either way; the situations are equivalent, so we might as well say it\u2019s the left pile to have something concrete to say. We will explore this idea of \u201cwithout loss of generality\u201d later on when we discuss mathematical logic, too.)\n\nWhen \\( n = 2 \\), we now have a few cases that might appear. Think about \\( P_1 \\)\u2019s possible moves. Again, they might act on either the left or right pile, but because they\u2019re ultimately identical and we can switch the two piles, let\u2019s just say (without loss of generality) that \\( P_1 \\) removes some stones from the left pile. How many? It could be one or two stones. Let\u2019s examine each case separately.\n\nIf \\( P_1 \\) removes both stones, how should \\( P_2 \\) react? Duh, they should take the other pile, so \\( P_1 \\) probably shouldn\u2019t have made that move in the first place. However, \\( P_1 \\) might not be thinking straight or something and, besides, we need to consider all possible situations here to fully analyze this game. Thus, in this case (the top line in the above diagram) \\( P_2 \\) wins. Okay, that\u2019s the easy situation.\n\nWhat if \\( P_1 \\) removes just one stone from the left pile (the bottom line above)? How should \\( P_2 \\) react? We now have some options:\n\n- If \\( P_2 \\) removes the other stone from the left pile . . . well, \\( P_1 \\) takes the other pile and \\( P_1 \\) wins.\n- If \\( P_2 \\) removes both stones from the right pile . . . well, \\( P_1 \\) takes the last stone from the left pile and \\( P_1 \\) wins.\nHowever, if $P_2$ removes just one stone from the right pile . . .\n\nNow we have exactly the same situation presented by $T_1$, which we already analyzed! It is, again, $P_1$\u2019s move first, so we know what will happen: $P_2$ wins no matter what. If you are player $P_2$, this is obviously the best move: you win no matter how $P_1$ responds!\n\nStepping back for a second, let\u2019s think about what this has shown: no matter what $P_1$ does first (remove one or two stones from either pile), there is some possible response that $P_2$ can make that will guarantee a win for $P_2$, regardless of $P_1$\u2019s subsequent response. Wow, $P_2$ is sitting pretty! Let\u2019s see if this happens for other values of $n$.\n\nWhen $n = 3$, we will again assume (without loss of generality) that player $P_1$ acted on the left pile. They could remove one, two, or three stones:\n\n- If $P_1$ removes all three, $P_2$ responds by taking the other pile completely and wins.\n\n- If $P_2$ removes two stones . . . well, what should player $P_2$ do?\n\nFinishing off that left pile is stupid (because $P_1$ can take the whole right pile and win), and pulling the entire right pile is similarly stupid (because $P_1$ can take the whole left pile and win), so something in between is required. Now, if $P_2$ removes just one stone from the right pile, notice that $P_1$ can respond with the same action; this leaves exactly one stone in both piles, but the roles reversed. With $P_2$ going first in such a situation, they are now bound to lose, per our previous analysis. Bad move, $P_2$!\n\nLet\u2019s try again. If $P_2$ removes two stones from the right pile instead . . . look at that! We now have exactly one stone in each pile, with $P_1$ up first, so we know $P_1$ is going to lose. $P_2$ strikes again!\nThink about the case where $n = 4$ for a minute, and you\u2019ll find the exact same analysis occurring. You\u2019ll another possibility to consider: player $P_1$ can remove one, or two, or three, or four stones from the left pile. Whatever they do, though, you\u2019ll find that $P_2$ can just mimic that action on the other pile, reducing the whole game to a previous, smaller version of the game, where $P_2$ was shown to be guaranteed a win! It looks like $P_2$ is in the driver\u2019s seat the whole time, since they can respond to whatever $P_1$ does, making an identical move on the other pile. No matter what $P_1$ does, there is always a response for $P_2$ that means they win, regardless of $P_1$\u2019s subsequent moves. In this sense, we say \u201c$P_2$ has a winning strategy\u201d. There is a clear and describable method for $P_2$ to assess the game situation and choose a specific move to guarantee a win.\n\nHow might we prove this? How does this even fit into this chapter on induction? It might be hard to see, at the moment. What are we really even proving here? What are the dominoes or rungs in our analogy for this problem? In wrapping your brain around this example, you should hopefully realize the following: induction is not about algebraic formulas all the time; induction represents some kind of \u201cbuilding-up\u201d structure, where larger situations depend on smaller ones; we have to prove some initial fact, and then argue how an arbitrary, larger fact can be reduced so that it depends on a previous fact. This is really what the dominoes analogy is meant to accomplish. It just so happens that this analogy is particularly illustrative for certain induction problems (but not all) and is visualizable and memorable. It does not perfectly apply to all situations, though.\n\nRead back through these four examples from this chapter and think about how they are similar and how they are different. Try to come up with a more precise, mathematical description of mathematical induction using some better terminology, perhaps of your own invention. (By this, we mean something better than our intuitive analogy. You\u2019d be surprised at how well you might be able to describe induction without really knowing what you \u201cought\u201d to say, and you\u2019ll actually learn a lot, in the process!) In due time, we will have a rigorous statement to make, and prove, about mathematical induction and its various forms. In the meantime, we need to take a trip through some other areas of mathematics to build up the necessary language, notation, and knowledge to come back and tackle this problem. Before we go, though, we should mention a few useful applications of mathematical induction.\n2.4.3 Questions & Exercises\n\nRemind Yourself\n\nAnswering the following questions briefly, either out loud or in writing. These are all based on the section you just read, so if you can\u2019t recall a specific definition or concept or example, go back and reread that part. Making sure you can confidently answer these before moving on will help your understanding and memory!\n\n(1) How are these two examples *inductive*? In what ways are they similar to the previous examples, with the cubes and lines? In what ways are they different?\n\n(2) With the domino tilings, how *many* previous values did we need to know to compute $T(n)$?\n\n(3) What is the difference between writing $T(n) = T(n - 1) + T(n - 2)$ and $T(n + 2) = T(n + 1) + T(n)$?\n\n(4) What is the winning strategy in the Takeaway game? Try playing with a friend who doesn\u2019t know the game, and use that strategy as player $P_2$. How frustrated do they get every time you win? Do they start to catch on?\n\nTry It\n\nTry answering the following short-answer questions. They require you to actually write something down, or describe something out loud (to a friend/classmate, perhaps). The goal is to get you to practice working with new concepts, definitions, and notation. They are meant to be easy, though; making sure you can work through them will help you!\n\n(1) What is $T(5)$? Can you draw all of those tilings?\n\n(2) Work through the possibilities for takeaway with two piles of 4 stones. Can you make sure that player $P_2$ always has a winning move?\n\n(3) **Challenge:** What happens if you play Takeaway with *three* piles of equal sizes? Can you find a winning strategy for either player? Try playing with a friend and see what happens!\n\n(4) Look up the *Fibonacci numbers*. How are they related to the sequence of numbers $T(n)$ we found in the domino tiling example?\n\n2.5 Applications\n\n2.5.1 Recursive Programming\n\nThe concepts behind mathematical induction are employed heavily in computer science, as well. Think back to how we first derived the formula for $\\sum_{k=1}^{n} k^2$.\nOnce we had a way to represent a cubic number in terms of a smaller cube and some leftover terms, we repeated this substitution process over and over until we arrived at the \u201csimplest\u201d case, namely, the one that we first observed when starting the problem: $2^3 = 1 + 3 + 3 + 1$. Recursive programming takes advantage of this technique: to solve a \u201clarge\u201d problem, identify how the problem depends on \u201csmaller\u201d cases, and reduce the problem until one reaches a simple, known case.\n\nA classical example of this type of technique is seen in writing code to compute the factorial function, $n!$, which is defined as the product of the first $n$ natural numbers:\n\n$$n! = 1 \\cdot 2 \\cdot 3 \\cdots (n - 1) \\cdot n$$\n\nThis is a simple definition that we, as humans, intuitively understand, but telling a computer how to perform this product doesn\u2019t work quite the same way. (Try it! How do you say \u201cand just keep going until you reach $n$\u201d in computer code?) A more efficient way to program the function, and one that models the mathematically inductive definition, in fact, is to have one program recursively call itself until it reaches that \u201csimple\u201d case. With the factorial function, that case is $1! = 1$. For any other value of $n$, we can simply apply the knowledge that\n\n$$n! = (n - 1)! \\cdot n$$\n\nover and over to compute $n!$. Consider the following pseudocode that represents this idea:\n\n```plaintext\nfactorial(n):\n    if n = 1\n        return 1\n    else\n        return n * factorial(n-1)\n```\n\nWe know that $1! = 1$, so if the program is asked to compute that, the correct value is returned right away. For any larger value of $n$, the program refers to itself and says, \u201cGo back and compute $(n - 1)!$ for me, then I\u2019ll add a factor of $n$ at the end, and we\u2019ll know the answer.\u201d To compute $(n - 1)!$, the program asks, again, if the input is 1; if not, it calls itself and says, \u201cGo back and compute $(n - 2)!$ for me, then I\u2019ll add a factor of $n - 1$ at the end.\u201d This process continues until the program returns $1! = 1$. From there, it knows how to find $2! = 1 \\times 2$, and then $3! = 2! \\times 3$, and so on, until $n! = (n - 1)! \\times n$.\n\nAnother example involving recursive programming arises with the Fibonacci numbers. You may have seen this sequence of numbers before in a mathematics course. (In fact, we even mentioned them in the last section, with the domino tilings!) You also might have heard about how they appear in nature in some interesting and strange ways. (The sequence was first \u201cdiscovered\u201d by the Italian mathematician Leonardo of Pisa while studying the growth of rabbit populations.) The first two numbers in the sequence are specified to be 1, and any\nnumber in the sequence is defined as the sum of the previous two. That is, if we say $F(n)$ represents the $n$-th Fibonacci number, then\n\n$$F(1) = 1 \\quad \\text{and} \\quad F(2) = 1 \\quad \\text{and} \\quad F(n) = F(n - 1) + F(n - 2) \\quad \\text{for every } n \\geq 3$$\n\nNow, what is $F(5)$? Or $F(100)$? Or $F(10000)$? This can be handled quite easily by a recursive program. The idea is the same: if the program refers to either one of the \u201csimple cases\u201d, i.e. $F(1)$ or $F(2)$, then it will know to return the correct value of 1 immediately. Otherwise, it will call itself to compute the previous two numbers and then add those together. Look at the pseudocode below and think about how it works. What would happen if we used this program to compute $F(10)$? How would it figure out the answer?\n\n```plaintext\nFibonacci(n):\n    if n = 1 or n = 2\n        return 1\n    else\n        return Fibonacci(n-1) + Fibonacci(n-2)\n    end\n```\n\nThis follows the same idea as the factorial program above (let the program call itself to compute values for \u201csmaller\u201d cases of the function until we reach a known value) but there\u2019s something a little deeper going on here. If we were to input $n = 10$ into the program, it would recognize that it does not know the output value yet, and it will call itself to compute Fibonacci(9) and Fibonacci(8). In each of those calls to the program, it would again recognize the value is as yet unknown. Thus, it would call upon itself again to compute Fibonacci(8) and Fibonacci(7), but also Fibonacci(7) and Fibonacci(6). That\u2019s right, the program calls itself multiple times with the same input value. To compute $F(9)$, we need to know $F(8)$ and $F(7)$, but meanwhile, to compute $F(8)$, we also need to know $F(7)$ and $F(6)$. In this way, we end up calling the program Fibonacci many times.\n\nTry to compare the programs Fibonacci and factorial, especially in regards to the inductive processes we have been investigating in this chapter. Do they use similar ideas? How do they relate to the \u201cdomino\u201d analogy of mathematical induction that we outlined? Think of the \u201cfact\u201d written on Domino $n$ as being the computation of the correct value of $n!$ or $F(n)$. How does the analogy work in each case? Will all the dominos fall? Keep these questions in mind as you read on. There is some very powerful mathematics underlying all of these ideas.\n\n### 2.5.2 The Tower of Hanoi\n\nLet\u2019s take a short break and play a game. Well, it\u2019s not exactly a break because this is, in a sense, an *inductive* game, so it\u2019s completely relevant. But it is a\ngame, nevertheless! The Tower of Hanoi is a very popular puzzle, partly because it involves such simple equipment and rules. Solving it is another matter, though!\n\nImagine that we have three vertical rods and three disks of three different sizes (colored blue, green, and red) stacked upon each other like so:\n\nThe goal is to move all three disks to another rod (either the middle or the right one, it doesn\u2019t matter) by following these rules:\n\n1. A single move consists of moving one (and only one) disk from the top of the stack on any rod and moving it to the top of the stack on another rod.\n\n2. A disk cannot be placed on top of a smaller disk.\n\nThat\u2019s it! Two simple rules, but a difficult game to play. Try modeling the game with a few coins or playing cards or whatever you have handy. (You can even buy Tower of Hanoi sets at some games stores.) Can you solve it? How many moves did it take you? Is your solution the \u201cbest\u201d one? Why or why not?\n\nWe mentioned that this is an inductive game, so let\u2019s explore that idea now. We want to consider how many moves it takes to solve the puzzle (where one move accounts for moving one disk from one rod to another) and, more specifically, identify the smallest possible number of moves it would take to solve the puzzle. To solve the puzzle with three disks, we could keep moving the smallest disk back and forth between two rods and generate 100 moves, if we wanted to, and then solve it, but that\u2019s certainly not the best way to do it, right? Let\u2019s say we found a way to solve the puzzle in a certain number of moves; how could we show that the number of moves we used is the smallest possible number of moves?\n\nTo address this question, we want to break down the method of solving the puzzle recursively. In doing so, we are actually going to answer a far more general question: What is the smallest number of moves required to solve the Tower of Hanoi puzzle with $n$ disks on 3 rods? We posed the puzzle above with just 3 disks to give you a concrete version to think about and work with, but we can answer this more general question by thinking carefully. To make sure we are on the same page, we will show you how we solved the version with 3 disks:\nNotice that the largest disk is essentially \u201cirrelevant\u201d for most of the solution. Since we are allowed to place any other disk on top of it, all we need to do is \u201cuncover\u201d that disk by moving the other disks onto a different rod, move the largest disk to the only empty rod, then move the other disks on top of the large one. In essence, we perform the same procedure (shifting the two smaller disks from one rod to another) twice and, in between those, we move the large disk from one rod to another. If the largest disk hadn\u2019t been there at all, what we actually did was solve the version of the puzzle with 2 disks, but twice! (Think carefully about this and make sure you see why this is true. Follow along with the moves in the diagrams above and pretend the large, blue disk isn\u2019t there.)\n\nThis shows that the way to solve the 3-disk puzzle involves two iterations of solving the 2-disk puzzle, with one extra move in between (moving the largest disk). This indicates a recursive procedure to solve the puzzle, in general. To optimally solve the $n$-disk puzzle, we would simply follow the procedure to optimally solve the $(n - 1)$-disk puzzle, use one move to shift the largest, $n$-th disk, then solve the $(n - 1)$-disk puzzle again.\n\nNow that we have some insight into how to optimally solve the puzzle, let\u2019s identify how many moves that procedure requires. Recognizing that solving this puzzle uses a recursive algorithm, we realize that proving anything about the optimal solution will require induction. Accordingly, we would need to identify a \u201cstarting point\u201d for our line of dominos, and it should correspond to the \u201csmallest\u201d or \u201csimplest\u201d version of the puzzle. For the Tower of Hanoi, this is the 1-disk puzzle. Of course, this is hardly a \u201cpuzzle\u201d because we can solve it in one move, by simply shifting the only disk from one rod to any other rod. If we let $M(n)$ represent the number of moves required to optimally solve the $n$-disk puzzle, then we\u2019ve just identified $M(1) = 1$. To identify $M(2)$, we can use our observation from the previous paragraph and say that\n\n$$M(2) = M(1) + 1 + M(1) = 1 + 1 + 1 = 3$$\n\nand then it must be that\n\n$$M(3) = M(2) + 1 + M(2) = 3 + 1 + 3 = 7$$\n\nand\n\n$$M(4) = M(3) + 1 + M(3) = 7 + 1 + 7 = 15$$\n\nand so on. Do you notice a pattern yet? Each of these numbers is one less than a power of 2, and specifically, we notice that $M(n) = 2^n - 1$, for each of the cases we have seen thus far. It\u2019s important to point out that observing this pattern doesn\u2019t prove the pattern; just because it works for the first 4 cases does not mean the trend will continue, but that\u2019s exactly what an induction proof would accomplish. Also, recognizing that pattern and \u201cobserving\u201d that $M(n) = 2^n - 1$ is a non-trivial matter, itself. We happened to know the answer and had no problem identifying the formula for you. You should probably try, on your own, to \u201csolve\u201d the following relationship\n\n$$M(n) = 2M(n - 1) + 1 \\quad \\text{and} \\quad M(1) = 1$$\nand see if you can derive the formula \\( M(n) = 2^n - 1 \\). The reason such a formula is nicer than the above relationship is that, now, \\( M(n) \\) depends only on \\( n \\), and not on previous terms (like \\( M(n-1) \\), for example). This relationship and others like it are known as recurrence relations, and they can be rather difficult to solve, in general!\n\nWe know how to solve this one, though, and it yields \\( M(n) = 2^n - 1 \\). We will leave it to you to verify this. You can do so by checking a few values in the equation above, but we all know that isn\u2019t a proof. Try working through the inductive steps to actually prove it! We have already done most of the work, but it will be up to you to arrange everything carefully and clearly. Remember that you should identify what the \u201cfact\u201d on each domino is, ensure that Domino 1 falls, and then make a general argument about Domino \\( n \\) toppling into Domino \\((n+1)\\). Try to write that proof. Do the details make sense to you? Try showing your proof to a friend and see if they understand it. Did you need to tell them anything else or guide them through it? Think about the best way to explain your method and steps so that the written version suffices and you don\u2019t have to add any verbal explanations.\n\n### 2.5.3 Questions & Exercises\n\n#### Remind Yourself\n\nAnswering the following questions briefly, either out loud or in writing. These are all based on the section you just read, so if you can\u2019t recall a specific definition or concept or example, go back and reread that part. Making sure you can confidently answer these before moving on will help your understanding and memory!\n\n1. How is a recursive program inductive?\n2. What is the inductive structure of the Tower of Hanoi? Where did we solve the 2-disk puzzle while solving the 3-disk puzzle?\n\n#### Try It\n\nTry answering the following short-answer questions. They require you to actually write something down, or describe something out loud (to a friend/classmate, perhaps). The goal is to get you to practice working with new concepts, definitions, and notation. They are meant to be easy, though; making sure you can work through them will help you!\n\n1. Follow the steps of the psuedocode `factorial` to compute 5!.\n2. Follow the steps of the psuedocode `Fibonacci` to compute \\( F(5) \\).\n3. Solve the Tower of Hanoi puzzle with 4 disks. Make sure that you can do it in the optimal number of moves, \\( 2^4 - 1 = 15 \\).\n2.6 Summary\n\nWe have now seen some examples of inductive arguments. We realized that some of the puzzles we were solving used similar argument styles, and explored several examples to get a flavor for the different issues that might come up in such arguments. Specifically, we saw how inductive arguments are not always about proving a summation formula or an equation: inductive arguments can apply to any situation where a fact depends on a \u201cprevious instance\u201d of that fact. This led us into developing an analogy for how induction works, mathematically speaking. We are comfortable with thinking of induction in terms of the \u201cDomino Analogy\u201d for now, but one of our main goals in moving forwards is rigorously stating and proving a principle of induction. For now, let\u2019s get lots of practice working with these kinds of arguments. This is what this chapter\u2019s exercises are meant to achieve. Later on, once we\u2019ve formalized induction, we\u2019ll be better off for it, and we\u2019ll have a thorough understanding of the concept!\n\n2.7 Chapter Exercises\n\nHere are some problems to get you comfortable working with inductive-style arguments. We aren\u2019t looking for fully rigorous proofs here, just a good description of what is going on and a write-up of your steps. We\u2019ll come back to some of these later and rigorously prove them, once we\u2019ve established the Principle of Mathematical Induction (PMI) and a corresponding proof strategy.\n\nProblem 2.7.1. Prove the following summation formula holds for every natural number, and for \\( n = 0 \\), as well:\n\n\\[\n\\sum_{i=0}^{n} 2^i = 2^{n+1} - 1\n\\]\n\nFollow-up question: use this result to state how many games are required to determine a winner in a single-elimination bracket tournament with \\( 2^n \\) teams. (For example, the NCAA March Madness Tournament uses this format, with \\( n = 6 \\).)\n\nProblem 2.7.2. Prove that \\( 3^n \\geq 2^{n+1} \\) for every natural number \\( n \\) that is greater than or equal to 2.\n\nProblem 2.7.3. For which natural numbers \\( n \\) do the following inequalities hold true? State a claim and then prove it.\n\n1. \\( 2^n \\geq (n + 1)^2 \\)\n2. \\( 2^n \\geq n! \\)\n3. \\( 3^{n+1} > n^4 \\)\n4. \\( n^3 + (n + 1)^3 > (n + 2)^3 \\)\nProblem 2.7.4. The December 31 Game: Two players take turns naming dates from a calendar. On each turn, a player can increase the month or date but not both. The starting position is January 1, and the winner is the person who says December 31. Determine a winning strategy for the first player.\n\nFor example, a sequence of moves that yields Player 1 winning is as follows:\n\n(1) January 10, (2) March 10, (1) August 10, (2) August 25, (1) August 28, (2) November 28, (1) November 30, (2) December 30, (1) December 31\n\nBy winning strategy we mean a method of play that Player 1 follows that guarantees a win, no matter what Player 2 does.\n\nProblem 2.7.5. Find and prove a formula for the sum of a geometric series, which is a series of the form\n\n$$\\sum_{i=0}^{n-1} q^i$$\n\nfor some real number $q$ and some natural number $n$. (Hint: be careful when $q = 1$.)\n\nProblem 2.7.6. Write a sentence that depends on $n$ such that the sentence is true for all values of $n$ from 1 to 99 (inclusive), but such that the sentence is false when $n = 100$.\n\nProblem 2.7.7. What is wrong with the following \u201cspoo\ufb01ng\u201d of the claim that $a^n = 1$ for every $n$?\n\n\u201cSpoo\ufb01ng\u201d: Let $a$ be a nonzero real number. Notice that $a^0 = 1$. Also, notice that we can inductively write\n\n$$a^{n+1} = a^n \\cdot a = a^n \\cdot \\frac{a^n}{a^{n-1}} = 1 \\cdot \\frac{1}{1} = 1$$\n\n\u201c\u25a1\u201d\n\nProblem 2.7.8. In a futuristic society, there are only two different denominations of currency: a coin worth 3 Brendans, and a coin worth 8 Brendans. There is also a nation-wide law that says shopkeepers can only charge prices that can be paid in exact change using these two coins.\n\nWhat are the legal costs that a shopkeeper could charge you for a cup of coffee?\n\nHint: Try a bunch of small values and see what happens.\n\nProblem 2.7.9. Consider a chessboard of size $2^n \\times 2^n$, for some arbitrary natural number $n$. Remove any square from the board. Is it possible to tile the remaining squares with L-shaped triominoes?\n\nIf your answer is Yes, prove it.\n\nIf your answer is No, provide a counterexample argument. (That is, find an $n$ such that no possible way of tiling the board will work, and show why this is the case.)\nProblem 2.7.10. Consider an $n \\times n$ grid of squares. How many sub-squares, of any size, exist within this grid? For example, when $n = 2$, the answer is 5: there are 4 $1 \\times 1$ squares and 1 $2 \\times 2$ square. Find a formula for your answer and try to prove it is correct.\n\nProblem 2.7.11. Prove that, in a line of at least 2 people, if the 1st person is a woman and the last person is a man, then somewhere in the line there is a man standing immediately behind a woman.\n\nProblem 2.7.12. Prove that $n^3 - n$ is a multiple of 3, for every natural number $n$.\n\nProblem 2.7.13. A **binary $n$-tuple** is an ordered string of 0s and 1s, with $n$ total numbers in the string. Provide an *inductive argument* to explain why there are $2^n$ possible binary $n$-tuples.\n\nProblem 2.7.14. Recall that the **Fibonacci Numbers** are defined by setting $f_0 = 0$ and $f_1 = 1$ and then, for every $n \\geq 2$, setting $f_n = f_{n-1} + f_{n-2}$. This produces the sequence $0, 1, 1, 2, 3, 5, 8, 13, 21, 34, \\ldots$\n\nYou might not know that the Fibonacci Numbers also have a *closed form*; that is, there is a specific *formula* that defines them, in addition to the usual recursive definition given above. Here it is:\n\n$$f_n = \\frac{1}{\\sqrt{5}} \\left[ \\left( \\frac{1 + \\sqrt{5}}{2} \\right)^n - \\left( \\frac{1 - \\sqrt{5}}{2} \\right)^n \\right]$$\n\nProve that this formula is correct for all values of $n \\geq 0$.\n\nProblem 2.7.15. Again, considering the Fibonacci Numbers, $f_n$, prove the following:\n\n1. $\\sum_{i=0}^{n} f_i = f_{n+2} - 1$\n2. $\\sum_{i=0}^{n} f_i^2 = f_n \\cdot f_{n+1}$\n3. $f_{n-1} \\cdot f_{n+1} - f_n^2 = (-1)^n$\n4. $f_{m+n} = f_n \\cdot f_{n+1} + f_{m-1} \\cdot f_n$\n5. $f_n^2 + f_{n+1}^2 = f_{2n+1}$\n\nProblem 2.7.16. Try to provide an inductive argument that explains why every natural number $n \\geq 2$ can be written as a product of prime numbers. Can you also show that this product is *unique*? That is, can you also explain why there is exactly one way to factor a natural number into primes?\n\nProblem 2.7.17. Prove that\n\n$$\\sum_{k=1}^{n} k \\cdot k! = 1 \\cdot 1! + 2 \\cdot 2! + 3 \\cdot 3! + \\cdots + n \\cdot n! = (n + 1)! - 1$$\nProblem 2.7.18. What is wrong with the following \u201cspoof\u201d that all pens have the same color.\n\n\u201cSpoof\u201d: Consider a group of pens with size 1. Since there is only 1 pen, it certainly has the same color as itself.\n\nAssume that any group of $n$ pens has only one color represented inside the group. (Note: we explained why this assumption is valid for $n = 1$ already, so we can make this assumption.) Take any group of $n + 1$ pens. Line them up on a table and number them from 1 to $n + 1$, left to right. Look at the first $n$ of them, i.e. look at pens 1, 2, 3, . . . , $n$. This is a group of $n$ pens so, by assumption, there is only one color represented in the group. (We don\u2019t know what color that is yet.) Then, look at the last $n$ of the pens; i.e. look at pens 2, 3, . . . , $n + 1$. This is also a group of $n$ pens so, by assumption, there is only one color represented in this group, too. Now, pen #2 happens to belong to both of these groups. Thus, whatever color pen #2 is, that is also the color of every pen in both groups. Thus, all $n + 1$ pens have the same color.\n\nBy induction, this shows that any group of pens, of any size, has only one color represented. Looking at the finite collection of pens in the world, then, we should only find one color.\n\nProblem 2.7.19. \u2605 This problem is extremely difficult to analyze, and is taken from the blog of the famous mathematician Terence Tao (link here).\n\nThere is an island upon which a tribe resides. The tribe consists of 1000 people, with various eye colours. Yet, their religion forbids them to know their own eye color, or even to discuss the topic; thus, each resident can (and does) see the eye colors of all other residents, but has no way of discovering his or her own (there are no reflective surfaces). If a tribesperson does discover his or her own eye color, then their religion compels them to commit ritual suicide at noon the following day in the village square for all to witness. All the tribespeople are highly logical and devout, and they all know that each other is also highly logical and devout (and they all know that they all know that each other is highly logical and devout, and so forth).\n\n(For the purposes of this logic puzzle, \u201dhighly logical\u201d means that any conclusion that can logically deduced from the information and observations available to an islander, will automatically be known to that islander.)\n\nOf the 1000 islanders, it turns out that 100 of them have blue eyes and 900 of them have brown eyes, although the islanders are not initially aware of these statistics (each of them can of course only see 999 of the 1000 tribespeople).\n\nOne day, a blue-eyed foreigner visits to the island and wins the complete trust of the tribe.\n\nOne evening, he addresses the entire tribe to thank them for their hospitality.\n\nHowever, not knowing the customs, the foreigner makes the mistake of mentioning eye color in his address, remarking how unusual it is to see another blue-eyed person like myself in this region of the world.\n\nWhat effect, if anything, does this faux pas have on the tribe?\n2.8 Lookahead\n\nIn this chapter, we have introduced you to the concept of mathematical induction. We looked at a few examples of puzzles where an inductive process guided our solution, and then we described how a proof by induction would follow to rigorously verify that solution. With the mathematical techniques and concepts we have at hand thus far, we had to rely on a non-technical analogy to describe this process to you. Thinking of an infinite line of dominos with \u201cfacts\u201d written on them knocking into each other is a perfectly reasonable interpretation of this process, but it fails to represent the full mathematical extent of induction. In a way, it\u2019s like having a friend describe to you how to swing a golf club, even though you\u2019ve never played golf before. Sure, they can provide you with some mental imagery of what a swing \u201cfeels like\u201d, but without getting out there and practicing yourself, how will you truly understand the mechanics of the golf swing? How will you learn how to adapt your swing, or tell the differences between using a driver and a five iron and a sand wedge? By investigating the underlying mechanics and practicing with those concepts, we hope to gain a better understanding of mathematical induction so that, in the future, we can use it appropriately, identify situations where it would be useful, and, eventually, learn how to adapt it to other situations. Of course, it will help to have that domino analogy in mind to guide our intuition, but we should also remember that it is not rigorous mathematics. It also doesn\u2019t perfectly describe the other examples we discussed, where a falling domino depended on not only the one immediately behind it, but several others before it.\n\nIn the next chapter, we will explore some relevant concepts needed to rigorously state and prove mathematical induction as a proof technique. Specifically, we will study some ideas of mathematical logic and investigate how to break down complicated mathematical statements and theorems into their constituent parts, and also how to build interesting and complex statements out of basic building blocks. Along the way, we will introduce some new notation and shorthand that will let us condense some of the wordy statements we make into concise (and precise) mathematical language. With that in hand, we will explore some more fundamental proof strategies, that we will then apply to everything else we do in this course, including the induction technique, itself! We will also study some of the ideas of set theory, a branch of mathematics that forms the foundation for all other branches. This will be extremely useful for organizing our ideas in the future, but it will also help us define the natural numbers in a rigorous manner. With some concepts and knowledge from these two branches of mathematics under our collective belts, we will be able to build mathematical induction on a solid foundation and continue to use it properly.", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/bws_book_ch2.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 48, "total-input-tokens": 84096, "total-output-tokens": 28061, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 1769, 1], [1769, 4495, 2], [4495, 7520, 3], [7520, 9998, 4], [9998, 10939, 5], [10939, 11985, 6], [11985, 12687, 7], [12687, 14013, 8], [14013, 16768, 9], [16768, 19003, 10], [19003, 21446, 11], [21446, 24531, 12], [24531, 25728, 13], [25728, 27498, 14], [27498, 30319, 15], [30319, 32298, 16], [32298, 35032, 17], [35032, 37369, 18], [37369, 39952, 19], [39952, 41453, 20], [41453, 43287, 21], [43287, 45539, 22], [45539, 47511, 23], [47511, 50432, 24], [50432, 53170, 25], [53170, 56257, 26], [56257, 59232, 27], [59232, 60951, 28], [60951, 62848, 29], [62848, 63647, 30], [63647, 65432, 31], [65432, 67316, 32], [67316, 69645, 33], [69645, 71649, 34], [71649, 73413, 35], [73413, 76068, 36], [76068, 78109, 37], [78109, 80801, 38], [80801, 83364, 39], [83364, 85572, 40], [85572, 85572, 41], [85572, 88567, 42], [88567, 91011, 43], [91011, 93192, 44], [93192, 95372, 45], [95372, 97517, 46], [97517, 100605, 47], [100605, 103562, 48]], "primary_language": ["en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en"], "is_rotation_valid": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "rotation_correction": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "is_table": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "is_diagram": [false, false, false, false, false, false, true, false, false, false, false, false, true, true, true, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, true, true, false, false, false, false, true, false, false, false, false, false, false, false]}}
{"id": "7b2a7d81a96e54a09d876acdec5d67ca00124193", "text": "DISCOVERYWORLD: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents\n\nPeter Jansen\u2217\u2021, Marc-Alexandre C\u00f4t\u00e9\u2020, Tushar Khot\u2217 Erin Bransom\u2217, Bhavana Dalvi Mishra\u2217, Bodhisattwa Prasad Majumder\u2217, Oyvind Tafjord\u2217, Peter Clark\u2217\n\u2217Allen Institute for Artificial Intelligence \u2020Microsoft Research \u2021University of Arizona\npeterj@allenai.org\n\nAbstract\n\nAutomated scientific discovery promises to accelerate progress across scientific domains. However, developing and evaluating an AI agent\u2019s capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent\u2019s ability to perform complete cycles of novel scientific discovery. DISCOVERYWORLD contains a variety of different challenges, covering topics as diverse as radioisotope dating, rocket science, and proteomics, to encourage development of general discovery skills rather than task-specific solutions. DISCOVERYWORLD itself is an inexpensive, simulated, text-based environment (with optional 2D visual overlay). It includes 120 different challenge tasks, spanning eight topics each with three levels of difficulty and several parametric variations. Each task requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. DISCOVERYWORLD further provides three automatic metrics for evaluating performance, based on (a) task completion, (b) task-relevant actions taken, and (c) the discovered explanatory knowledge. We find that strong baseline agents, that perform well in prior published environments, struggle on most DISCOVERYWORLD tasks, suggesting that DISCOVERYWORLD captures some of the novel challenges of discovery, and thus that DISCOVERYWORLD may help accelerate near-term development and assessment of scientific discovery competency in agents. Code available at github.com/allenai/discoveryworld.\n\n1 Introduction\n\nA long-standing dream of AI has been to build systems that can perform scientific discovery, potentially leading to new breakthroughs for the benefit of humanity [13]. Recently, with the rise of neural techniques, there have been several successful discovery systems developed for specialized problems such as protein folding [10][15], mathematics [22], and material science [24]. However, while the results have been impressive, these systems (deliberately) bypass the full discovery process of ideation, hypothesis formation, experiment design, etc., and instead (expertly) perform systematic searches over a pre-defined hypothesis space, with pre-defined goals. This raises the question: how much more can be achieved if AI is applied to the broader scientific process? Some works have indeed developed early systems for this, for example, in chemistry [1], and genetics [12]. These systems can also generate hypotheses, design experiments, and execute them (via robotics) in real environments. However, operating in real environments is expensive and complex, creating a barrier\n\n1Released under Apache-2.0 license.\nfor entry. In addition, real environments inevitably encourage a focus on task-specific details, at the potential cost of developing more general discovery skills in an agent.\n\nOur goal is to help remedy these by creating the first virtual discovery environment where solving tasks demands all of the key facets in end-to-end scientific discovery\\(^2\\) and which covers a broad variety of discovery topics. Our approach is to develop a text-based simulated world (with optional 2D visual overlay), called DISCOVERYWORLD, where agents can navigate around, interact with objects in the world, use scientific equipment (measuring devices, tools, etc.), and make observations. Agents can then form hypotheses, plan and execute experiments, and draw conclusions to solve challenge tasks developed for this virtual world. DISCOVERYWORLD tasks are grounded in eight varied topics, such as radioisotope dating, rocket science, and proteomics, to encourage development of agents with general discovery skills rather than hard-wiring to a particular challenge (see Figure 1). The tasks themselves are realistic (but simplified), allowing agents to apply both scientific and commonsense knowledge when attempting them. DISCOVERYWORLD thus provides an environment for exercising and evaluating general-purpose skills in end-to-end AI discovery systems (see Figure 2).\n\nDISCOVERYWORLD is inspired by a growing number of text-based simulation environments\\(^5\\)\\(^{31}\\) inter alia], while also being novel in both its environment and tasks:\n\n- DISCOVERYWORLD tasks are long-horizon, requiring multiple facets of discovery including ideation, experimentation, systematic search, and analysis to be performed to solve a task.\n- The tasks do not suggest a solution approach, instead requiring the agent to ideate and define hypotheses to explore. This contrasts with tasks in many adventure game environments where solution approaches are often more stylistic or constrained.\n- DISCOVERYWORLD is realistic (but simplified) rather than counterfactual, so that background knowledge can be sensibly applied.\n- The tasks cover eight diverse topics, from identifying the cause of space illnesses to reactor tuning, to encourage development of general rather than task-specific solutions.\n\nFinally, automatically evaluating an agent\u2019s progress on a discovery task is itself challenging. We devise a three-part evaluation strategy to help with this, more on this in Section 3.4.\n\nOur contributions are thus:\n\n\\(^2\\)While a precise definition of \"scientific discovery\" is somewhat elusive\\(^17\\)\\(^{19}\\), we here adopt a pragmatic approach: Our goal is to help develop systems that can perform the end-to-end research process that human scientists engage in as they work on a problem, attempt to answer a question, or more generally advance their field of study. We use the term \u201cscientific discovery\u201d here to refer to this process.\nWe introduce the first virtual environment for benchmarking an agent\u2019s general ability to perform complete cycles of novel scientific discovery.\n\n- A comprehensive evaluation set of 120 different tasks for this environment, spanning eight diverse topics, each with three levels of difficulty and several parametric variations.\n- An evaluation framework allowing automatic evaluation of agents in DISCOVERYWORLD.\n- Baseline results for agents in this environment, illustrating that DISCOVERYWORLD captures several novel challenges that contemporary agent models struggle with.\n\nTogether, we hope DISCOVERYWORLD will inspire and accelerate the development of new, general AI discovery agents by the community.\n\n2 Related Work\n\nRecently, several real-world discovery systems have shown success in areas such as genetics (Adam [12], Eve [32]), chemistry (CoScientist [1], ChemCrow [2]) and proteomics (AlphaFold [10], RoseTTAFold [15]). However, these systems are expensive and task-specific. Inspired by these, DISCOVERYWORLD aims to be a broad coverage, virtual environment allowing general scientific research skills to be developed and evaluated. There are many virtual environments that touch on aspects of the discovery process (see Table 1), although DISCOVERYWORLD is the first aimed at the full end-to-end pipeline:\n\nMany physical simulation environments were developed (e.g., for robotics), focusing on object manipulation and navigation. Some are visual/spatial environments (e.g., Al2-Thor [14], ALFRED [25]), while others are textual/symbolic (e.g., TextWorld [5], MiniGrid [4], ALFWorld [26]).\n\nMany game worlds require exploration and discovery, e.g., NetHack [16], MineDojo [6]. However, these operate in counterfactual worlds where good scientific choices are not always rewarded, and are primarily aimed at entertainment.\n\nSome virtual environments (abstractly) cover real world tasks. WebArena [26] simulates Web-based activities, e.g., browsing for a phone-number, on-line shopping, aiming to improve an agent\u2019s task-specific Web navigation skills. Perhaps closest to DISCOVERYWORLD is ScienceWorld [31], a text-based environment for solving simple science quests known to elementary science students, such as \u201cconvert a liquid into a solid\u201d (e.g., put water in the freezer). ScienceWorld requires commonsense object manipulation at the level of an elementary science student, but not ideation or systematic search to complete tasks. In contrast, we show that DISCOVERYWORLD contains discovery tasks that are challenging even for human scientists with PhDs in the natural sciences.\n\nFinally, some environments are explicitly designed to host hypothesis search. Alchemy [28] is a 3D video game to find which mixture of potions transforms a stone into a more valuable form.\nTable 1: A comparison of existing virtual environments with DISCOVERYWORLD. Note that to control for spatial complexity across environments, for the purposes of counting actions, move actions are considered single actions (i.e. move [dir]). Information in this table has been pieced together by looking at the different source materials for each environment (e.g., paper, website, and codebase). These should be taken as our best estimate.\n\n| Environment     | Multi-modal | Domain       | # of Tasks | Parametric | # Obj. Props. | # of Actions | Task Length |\n|-----------------|-------------|--------------|------------|------------|---------------|--------------|-------------|\n| MINIGRID [4]    | Image/Symbol| Pick+Place   | 23         | Yes        | 4             | 4            | 85          |\n| ALFRED [25]     | Image       | Pick+Place   | 6          | Yes        | 20            | 7            | 50          |\n| ALTWORLD [26]   | Text/Image  | Pick+Place   | 6          | Yes        | 16            | 9            | 10          |\n| MINE DOJO [6]   | Image       | Minecraft    | 10\u2020        | Yes        | 256+          | 12           | 100k        |\n| NETHACK LE [16] | Text+Image  | Dungeon      | 1          | Yes        | 69            | 78           | 80k         |\n| ALCHEMY [28]    | Image/Symbol| Chemistry    | 1          | Yes        | 3             | 9            | 200         |\n| IVRE [33]       | Image/Symbol| Hypothesis Testing | 1 | Yes | 3 | 9 | 10 |\n| SCIENCEWORLD [30]| Text       | Elem. Science| 30         | Yes        | 36            | 25           | 100         |\n| DISCOVERYWORLD  | Text+Image  | Sci. Discovery| 24+10     | Yes        | 63            | 14           | 1k          |\n\nTable 2: High-level descriptions of the 8 discovery themes in DISCOVERYWORLD, with full task descriptions (including spoilers) provided in Appendix C.3. It is from these 8 discovery themes \u00d7 3 difficulty levels that we parametrically generate 120 unique instances of discovery tasks.\n\n| Theme            | Description                                                                                                                                                                                                 |\n|------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Proteomics       | Identify which species in a region migrated in the recent past by discovering that one is an outlier in a clustering analysis of protein concentration values. Higher difficulties involve more data dimensions. |\n| Chemistry        | Manufacture a rust removal agent by mixing different concentrations of chemicals then testing those solutions, guided by a hill-climbing signal of decreased rust that can reduce the search space. |\n| Archaeology      | Validate radioisotope dating by correlating radioisotope levels with known artifacts\u2019 ages, choosing the correct radioisotope between several alternatives for dating, then identify the oldest unknown artifact. |\n| Reactor Lab      | Discover a relationship (linear or quadratic) between a physical crystal property (like temperature or density) and its resonance frequency through regression, and use this to tune and activate a reactor. |\n| Plant Nutrients  | Discover that plants on PLANET X prefer specific combinations of nutrients that follow logical rules (e.g. XOR, AND, OR, NOT), then grow plants by setting soil nutrient levels that follow those rules. |\n| Space Sick       | Investigate the cause of a mild and occasional colonist illness in response to eating local food, then formulate and implement a solution so that future colonists no longer contend with this illness. |\n| Rocket Science   | Measure a number of unknown planetary properties (such as the radius and mass of PLANET X), then use provided equations to calculate orbital velocity and propellant needs for a rocket launch. |\n| Translation      | Explore an environment to infer the meanings of words in an unknown language by grounding them to observations of specific objects and actions, then take actions based on the translated utterances. |\n\nSimilarly in IVRE [33], users perform artificial category experiments to identify which blocks are \u201cblickets\u201d. These environments exercise the systematic search part of the discovery pipeline. Likewise, MLAgentBench [8] requires software experiments to solve a challenge (improve a ML algorithm), but in the constrained environment of ML software. In contrast, DISCOVERYWORLD aims to cover a broad range of tasks in a (simulated) physical environment, covering the full end-to-end discovery pipeline.\n\n3 DISCOVERYWORLD Simulator and Environments\n\n3.1 Simulator\n\nEngine: DISCOVERYWORLD is implemented in a custom simulation engine developed from the ground-up to enable building dynamic discovery simulations with a variety of object properties and simulated object behaviors. Every object in DISCOVERYWORLD is constructed from materials with measurable properties, many of which are observable with instruments available in the environment (a list of 60+ frequent object properties is provided in Appendix C.5). The simulator is implemented as approximately 20k lines of PYTHON using the PYGAME framework [21], and provides both an API for developing agents, as well as a graphical user interface for humans to play DISCOVERYWORLD tasks. The API resembles the OPENAI GYM specifications [3,27], such that at each step, the agent\n\n\u2020We only considered programmatic tasks as they can be measured accurately. Then, we only count the (sub)categories since most programmatic tasks are parametric variations of those.\nis provided with an observation from the environment, and must choose a single action to take during that turn from a set of possible actions. An automatic scorer runs in the background, and a given task continues until either it is solved, or the agent/human ends the simulation.\n\n**Environment Space:** All current environments are represented as a $32 \\times 32$ tile grid. As in text game simulators, all objects at a given tile are represented by an object tree [9], where the root node contains objects that are directly on the environment tile (such as a table), while child nodes of each object contain their contents (such as a jar on the table).\n\n**Observations:** Observations in DISCOVERYWORLD can be provided as text, visual output, or both. Text observations are provided as JSON[4] and contain the following: (1) a list of all objects near the agent, their names, text descriptions, and unique identifier numbers; (2) a list of objects in the agent\u2019s inventory; (3) a list of objects the agent can directly interact with (either by being in the agent\u2019s inventory, or directly beside the agent); (4) the agent\u2019s current world location, direction, and directions it can move to (that are not blocked by objects); (5) whether the agent is currently engaged in dialog with another agent, and if so, what pre-determined options it can say; (6) the current task description, and whether or not it has been completed. For tasks that require interacting with other agents (such as Space Sick), we also implemented DiscoveryFeed, a Twitter-like posting environment that reduces the need for an agent to continually visit other agents to check on their status. The most recent DiscoveryFeed posts are included in the observation.\n\nThe visual observation contains a screenshot ($768\\text{px} \\times 512\\text{px}$) similar to the user interface, which provides a $24 \\times 16$ tile view of the world (each tile is $32\\text{px} \\times 32\\text{px}$) centered on the agent (see Fig. 4 in the APPENDIX). This provides information about objects that are farther away than the JSON observation, which is typically limited to a certain distance (configurable, default within 3 tiles) around the agent due to the size (in tokens) of the objects and their associated descriptions.\n\n**Action space:** DISCOVERYWORLD includes 14 possible actions, most of which are common actions such as taking, dropping, or moving objects, opening/closing objects, activating/deactivating objects, using one object on another, and other actions found in other simulated environments [5][50][26]. Each action can take zero (e.g. wait), one (e.g. take seed), or two (e.g. use spectrometer on plant) arguments. A list of all actions is shown in APPENDIX C.1. For agents, we include two additional handicap actions to assist with their (generally) poor abilities for navigation: the ability to teleport to specific task-relevant named locations provided in each task (such as the science lab or cafeteria), and the ability to teleport directly beside any object it is currently (or has previously) observed.\n\n### 3.2 Discovery Task Themes, Difficulty Levels, and Parametric Variations\n\nSpecific tasks in DISCOVERYWORLD are parametrically generated from a set of 24 high-level templates. These templates are divided into 8 discovery task themes with 3 levels of difficulty for each theme. A high-level description of the 8 discovery task themes is shown in Table 2, with full details (including spoilers) in APPENDIX C.3. For each of the 24 {theme $\\times$ difficulty} combinations, the DISCOVERYWORLD simulator is capable of generating a large number of parametric versions of that theme that constitute a particular instance of a task. These parametric variations vary the specific task solution, which typically involves dynamically generating new values for specific object properties, and/or substituting in different task objects. For example, in the Proteomics theme, each parametric variation of a given task generates a different clustering problem, with different data, that points to a different animal as the solution, and places the animals at different starting locations within the environment. Parametric variation generation is deterministic and controlled by a single random seed provided to the task generator. While a large number of specific instantiations of each theme are possible, due to the cost associated with evaluating a large number of tasks, our official benchmark is evaluated on 5 seeds (i.e., random seeds zero through four) for each theme and difficulty level, resulting in a total of $8 \\times 3 \\times 5 = 120$ different game instances or tasks.\n\n### 3.3 Unit Tests\n\nIn addition to the 8 discovery themes, to help disentangle whether a given model\u2019s performance is due to a difficulty in completing normal day-to-day tasks within the environment, versus completing tasks requiring scientific discovery skills in particular, we include 10 additional unit test themes that\n\n---\n\n3Text-based games for research typically render observations as lists of objects [5][50]. Here, we provide the same list-of-object content, but in JSON format.\nTable 3: An example scorecard provided by DISCOVERYWORLD for an instance of a Reactor Lab task, including task completion, task process, and discovered explanatory knowledge scores.\n\n| Scorecard: Rector Lab, Normal Difficulty, Seed 1 | Out of |\n|--------------------------------------------------|-------|\n| Task Completion: Was the task completed successfully? | /1    |\n| Procedural Process:                                |       |\n| P1 The quantum crystals have each been in an agent\u2019s inventory | /4    |\n| P2 Each scientific instrument has been used with at least one crystal | /5    |\n| P3 Each crystal has been examined by the critical instrument | /4    |\n| P4 The resonance frequency of the unknown reactors have been changed | /2    |\n| P5 The resonance frequency of the unknown reactors is correct | /2    |\n| P6 The reactors have been successfully activated | /4    |\n| Total Procedural Score:                           | /25   |\n| Explanatory Knowledge Discovery Questions:        |       |\n| Q1 Does it clearly state that the resonance frequency of the crystals is dependent upon the densitometer reading? | /1    |\n| Q2 Does it clearly state that the relationship is linear, with crystal frequency = (96 * densitometer reading) + 102 | /1    |\n| Total Discovery Knowledge Score:                  | /2    |\n\ntest specific common task competencies. These include a combination of common pick-and-place and navigation tasks (such as those found in ALFWORLD [26], MINIGRID [4], and MINIHAck [23]), as well as DISCOVERYWORLD-themed tasks, such as measuring objects with instruments, or having multi-turn interactions with other agents. The unit test generator is also parametric and capable of generating a large number of specific tasks for each unit test theme. Specific unit test themes are described in detail in APPENDIX C.4.\n\n3.4 Evaluation Metrics\n\nTo evaluate agents progress in DISCOVERYWORLD, we devised three automatic metrics: (1) task completion (a binary metric); (2) a fine-grained report card for each task tracking task-relevant actions, to measure partial performance on relevant discovery procedures; (3) the accuracy of discovered explanatory knowledge with respect to a gold reference. Together these allow an agent\u2019s progress (including partial progress) to be automatically assessed. An example scorecard is shown in Table 3.\n\nTwo of these three metrics (task completion, and task process) are measured automatically by DISCOVERYWORLD. For the third, discovered explanatory knowledge, the scorecard provides specific binary questions to answer with reference to knowledge that an agent has produced (in its explanations, memories, reports, or other knowledge structures). These can either be graded manually by a human, or provided to a language model to grade. DISCOVERYWORLD provides code for automatic grading using OPENAI models, and in our evaluation we make use of GPT-4O, a long-context (128k token) model that allows even large knowledge structures to be graded. Examples of both positive and negative knowledge assessments are provided in APPENDIX E.\n\n4 Experiments, Baseline Agents, and Human Baselines\n\nIn this section we examine the performance of strong baseline agents on each of the DISCOVERYWORLD tasks. In addition, we investigate the performance of human scientists, and highlight the performance gap between current agent models and real human scientists.\n\n4.1 Experimental setup\n\nFor the purposes of this work, we seek to better understand the zero-shot generalization performance of AI agents on tasks that require iterative scientific discovery: that is, coming up with hypotheses, doing in-game experiments to (in)validate them, then arriving at a discovered solution for the task. As such, we evaluate the performance of three contemporary baselines in a zero-shot setting (though single-task, multi-task, or curriculum learning settings with separate training and evaluation sets are possible with this benchmark; see APPENDIX C.2 for these configurations). In the zero-shot setting, an agent has no prior exposure to DISCOVERYWORLD, and is evaluated on all 120 tasks. Each task is evaluated independently, without carry-over knowledge from one task to another.\nTable 4: Baseline model performance on each of the three scoring metrics (task completion, task process, explanatory knowledge discovery) across all 24 DISCOVERYWORLD tasks. Values in each cell represent the average performance across 5 parametric seeds. Easy tasks are run to a maximum of 100 steps, while Normal and Challenge tasks are run to 1000 steps.\n\n| #   | Topic          | Task                          | ReACT Procedure | Completion | Knowledge | Plan+Execute Procedure | Completion | Knowledge | Hypothesizer Procedure | Completion | Knowledge |\n|-----|----------------|-------------------------------|-----------------|------------|-----------|------------------------|------------|-----------|------------------------|------------|-----------|\n|     |                |                               |                 |            |           |                        |            |           |                        |            |           |\n| 1   | Proteomics     | Clustering                    | 0.87            | 0.20       | 0.20      | 0.80                   | 0.00       | 0.00      | 0.90                   | 0.40       | 1.00      |\n| 2   |                | Simplified Clustering         | 0.88            | 0.40       | 0.40      | 0.68                   | 0.20       | 0.00      | 0.93                   | 0.40       | 0.40      |\n| 3   |                | Clustering (2D)               | 0.88            | 0.40       | 0.60      | 0.58                   | 0.20       | 0.00      | 0.93                   | 0.40       | 0.60      |\n| 4   | Chemistry      | Exploring Combinations and Hill Climbing | 0.87            | 0.00       | 0.00      | 0.70                   | 0.60       | 0.40      | 0.90                   | 0.00       | 0.40      |\n| 5   |                | Substances                    | 0.82            | 0.00       | 0.00      | 0.87                   | 0.40       | 0.00      | 0.93                   | 0.00       | 0.40      |\n| 6   |                | Mix of 3 substances           | 0.90            | 0.40       | 0.00      | 0.90                   | 0.40       | 0.00      | 0.87                   | 0.00       | 0.00      |\n| 7   | Archaeology    | Correlations                  | 0.27            | 0.60       | 0.00      | 0.33                   | 0.20       | 0.00      | 0.60                   | 0.20       | 0.50      |\n| 8   |                | Simple instrument             | 0.72            | 0.40       | 0.30      | 0.74                   | 0.00       | 0.00      | 0.64                   | 0.40       | 0.40      |\n| 9   |                | Instrument Use                | 0.46            | 0.20       | 0.00      | 0.46                   | 0.00       | 0.05      | 0.55                   | 0.20       | 0.05      |\n| 10  | Reactor Lab    | Regression                    | 0.42            | 0.00       | 0.40      | 0.44                   | 0.00       | 0.10      | 0.38                   | 0.00       | 0.20      |\n| 11  |                | Slope only                    | 0.44            | 0.00       | 0.20      | 0.49                   | 0.00       | 0.00      | 0.51                   | 0.00       | 0.00      |\n| 12  |                | Linear regression             | 0.43            | 0.00       | 0.20      | 0.39                   | 0.00       | 0.00      | 0.39                   | 0.00       | 0.00      |\n| 13  | Plant Nutrients| Uncovering systems of rules   | 0.80            | 0.20       | 0.20      | 0.70                   | 0.20       | 0.20      | 0.60                   | 0.00       | 0.00      |\n| 14  |                | Simplified rules              | 0.91            | 0.60       | 0.00      | 0.84                   | 0.40       | 0.00      | 0.56                   | 0.00       | 0.00      |\n| 15  |                | Presence rules                | 0.89            | 0.40       | 0.00      | 0.73                   | 0.40       | 0.00      | 0.62                   | 0.00       | 0.00      |\n| 16  | Space Sick     | Open-ended discovery          | 0.78            | 0.60       | 0.00      | 0.68                   | 0.40       | 0.10      | 0.80                   | 1.00       | 0.60      |\n| 17  |                | Single instrument             | 0.58            | 0.00       | 0.13      | 0.45                   | 0.00       | 0.13      | 0.16                   | 0.00       | 0.33      |\n| 18  |                | Multiple instruments          | 0.55            | 0.00       | 0.00      | 0.26                   | 0.00       | 0.00      | 0.20                   | 0.00       | 0.00      |\n| 19  | Rocket Science | Multi-step measurements and applying formulas | 0.33            | 0.00       | 0.00      | 0.53                   | 0.00       | 0.07      | 0.13                   | 0.40       | 0.00      |\n| 20  |                | Look-up variables             | 0.51            | 0.00       | 0.05      | 0.34                   | 0.00       | 0.00      | 0.11                   | 0.00       | 0.00      |\n| 21  |                | Measure 2 variables           | 0.43            | 0.00       | 0.00      | 0.15                   | 0.00       | 0.00      | 0.22                   | 0.00       | 0.03      |\n| 22  | Translation    | Rosetta-stone style linguistic discovery of alien language | 0.40            | 0.40       | 0.20      | 0.30                   | 0.00       | 0.00      | 0.20                   | 0.20       | 0.00      |\n| 23  |                | Single noun                   | 0.20            | 0.00       | 0.00      | 0.68                   | 0.40       | 0.00      | 0.84                   | 0.40       | 0.00      |\n| 24  |                | Noun and verb                 | 0.49            | 0.00       | 0.00      | 0.55                   | 0.20       | 0.05      | 0.15                   | 0.00       | 0.00      |\n|     | Average (Easy) |                               | 0.39            | 0.38       | 0.25      | 0.56                   | 0.18       | 0.11      | 0.56                   | 0.28       | 0.34      |\n|     | Average (Normal)|                              | 0.63            | 0.18       | 0.14      | 0.64                   | 0.18       | 0.02      | 0.58                   | 0.23       | 0.19      |\n|     | Average (Challenge)|                             | 0.63            | 0.18       | 0.10      | 0.50                   | 0.15       | 0.01      | 0.49                   | 0.08       | 0.08      |\n\nTable 5: Baseline model performance on each of the three scoring metrics (task completion, task process, explanatory knowledge discovery) across all 10 unit test tasks. Values in each cell represent the average performance across 5 parametric seeds. Unit tests tasks are run to a maximum of 100 steps.\n\n| #   | Unit Test Topic                                      | ReACT Procedure | Completion | Plan+Execute Procedure | Completion | Hypothesizer Procedure | Completion |\n|-----|------------------------------------------------------|-----------------|------------|------------------------|------------|------------------------|------------|\n| 25  | Multi-turn dialog with an agent                      | 1.00            | 1.00       | 1.00                   | 1.00       | 1.00                   | 1.00       |\n| 26  | Measure an object with an instrument                 | 0.87            | 0.60       | 0.73                   | 0.40       | 1.00                   | 1.00       |\n| 27  | Pick-and-place object                                | 0.90            | 0.80       | 0.80                   | 0.60       | 1.00                   | 1.00       |\n| 28  | Pick-and-give object                                 | 1.00            | 1.00       | 1.00                   | 1.00       | 1.00                   | 1.00       |\n| 29  | Read/discover wall posts                             | 1.00            | 1.00       | 0.80                   | 0.80       | 1.00                   | 1.00       |\n| 30  | Move through doors                                   | 0.58            | 0.20       | 0.25                   | 0.00       | 0.30                   | 0.00       |\n| 31  | Using keys with doors                                | 0.69            | 0.20       | 0.54                   | 0.00       | 0.69                   | 0.00       |\n| 32  | Navigate to a specific room in a house               | 0.20            | 0.20       | 0.20                   | 0.00       | 0.20                   | 0.20       |\n| 33  | Search an environment for an object                  | 0.80            | 0.80       | 0.60                   | 0.60       | 1.00                   | 1.00       |\n| 34  | Interact with a moving agent                         | 0.60            | 0.20       | 0.53                   | 0.00       | 0.53                   | 0.20       |\n|     | Average (Unit Tests)                                 | 0.76            | 0.60       | 0.66                   | 0.44       | 0.77                   | 0.64       |\n\n4.2 Baseline Agent Models\n\nThe baseline agents are described below, with model performance on Discovery tasks shown in Table 4 and performance on Unit Tests shown in Table 5. We use the GPT-4O model for all our agents due to its higher performance and lower cost compared to other models. For space we provide\nTable 6: Expert human scientist performance on DISCOVERYWORLD tasks, as well as average task completion time. Scores represent average performance of up to 11 humans when playing the same seed of a discovery task.\n\n| #   | Topic          | Task     | Procedural | Completion | Knowledge | Avg. Steps | Movement Steps | Action Steps | Prop. Action Steps | # Samples |\n|-----|----------------|----------|------------|------------|-----------|------------|---------------|--------------|-------------------|-----------|\n| 2   | Proteomics     | Normal   | 0.90       | 0.80       | 0.90      | 277        | 262           | 15           | 0.06              | 10        |\n| 3   | Proteomics     | Challenge| 1.00       | 1.00       | 1.00      | 203        | 192           | 11           | 0.05              | 10        |\n| 5   | Chemistry      | Normal   | 0.98       | 0.90       | 0.64      | 369        | 293           | 76           | 0.23              | 10        |\n| 6   | Chemistry      | Challenge| 0.95       | 0.89       | 0.77      | 401        | 324           | 76           | 0.21              | 9         |\n| 8   | Archaeology    | Normal   | 0.92       | 1.00       | 0.91      | 310        | 275           | 35           | 0.14              | 10        |\n| 9   | Archaeology    | Challenge| 0.77       | 0.36       | 0.09      | 276        | 240           | 36           | 0.13              | 11        |\n| 11  | Reactor Lab    | Normal   | 0.78       | 0.60       | 0.36      | 414        | 340           | 74           | 0.18              | 10        |\n| 12  | Reactor Lab    | Challenge| 0.70       | 0.33       | 0.25      | 281        | 236           | 45           | 0.16              | 9         |\n| 14  | Plant Nutrients| Normal   | 0.93       | 1.00       | 0.64      | 365        | 310           | 55           | 0.15              | 10        |\n| 15  | Plant Nutrients| Challenge| 0.88       | 0.70       | 0.32      | 358        | 306           | 52           | 0.16              | 10        |\n| 17  | Space Sick     | Normal   | 0.69       | 0.73       | 0.59      | 2111       | 1958          | 153          | 0.08              | 11        |\n| 18  | Space Sick     | Challenge| 0.60       | 0.11       | 0.11      | 3458       | 2988          | 470          | 0.13              | 9         |\n| 20  | Rocket Science | Normal   | 0.58       | 0.30       | 0.40      | 274        | 240           | 34           | 0.13              | 10        |\n| 21  | Rocket Science | Challenge| 0.57       | 0.11       | 0.33      | 487        | 334           | 153          | 0.36              | 9         |\n| 23  | Translation    | Normal   | 0.79       | 0.73       | 0.77      | 1033       | 948           | 86           | 0.07              | 11        |\n| 24  | Translation    | Challenge| 0.62       | 1.00       | 0.68      | 859        | 794           | 65           | 0.07              | 11        |\n\nAverage (Human) | 0.79 | 0.66 | 0.55 | 717 | 628 | 90 | 0.14 | 10\n\nhigh-level descriptions of each model here, with additional implementation details and run costs provided in APPENDIX D.\n\nReAct: This agent uses the ReAct [35] approach of generating a thought and action at each step given the recent trajectory of thoughts, actions and observations. Each action is executed in the environment and the observation is added to the trajectory. In addition to this trajectory, we also provide the current game state information as text, e.g., nearby objects, teleportable locations, etc. If needed, we trim the trajectory (remove oldest steps first) to fit the prompt within the maximum token limit, which (in practice) included up to the last 40 steps of the trajectory. To evaluate this agent\u2019s discovered knowledge, we evaluate the concatenation of the agent\u2019s \u201cthoughts\u201d across all time steps.\n\nPlan+Exec: Since the ReAct trajectories can be very long and lead to errors due to distractors, we also evaluate a plan-and-execute [29, 34] approach. We use the LLM to generate a plan, and each step of the plan is independently executed using the same ReAct agent as above. Since each plan step is simpler than the task, their ReAct trajectories are much smaller, reducing the distractors. Discovery tasks require an iterative planning approach to adapt to new findings, so we use iterative decomposition [11] to only generate one step of the plan based on the previous planning steps and their success. Discovered knowledge is evaluated as with the ReAct agent from the execution steps.\n\nHypothesizer: This agent resembles the architecture of CLIN [18], with the agent keeping an explicit working memory of running hypotheses and measurements that are updated after taking each action, and conditioning its next actions on the contents of this memory. This explicit knowledge store allows more directly evaluating an agent\u2019s discovered knowledge (with an example of Hypothesizer\u2019s knowledge provided in subsection D.3). The agent similarly maintains a brief plan and running hypothesis, and is prompted to explain how its action progresses the plan to evaluate that hypothesis.\n\n4.3 Human Evaluation\n\nTo compare model performance against human performance, we recruited 11 practicing human scientists to complete the DISCOVERYWORLD tasks, with their performance shown in Table 6. Scientists were recruited on the UPWORK platform, each with: (1) an MSc or PhD in a natural science, (2) self-evaluated comfort and fluency with statistical methods and common software like spreadsheets, (3) comfort and previous experience with 2D top-down games. Additional details regarding human participant experiments are provided in APPENDIX F.\n5 Discussion\n\nHuman Performance  As shown in Table 6, discovery task completion rates varied from tasks that were solved by all participants, to several Challenge difficulty tasks that were solved by only a single participant, highlighting the range of difficulties found across task themes, and range of expertise provided by each scientist. Overall, the average completion rate across tasks was 66%, with 11 of 16 tasks performed by humans having completion rates exceeding 60%. Average knowledge performance was slightly lower at 55%, reflecting that when stuck, the humans sometimes attempted brute force solutions to tasks that may have eventually yielded correct answers (e.g., trying all possible crystal frequencies, if they didn\u2019t use regression to fit the data), but without producing the required explanatory discovery.\n\nAgent Performance  In contrast to human scientists, the baseline agent scientists exhibited poor overall performance, as shown in Table 4. The most performant discovery agent (REACT) completed 38% of easy tasks and 18% of challenge tasks, while the agent that best discovered explanatory knowledge (HYPOTHESIZER) discovered only 34% of gold knowledge in easy tasks, and only 8% in challenge tasks. An analysis of these agents performance on the Unit Test tasks (Table 5) shows moderate overall performance, with completion rates in the 60%+ range, suggesting that current agents are competent at many of the components of scientific discovery \u2013 like measuring objects with instruments \u2013 but currently lack the capacity to perform end-to-end discovery in most DISCOVERYWORLD settings.\n\n6 Conclusion\n\nWe have presented DISCOVERYWORLD, the first virtual environment for developing and benchmarking an agent\u2019s ability to perform end-to-end scientific discovery. Each of the 120 tasks requires an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. We empirically demonstrate that expert human scientists find the challenge tasks in DISCOVERYWORLD difficult but solvable, while strong agent baselines struggle to complete most tasks, or discover critical explanatory knowledge. We hope DISCOVERYWORLD will inspire and accelerate the development of new, general AI discovery agents by the community.\n\nAcknowledgments and Disclosure of Funding\n\nWe thank members of the Aristo and Semantic Scholar teams at the Allen Institute for Artificial Intelligence for their helpful comments and support throughout the development of this work. Game assets (e.g., sprites) used in DISCOVERYWORLD are in large part from the CUTERPQ pack by PixyMoon that was purchased for this work and requires attribution, with some additions or modifications from the authors and OPENAI DALL-E for science-themed content.\n\nReferences\n\n[1] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes. Autonomous chemical research with large language models. Nature, 624:570 \u2013 578, 2023. URL https://api.semanticscholar.org/CorpusID:266432059\n\n[2] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White, and P. Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6:525 \u2013 535, 2023. URL https://api.semanticscholar.org/CorpusID:258059792\n\n[3] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016.\n\n[4] M. Chevalier-Boisvert, B. Dai, M. Towers, R. de Lazcano, L. Willems, S. Lahlou, S. Pal, P. S. Castro, and J. Terry. Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks. CoRR, abs/2306.13831, 2023.\n\n[5] M.-A. C\u00f4t\u00e9, \u00c1. K\u00e1d\u00e1r, X. Yuan, B. A. Kybartas, T. Barnes, E. Fine, J. Moore, M. J. Hausknecht, L. E. Asri, M. Adada, W. Tay, and A. Trischler. TextWorld: A learning environment for\n[6] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang, H. Zhu, A. Tang, D.-A. Huang, Y. Zhu, and A. Anandkumar. MineDojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=rc8o_j8I8PX.\n\n[7] M. J. Hausknecht, P. Ammanabrolu, M.-A. C\u00f4t\u00e9, and X. Yuan. Interactive fiction games: A colossal adventure. In AAAI Conference on Artificial Intelligence, 2019. URL https://api.semanticscholar.org/CorpusID:20265447.\n\n[8] Q. Huang, J. Vora, P. Liang, and J. Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2024. URL https://api.semanticscholar.org/CorpusID:263671541.\n\n[9] Infocom. Learning zil, 1989. URL https://web.archive.org/web/20100807003406/http://xlisp.org/zil.pdf Accessed: 2024-06-05.\n\n[10] J. M. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. \u017d\u00eddek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. Ballard, A. Cowie, B. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska, T. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with alphafold. Nature, 596:583 \u2013 589, 2021. URL https://api.semanticscholar.org/CorpusID:235959867.\n\n[11] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson, P. Clark, and A. Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. In ICLR, 2023.\n\n[12] R. D. King, K. E. Whelan, F. M. Jones, P. G. K. Reiser, C. H. Bryant, S. H. Muggleton, D. B. Kell, and S. G. Oliver. Functional genomic hypothesis generation and experimentation by a robot scientist. Nature, 427:247\u2013252, 2004. URL https://api.semanticscholar.org/CorpusID:4428725.\n\n[13] H. Kitano. Artificial intelligence to win the nobel prize and beyond: Creating the engine for scientific discovery. AI magazine, 37(1):39\u201349, 2016.\n\n[14] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, M. Deitke, K. Ehsani, D. Gordon, Y. Zhu, A. Kembhavi, A. K. Gupta, and A. Farhadi. Al2-THOR: An interactive 3d environment for visual ai. ArXiv, abs/1712.05474, 2017. URL https://api.semanticscholar.org/CorpusID:28328610.\n\n[15] R. Krishna, J. Wang, W. Ahern, P. Sturmfels, P. Venkatesh, I. Kalvet, G. R. Lee, F. S. Morey-Burrows, I. V. Anishchenko, I. R. Humphreys, R. McHugh, D. Vafeados, X. Li, G. A. Sutherland, A. Hitchcock, C. N. Hunter, A. Kang, E. Brackenbrough, A. K. Bera, M. Baek, F. DiMaio, and D. Baker. Generalized biomolecular modeling and design with rosettafold all-atom. bioRxiv, 2023. URL https://api.semanticscholar.org/CorpusID:264039660.\n\n[16] H. K\u00fcttler, N. Nardelli, A. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, and T. Rockt\u00e4schel. The NetHack learning environment. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7671\u20137684. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/files/paper/2020/file/569ff987c643b4bedf504efda8f786c2-Paper.pdf.\n\n[17] P. Langley. Scientific discovery: Computational explorations of the creative processes. MIT press, 1987.\n\n[18] B. P. Majumder, B. Dalvi, P. A. Jansen, O. Tafjord, N. Tandon, L. Zhang, C. Callison-Burch, and P. Clark. CLIN: A continually learning language agent for rapid task adaptation and generalization. ArXiv, abs/2310.10134, 2023. URL https://api.semanticscholar.org/CorpusID:264146262.\n[19] K. Popper. *The logic of scientific discovery*. Routledge, 2005.\n\n[20] A. Prasad, A. Koller, M. Hartmann, P. Clark, A. Sabharwal, M. Bansal, and T. Khot. ADaPT: As-needed decomposition and planning with language models. In *NAACL (Findings)*, 2023.\n\n[21] Pygame. Pygame. https://www.pygame.org/ 2024. Accessed: 2024-06-05.\n\n[22] B. Romera-Paredes, M. Barekatain, A. Novikov, M. Balog, M. P. Kumar, E. Dupont, F. J. R. Ruiz, J. S. Ellenberg, P. Wang, O. Fawzi, P. Kohli, A. Fawzi, J. Grochow, A. Lodi, J.-B. Mouret, T. Ringer, and T. Yu. Mathematical discoveries from program search with large language models. *Nature*, 625:468 \u2013 475, 2023. URL https://api.semanticscholar.org/CorpusID:266223700\n\n[23] M. Samvelyan, R. Kirk, V. Kurim, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. K\u00fcttler, E. Grefenstette, and T. Rockt\u00e4schel. MiniHack the planet: A sandbox for open-ended reinforcement learning research. *ArXiv*, abs/2109.13202, 2021. URL https://api.semanticscholar.org/CorpusID:237274213\n\n[24] A. D. Sendek, E. D. Cubuk, E. R. Antoniuk, G. Cheon, Y. Cui, and E. J. Reed. Machine learning-assisted discovery of solid li-ion conducting materials. *Chemistry of Materials*, 2018. URL https://api.semanticscholar.org/CorpusID:139501995\n\n[25] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and D. Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020. URL https://arxiv.org/abs/1912.01734\n\n[26] M. Shridhar, X. Yuan, M.-A. C\u00f4t\u00e9, Y. Bisk, A. Trischler, and M. J. Hausknecht. ALFWorld: Aligning text and embodied environments for interactive learning. *ArXiv*, abs/2010.03768, 2020. URL https://api.semanticscholar.org/CorpusID:222208810\n\n[27] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d. Cola, T. Deleu, M. Goul\u00e3o, A. Kallinteris, A. KG, M. Krimmel, R. Perez-Vicente, A. Pierr\u00e9, S. Schulhoff, J. J. Tai, A. T. J. Shen, and O. G. Youmis. Gymnasium, Mar. 2023. URL https://zenodo.org/record/8127025\n\n[28] J. Wang, M. King, N. Porcel, Z. Kurth-Nelson, T. Zhu, C. Deck, P. Choy, M. Cassin, M. Reynolds, F. Song, G. Buttimore, D. Reichert, N. Rabinowitz, L. Matthey, D. Hassabis, A. Lerchner, and M. Botvinick. Alchemy: A structured task distribution for meta-reinforcement learning. *arXiv preprint arXiv:2102.02926*, 2021. URL https://arxiv.org/abs/2102.02926\n\n[29] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In *ACL*, 2023.\n\n[30] R. Wang, P. Jansen, M.-A. C\u00f4t\u00e9, and P. Ammanabrolu. ScienceWorld: Is your agent smarter than a 5th grader? *arXiv preprint arXiv:2203.07540*, 2022.\n\n[31] R. Wang, P. A. Jansen, M.-A. C\u00f4t\u00e9, and P. Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In *Conference on Empirical Methods in Natural Language Processing*, 2022. URL https://api.semanticscholar.org/CorpusID:247451124\n\n[32] K. A. Williams, E. Bilsland, A. Sparkes, W. Aubrey, M. Young, L. N. Soldatova, K. D. Grave, J. Ramon, M. de Clare, W. Sirawaraporn, S. G. Oliver, and R. D. King. Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. *Journal of the Royal Society Interface*, 12, 2015. URL https://api.semanticscholar.org/CorpusID:12002477\n\n[33] M. Xu, G. Jiang, W. Liang, C. Zhang, and Y. Zhu. Interactive visual reasoning under uncertainty, 2023.\n\n[34] J. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. InterCode: Standardizing and benchmarking interactive coding with execution feedback. In *Advances in Neural Information Processing Systems*, volume 36, 2023.\n[35] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing reasoning and acting in language models. *ArXiv*, abs/2210.03629, 2022. URL https://api.semanticscholar.org/CorpusID:252762395\n\n[36] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, U. Alon, and G. Neubig. WebArena: A realistic web environment for building autonomous agents, 2024.\nChecklist\n\n1. For all authors...\n   (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope? [Yes]\n   (b) Did you describe the limitations of your work? [Yes] See Appendix B\n   (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B\n   (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n\n2. If you are including theoretical results...\n   (a) Did you state the full set of assumptions of all theoretical results? [N/A]\n   (b) Did you include complete proofs of all theoretical results? [N/A]\n\n3. If you ran experiments (e.g., for benchmarks)...\n   (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] The URL to the code repository is provided as a footnote on the first page.\n   (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] The details for the baselines is provided in the appendix and the code is available in the code repository.\n   (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A]\n   (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Section D.4 describes the cost associated to running the baselines.\n\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n   (a) If your work uses existing assets, did you cite the creators? [Yes] We mention in the acknowledgements where the image assets came from.\n   (b) Did you mention the license of the assets? [Yes] The code repository contains the relevant license for the image assets and proper attribution was made in the acknowledgements.\n   (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] We are releasing (1) a benchmark environment (code), and (2) anonymized data collected from the human participants (i.e., trajectories and notes).\n   (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? [Yes] For the game assets, it is mentioned in the acknowledgements section. For the human curated data, we provide details in Appendix F.\n   (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] In Appendix F, we explained how the human data is curated, anonymized, and sterilized.\n\n5. If you used crowdsourcing or conducted research with human subjects...\n   (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] Appendix F provides the link to the instructions and includes a screenshot of the interface used by human participants.\n   (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] Appendix F contains a paragraph on potential participant risks.\n   (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [Yes] Appendix F contains a paragraph on participant compensation.\nA Overview\n\nThe supplementary materials is structured as follows:\n\n- **Appendix B**: Limitations and Broader Impacts\n- **Appendix C**: Additional Details on DISCOVERYWORLD. This section covers the Action Space and common Object Properties of DISCOVERYWORLD, the recommended practices for different Experimental Settings, more details on the Discovery Themes and the Unit Tests.\n- **Appendix D**: Additional Baseline Model Details\n- **Appendix E**: Automatic Evaluation of Explanatory Discovery Knowledge\n- **Appendix F**: Human Scientist Participants\nB Limitations and Broader Impacts\n\nSimulation Fidelity: DISCOVERYWORLD is inherently a low-fidelity representation of the physical world, with an abstracted action space. As such, agents that perform well on DISCOVERYWORLD may not necessarily perform well at making discoveries in the real world, given the much larger search and action space. That being said, while the simulated environment is low-fidelity compared to the physical world, the steps of discovery that are simulated (from ideation, hypothesis formation, experiment design and execution, data analysis, and forming conclusions) are common steps in the scientific method regardless of whether those actions are taking place in the real world or a virtual environment.\n\nAgent Cost: The agent models that we run are currently quite costly, ranging from approximately USD$3k-$10k to run for the complete set of 120 tasks in DISCOVERYWORLD. This cost is due in large part to (1) the long (1000+ steps) runtimes, each step requiring at least one LLM call, (2) the large number of individual tasks to evaluate, and (3) the use of performant but costly API-based models that charge per token. We believe that developing inexpensive models that allow for rapid iteration is a clear near-term goal for developing further discovery agents.\n\nSocietal Benefits and Risks: Automated scientific discovery has the potential for broadly positive societal impact by accelerating the pace of scientific progress, potentially decreasing the time it takes for novel discoveries in medicine, chemistry, technology, and other areas of broad impact. If discovery systems are used by individuals or groups with prosocial intentions, there is the potential for broad societal impact. Conversely, if individuals or groups with negative intentions choose to attempt to accelerate discoveries that may be harmful, there is the potential for those individuals or groups to cause harm using automated scientific discovery models.\n\nC Additional Details on DISCOVERYWORLD\n\nTo encourage the community to work on general AI discovery agents, we have open-sourced DISCOVERYWORLD under Apache-2.0 license on GitHub at [github.com/allenai/discoveryworld](https://github.com/allenai/discoveryworld).\n\nC.1 Action Space\n\nThe action space for DISCOVERYWORLD is shown in Table 7.\n\nTable 7: The action space for DiscoveryWorld, which includes 14 main actions. Actions can take zero (e.g. WAIT), one (e.g. TAKE THERMOMETER), or two (e.g. USE SPECTROMETER ON SOIL) arguments. Two handicap actions (*) are available to agents to assist with poor spatial/navigation abilities.\n\n| Action                  | Description          | Action                  | Description          |\n|-------------------------|----------------------|-------------------------|----------------------|\n| MOVE DIR                | Move North           | USE OBJ [ON OBJ]        | Use spectrometer on soil |\n| TAKE OBJ                | Take thermometer     | EAT OBJ                 | Eat mushroom         |\n| DROP OBJ                | Drop seed            | READ OBJ                | Read rocketry book   |\n| PUT/GIVE OBJ TO OBJ     | Put sample in jar    | WAIT                    | Do nothing           |\n| OPEN/CLOSE OBJ          | Open door            | FEED                    | View DiscoveryFeed   |\n| DE/ACTIVATE OBJ         | Activate pendulum    | TELEPORT LOC *          | Teleport to Science Lab |\n| TALK OBJ                | Talk to Colonist     | TELEPORT OBJ *          | Teleport to Microscope |\n\nC.2 Experimental Settings\n\nAs both a development environment and benchmarking tool, we provide the following recommended configurations for evaluating on DISCOVERYWORLD in common experimental settings:\n\nZero-shot: For zero-shot evaluation, an agent should not have prior exposure to DISCOVERYWORLD, and can be evaluated on all 120 tasks. Each task should be evaluated independently, without any carry-over knowledge from one task to another. All of the baseline agent models provided in this work are evaluated zero-shot.\n**Single-Task Learning:** For agents that require training data (e.g., RL agents, few-shot examples, etc.) of highly similar tasks (i.e., within-task designs), following the TEXTWORLD [5], ALFWORLD [26], and SCIENCEWORLD [30] conventions, we recommend the following within-theme design: training on parametric seeds 0 and 1, developing on seed 2, and testing on seeds 3 and 4. In addition, all tasks in the Unit Tests are available for training.\n\n**Multi-Task Learning:** For agents that require training data (e.g., RL agents, few-shot examples, etc.), following the JERICHO benchmark [7] for evaluating text game suites, we recommend training and evaluating across themes. Tasks 1 to 6 (i.e., theme Proteomics and Chemistry) can be used for training, tasks 7 through 12 (Archaeology and Reactor Lab) for development, and the remaining tasks 13 through 24 (Plant Nutrients, Space Sick, Rocket Science, and Translation) for testing. In addition, all tasks in the Unit Tests are available for training.\n\n**Curriculum Learning:** For agents that require training data of growing complexity, irrespective of the experimental setting, we recommend to simply follow the natural progression of the difficulty levels for each theme. In addition, all tasks in the Unit Tests are available for training.\n\n### C.3 Discovery Themes\n\nThe following are descriptions of the 8 main discovery themes, and how these themes vary across each of the 3 levels of difficulty. *Note: This section contains spoilers!*\n\n**Proteomics** This theme examines an agent\u2019s ability to perform analyses that involve clustering and identifying outliers. The agent is told that a number of animal species exist in a remote habitat, with one hypothesized to have migrated from an isolated island in the distant past \u2013 with the agent required to measure the abundance of different proteins (using a protein meter) to determine which species is the outlier. In normal difficulty the clustering problem is two dimensional (that is, values of two proteins are provided for each measured animal), while in challenge difficulty, the clustering problem is three dimensional. Protein concentrations are provided as values between zero and one. The protein concentration data is manufactured such that all animals protein values exist on the surface of a sphere centered on a randomly chosen point, with inliers having a small radius from the center (e.g. 0.1), and outliers having a much farther radius (e.g. 0.4). Inliers and outliers are clearly visible on a plot by inspection. The easy difficulty level reduces the number of animals from 5 to 3, and has the entire task take place in a small lab with unmoving animals, rather than a large map with moving animals.\n\n**Chemistry** The chemistry theme evaluates an agent\u2019s ability to explore a large solution space in the presence of a hill-climbing signal. Here, the agent is told they must manufacture a chemical (a rust removal agent), from some combination of unique chemicals available to them. The agent must enumerate the possible combinations of chemicals, prepare these solutions, and test them on a specific rusted object. After being immersed in a solution, the rusted object will decrease from heavily rusted to moderately or lightly rusted based on the cosine similarity between the chemical concentration of the solution that it is immersed in, and the correct solution, providing a hill-climbing signal for agents to use to more quickly explore the space of possible solutions. The normal difficulty has three possible chemical dispensers, the challenge has four chemical dispensers, and both require agents to find specific concentrations of chemicals (e.g. 1 part Chemical A, 2 parts Chemical B). The easy difficulty has four chemical dispensers, but requires no mixing \u2013 one of the four chemicals is a direct solution.\n\n**Archaeology** The archaeology theme involves measuring (and validating) artifacts using radioisotope dating. In the challenge difficulty, the agent is presented with a dig site that contains six artifacts \u2013 three known, three unknown \u2013 as well as a radioisotope meter that measures 4 different radioisotopes. The agent is prompted that it is unknown if radioisotope dating works on Planet X, and if it does, which of the 4 radioisotopes will be useful for dating. The agent is tasked with finding the oldest unknown artifact, and placing a red flag beside its dig site. The known artifacts include one from the stone age (a stone hammer), one from the bronze age (a bronze chisel), and one from the iron age (iron tongs). The agent must know that in radioisotope dating, older artifacts have lower concentrations of a given radioisotope due to radioactive decay. The agent must also make the critical insight that stone age artifacts are older than bronze age artifacts, and iron age artifacts are younger than bronze.\nage artifacts. The data are manufactured such that one radioisotope follows this pattern, while others don\u2019t (with a correlation of $R^2 < 0.1$ between age and radioisotope value for all incorrect channels). In the normal difficulty, the instrument directly supplies age, and evaluates instrument use rather than instrument validation. The easy difficulty is similar to normal but in a small lab environment.\n\n**Reactor Lab** The reactor lab theme requires agents to discover and use mathematical relationships between measured quantities using regression. The reactor lab contains a number of quantum crystals, each with a specific resonant frequency. The agent must place all crystals into specific slots, and tune each slot to that crystal\u2019s frequency, for the reactor to activate. The frequency depends upon one of five measured properties \u2013 density, temperature, radioactivity, crystal size, or crystal spectra \u2013 which can be measured using instruments in the adjoining science lab. The agent is provided with some number of crystals with known frequencies, and two crystals with unknown frequencies. They must measure each crystal with each of the instruments, determine which physical property is related to the frequency (as well as how), and use this equation to calculate the frequencies of the unknown crystals. In the normal setting, 2 known crystals are provided, and the relationship can be found with linear regression ($y = mx + b$). In the challenge setting, 3 known crystals are provided, and the relationship is quadratic ($y = ax^2 + bx + c$). In the easy setting, only a single (correct) instrument is provided, there is only one unknown crystal, and the relationship requires only inferring the slope ($y = mx$).\n\n**Plant Nutrients** The plant nutrient theme requires agents to infer systems of rules based on observing both positive and negative examples. The agent finds itself at a botanical research center, and given the task of identifying the (unusual) nutrients that plants on PLANET X prefer. The research center includes a \u201cpilot field\u201d where 12 seeds have been planted, some of which sprouted correctly, as well as 3 \u201ctest fields\u201d, each controlled by a soil computer that allows manipulating the nutrients in its soil. The agent must infer the necessary levels of nutrients in the soil from the pilot field, then successfully grow at least two new plants by setting the nutrient levels in a test field to be appropriate, and planting (and growing) seeds in that field. All settings include five different nutrients. In the normal setting, rules are simple presence-at-value rules (i.e. plants require a specific nutrient, at a specific value of either low, medium, or high to grow), and these can be inferred from positive examples alone. In the challenge setting, the rules involve logical relations (e.g. XOR, AND, OR, NOT) between nutrients, and more examples are provided. The easy setting resembles normal except that nutrients are binary (present/absent) rather than varying in concentration.\n\n**Space Sick** The space sick theme requires agents to use open-ended discovery skills to investigate the cause of an illness. The agents are tasked with identifying why some colonists are becoming mildly ill when eating local food, and correcting this. The agent must discover that some food has been contaminated by mold, which can be directly observed with some instruments, and indirectly observed with others (e.g. through elevated spectrometer readings), or indirectly inferred (e.g. cooking food causes it to become safe). Distractors (such as some food being slightly radioactive) lead the agent down paths that do not solve the task. In the normal difficulty, the mold is directly observable using instrumentation. In the challenge difficulty, the agent must discover novel detection instrumentation. In the easy setting, the agent is placed in a lab with 3 samples of food and 4 instruments, and must identify which food sample is contaminated (which is detectable by only a single instrument).\n\n**It\u2019s (not) Rocket Science!** The rocket science theme requires agents to measure quantities in an environment, then substitute these quantities into known equations to complete a task. Here, the agent is tasked with sending a rocket into a specific orbital height around PLANET X. In the challenge version of this task, the agent needs to enter to appropriate orbital velocity, as well as the type (and quantity) of propellant to be used. The agent is provided with a rocketry book containing all the formulas needed to complete the task. However, before it can calculate the required quantities, it needs to infer out how to measure unknown values such as PLANET X\u2019s gravity and radius, as well as properties related to the three available types of propellent such as density, mass flow rate, and thrust when consumed by the rocket engine. This theme involves recalling known experiments performed on Earth and transposing them to PLANET X (e.g., Eratosthenes\u2019 classical experiment to measure Earth\u2019s radius\\footnote{https://en.wikipedia.org/wiki/Eratosthenes#Measurement_of_Earth\u2019s_circumference}). The normal difficulty simplifies the task to requiring only orbital velocity (not propellant), while the easy difficulty tells the agent they are on a planet with a similar mass and radius.\nto a known celestial body (e.g., Earth, Mars, Venus), eliminating the requirement to measure these values of PLANET X.\n\n**Lost in Translation** The translation theme requires agents to infer the meanings of progressively more complicated utterances given access to an environment containing Rosetta-stone-style information. Here, the agent must translate an unknown language used by the native inhabitants of PLANET X. The agent first talks to one of the inhabitants, who speaks an utterance. From this, the agent must explore the village to gather clues about what each word means (e.g., visiting shops will help identifying the name of the items being sold there). In the *challenge* difficulty, instructions are composed of a verb, an amount, a color, and an item name (e.g., bring 3 red flowers). Bringing the correct number of the correct objects back to the inhabitant will complete the task. In the *normal* difficulty, the utterance requires translating only a single specific object. In the *easy* setting, the task is reduced to a small lab setting, where the agent must identify which item to bring to another agent based on its translation (accessible by a sign near the object).\n\n### C.4 Unit Tests\n\n![Figure 3: Example instances of the 10 Unit Test themes.](image)\n\n**C.4.1 Unit Test Environments**\n\nThe unit tests are shown in Figure 3 and described in Table 8.\n\n### C.5 Object Properties\n\nA list of common object properties provided by the base `OBJECT` storage class is shown in Table 9. Note that specific objects (like a `QUANTUMCRYSTAL`) may implement properties not included in the base class (such as `resonanceFrequency`).\nTable 8: High-level descriptions of the 10 unit test themes in DISCOVERYWORLD.\n\nSpecific unit tests tasks are parametrically generated from a given theme.\n\n| #  | Theme                          | Description                                                                                                                                                                                                 |\n|----|--------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 25 | Multi-turn dialog with agent   | The user agent must talk to an NPC agent, and respond correctly to its requests. The NPC agent asks the user agent to select the dialog item that says (for example) \u2018kiwi\u2019, and the user agent must select this dialog option in the dialog tree amongst 8 distractors. The task completes when this process happens successfully 3 times. |\n| 26 | Measure an object with an instrument | The user agent must take a specific item (from 4 possible items), measure a specific property (from 5 possible instruments), and place the item in one of 4 containers depending on the range (e.g., temperature range 0-2.5\u00b0C in Container A, temperature range 2.5-5.0\u00b0C in Container B, etc.). |\n| 27 | Pick-and-place object          | The user agent must take a specific item (from 5 possible items) and place it in a specific container.                                                                                                           |\n| 28 | Pick-and-give object           | The user must take a specific item (from 5 possible items) and give it to a specific NPC agent (from 3 possible agents).                                                                                       |\n| 29 | Read DiscoveryFeed posts       | The user agent must take a specific item (from 5 possible items), and place it in a specific container. The item to take is provided in a DiscoveryFeed post.                                                   |\n| 30 | Move through doors             | The user agent must successfully navigate a randomly generated maze (in the form of a house with walls/doors in random locations), and pick up a flag at the end of the maze. All doors must be opened and the flag must be taken for task success. |\n| 31 | Using keys with doors          | The user agent must successfully navigate a randomly generated house that contains 3 doors, and pick up a flag in the final room. The doors require keys, which are randomly placed near the doors. |\n| 32 | Navigate to a specific room in a house | The user agent must successfully navigate to a specific room in a house (such as the kitchen or bedroom), which it must infer based on the contents of each room. Once in the correct room, it signifies its selection by dropping a flag. |\n| 33 | Search an environment for an object | The user agent must navigate the interior and exterior of a house environment in search of a red flag.                                                                                                         |\n| 34 | Interact with a moving agent   | The user agent must interact with 3 moving NPC agents spread throughout the environment, notifying each that it\u2019s about to rain, and they may want to return home to avoid becoming wet. |\nTable 9: A list of object properties common to the **OBJECT** storage class in **DISCOVERYWORLD**. From this list is omitted any properties tied to a specific object, e.g., the fuel quantity in the rocket.\n\n| #   | Property Name                  | Description                                                                 |\n|-----|--------------------------------|-----------------------------------------------------------------------------|\n| 1   | grid location                  | Initial world location (X, Y)                                              |\n| 2   | isMovable                      | Can it be moved?                                                            |\n| 3   | isPassable                     | Can an agent walk over this?                                               |\n| 4   | obscuresObjectsBelow           | Does it obscure/hide objects on layers below it?                           |\n| 5   | isActivatable                  | Is this a device? (more specifically, can it be activated/deactivated?)    |\n| 6   | isActivated                    | Is this device currently activated?                                        |\n| 7   | isUsable                       | Can this device be used with another object? (e.g., specifically through the 'use' action). |\n| 8   | isDialogable                   | Can it be dialoged with?                                                   |\n| 9   | isShovelable                   | Can it be shoveled?                                                        |\n| 10  | isReadable                     | Can it be read?                                                            |\n| 11  | document                       | Any text to read                                                           |\n| 12  | temperatureC                   | The current object temperature, in Celsius                                 |\n| 13  | heatSourceMaxTemp              | If it is a heat source, then this is the maximum temperature that it can reach |\n| 14  | coolSourceMinTemp              | If it is a cool source, then this is the minimum temperature that it can reach |\n| 15  | isLiving                       | Is it alive?                                                               |\n| 16  | substanceName                  | Name of the substance                                                      |\n| 17  | isSubstance                    | Is it a substance?                                                         |\n| 18  | isAutoReacting                 | Does it react automatically with other substances?                         |\n| 19  | mixtureDict                    | Dictionary of substances and their proportions in the mixture             |\n| 20  | requiresKey                    | If it requires a key to open/use, then this is a special ID for the key. If the value is <=0, then it doesn't require a key. |\n| 21  | keyID                          | If this object acts as a key, here's its ID (0 by default)                 |\n| 22  | radiocarbonAge                 | Radiocarbon dating age (in years). -1 means it's not applicable/inconclusive. |\n| 23  | radioisotopeValues             | Radioisotope values. If empty, then it's not applicable/inconclusive.      |\n| 24  | soilNutrients                  | Soil nutrients. If empty, then it's not applicable/inconclusive.           |\n| 25  | needsNutrientLevels            | For seed/plants: What nutrient levels do they need to grow?                |\n| 26  | antirequirementsNutrientLevels | A list of dictionaries, each containing a list of nutrient levels under which the seed/plant will NOT grow. |\n| 27  | density                        | Object density (in g/cm\u00b3). <=0 means it's not applicable/inconclusive.     |\n| 28  | microscopeModifierText         | Modifier text, if the object is viewed under a microscope. This is a list of strings, which are displayed in the microscope view. |\n| 29  | microscopeDesc                 | Short description to be displayed under a microscope.                      |\n| 30  | color                          | Color description of the object.                                           |\n| 31  | spectrum                       | Spectrum data of the object.                                               |\n| 32  | ph                             | pH value of the object.                                                    |\n| 33  | radiationsvh                   | Radiation level in microsieverts per hour.                                 |\n| 34  | nitrogen                       | Nitrogen content.                                                          |\n| 35  | phosphorus                     | Phosphorus content.                                                        |\n| 36  | potassium                     | Potassium content.                                                         |\n| 37  | cosCanBeLiquid                 | Can it exist in liquid form?                                               |\n| 38  | cosCanBeSolid                  | Can it exist in solid form?                                                |\n| 39  | cosCanBeGas                    | Can it exist in gas form?                                                  |\n| 40  | cosMeltingPointC               | Melting point in Celsius.                                                  |\n| 41  | cosBoilingPointC               | Boiling point in Celsius.                                                  |\n| 42  | cosCombustionPointC            | Combustion point in Celsius.                                               |\n| 43  | livingMinTemp                  | Minimum temperature for living organisms.                                  |\n| 44  | livingMaxTemp                  | Maximum temperature for living organisms.                                  |\n| 45  | isContainer                    | Is it a container?                                                         |\n| 46  | isOpenable                     | If it's a container, can you open/close it?                                |\n| 47  | isOpenContainer                | If it's a container, then is it open?                                      |\n| 48  | containerPrefix                | Container prefix (e.g., \"in\" or \"on\").                                     |\n| 49  | isOpen                         | Closed by default.                                                         |\n| 50  | contentsVisible2D              | If it is a container, do we render the contents in the 2D representation, or is that already handled (e.g., for pots/jars, that render generic contents if they contain any objects). |\n| 51  | contents                       | Contents of the container (other objects).                                 |\n| 52  | parentContainer                | Back-reference for the container that this object is in.                   |\n| 53  | parts                          | List of parts that this object is made of.                                 |\n| 54  | subparts                       | Subparts of the object.                                                    |\n| 55  | isEdible                       | Can it be eaten?                                                           |\n| 56  | isCooked                       | Is it cooked?                                                              |\n| 57  | isPoisonous                    | Is it poisonous?                                                           |\n| 58  | isPassage                      | Is this a passage?                                                         |\n| 59  | materials                      | List of materials that this object is made of.                             |\n| 60  | materialMaterialNames          | A list of material types to add during initialization (in code, rather than from the spreadsheet). |\n| 61  | actionHistory                  | Agent action history, if applicable (None by default).                     |\n| 62  | isAgent                        | Is this object an agent? (e.g., a person).                                 |\n| 63  | isNPC                          | Is this agent an NPC?                                                      |\nD Additional Baseline Model Details\n\nAll models and their implementations are provided in the DISCOVERYWORLD code repository.\n\nD.1 ReAct\n\nThe ReAct agent generates a thought and action at each step given a prompt describing the environment and the previous steps. The prompt contains information about the environment, how to interact with the environment, current environment state, teleportable locations, interactable objects, and valid actions. The previous steps are shown as a sequence of (thought, action, observation)s, e.g.:\n\n```\nHistory of action-observations:\n  Action:\n    ```json\n    {\n      \"action\": \"ROTATE_DIRECTION\",\n      \"arg1\": \"north\",\n      \"thought\": \"I need to face north to approach the door and enter the building.\"\n    }\n    ```\n  Observation:\n    ```json\n    {\"message\": \"I rotated to face north.\", \"errors\": [], \"success\": true}\n    ```\n  Action:\n    ```json\n    {\n      \"action\": \"OPEN\",\n      \"arg1\": 2056,\n      \"thought\": \"I need to open the door in front of me to enter the building.\"\n    }\n    ```\n  Observation:\n    ```json\n    {\"message\": \"I opened the door.\", \"errors\": [], \"success\": true}\n    ```\n```\n\nThe agent generates thoughts and actions using the JSON formatted output as shown above. Each action is executed in the environment and the returned observation is then added to the trajectory. Since tasks in DiscoveryWorld can be extremely long, the trajectories can often exceed the maximum token limit of our LLM. We generally hit the 8K token limit of GPT4-O in 40 steps. To avoid this, we limit the trajectory in the prompt to a maximum of 10K characters by removing the oldest (thought, action, observation)s and replacing it with the string [TRIMMED HISTORY].\n\nD.2 Plan-and-Execute\n\nAnother approach to deal with long trajectories used in prior work [34, 20] is to decompose the problem into a plan of simpler steps and executing each step independently using ReAct. Since it would be challenging to generate the entire plan in one shot without exploring the environment, we use iterative decomposition [11] where we generate one step of the plan, then execute it and based on the result generate the next step. The planner uses the same base LLM with the prompt describing the format of the plan, e.g.,\n\n```\nTask: Open the door and exit the building.\n\nPlan:\nStep 1: Go to the door that leads to the exit\n  -- Task completed! I am at the door that leads to the exit\n```\nStep 2: Open the door\n-- Task failed! Door is locked and I don\u2019t have the key\nStep 3: Find the key to the door\n\nTo execute each step of the plan, we use the same ReAct agent as above but additionally add the previous steps of the plans and their results to the prompt. To prevent the ReAct agent from wasting time on infeasible steps, we add a hint to the prompt to consider returning the Task failed! message (e.g. Step 2 above) when $1/5^{th}$ of the environment step budget is used up. We use the same truncation strategy for the ReAct agent and no truncation is needed for the planner due to the much shorter plans.\n\nD.3 Hypothesizer\n\nThis agent maintains a working memory of science-related knowledge (allowing the knowledge this agent has discovered to be more directly evaluated). To assist in planning and execution, this agent also maintains a running hypothesis for the task solution, as well as a short natural language plan for the immediate steps it should take. More specifically, at each step, HYPOTHESIZER chooses an action based on its current working memory, plan, and running hypothesis. It then reflects on the results of that action, and updates the working memory based on the results. The working memory is science themed, storing two types of records: (1) HYPOTHESES, which include the hypothesis statement, its current status (i.e. whether it has been confirmed, rejected, or is still pending), and a list of supporting evidence the agent has listed as helping make this determination. (2) and MEASUREMENTS, which take the form of specific observations the agent has made of objects in the environment \u2013 for example, that a particular plant appeared to grow in a mixture including a specific nutrient. This memory and reflection is similar to CLIN [18], except: (a) this memory is structured to hold science-domain relations, where CLIN\u2019s holds causal relations, and (b) this memory is updated after each action in the environment, where CLIN\u2019s is updated only after running an entire task to completion. This means that, if an agent takes $N$ steps in an environment, it requires $2N$ LLM calls to accommodate this act-then-reflect cycle. Because the memory can grow large, after every 10 actions, the working memory is summarized through a separate LLM call that requests a maximum of 40 separate hypothesis or measurement entries, and a maximum of $2k$ tokens for the entire memory. When producing actions, HYPOTHESIZER is prompted to produce (1) a specific action, (2) an explanation that includes its running hypothesis, and how this action helps move it towards evaluating this hypothesis, and (3) a short memory describing its plans or any other helpful information it wishes to pass on to the next step. When prompted to produce actions, HYPOTHESIZER is provided with (a) the current observation from the environment, (b) the last 3 actions it took (and their respective explanations and notes), (c) its current working memory, (d) a list of possible actions and which arguments they take. All output is produced as a JSON dictionary.\n\nThe working memory that Hypothesizer stores can be long, especially for longer tasks. An example of the Hypothesizer agent\u2019s working memory at the successful completion of a specific shorter task (Proteomics, Normal, Seed 2) is provided below.\n\nTask Description: You are in a biological preserve on Planet X, that has 5 different animal species. We suspect that one of these animal species is not native to the area, but migrated from an isolated island in the recent past. Your task is to use the proteomics meter to analyze the proteins of each of the 5 animal species, which can be found throughout the environment away from the central statue area, and determine which species is the anomaly. Once you have completed your task, return to the statue area and drop the red flag directly beside the statue of the animal species that is the anomaly.\n\n\"working_memory\": {\n    \"scientific_knowledge\": [\n        {\n            \"measurement\": \"The spheroid (uuid: 23195) is located in Possible Animal Area 1.\",\n            \"step\": 2\n        },\n        {\n            \"measurement\": \"The proteomics meter (uuid: 33276) is now in my inventory.\",\n            \"step\": 8\n        },\n        {\n            \"measurement\": \"I am analyzing the spheroid (uuid: 23195) using the proteomics meter (uuid: 33276).\",\n            \"step\": 10\n        }\n    ]\n}\n\"measurement\": \"The proteomics meter (uuid: 33276) requires two argument objects to be used.\",\n\"step\": 10\n},\n{\n\"hypothesis\": \"if (object:spheroid) is (action:directly in front) then (object:proteomics meter) can be used to analyze its proteins.\",\n\"status\": \"confirmed\",\n\"step\": 13,\n\"supporting evidence\": \"Using the proteomics meter on the spheroid yielded protein data:\nProtein A: 0.87, Protein B: 0.80.\"\n},\n{\n\"measurement\": \"The spheroid (uuid: 23195) has Protein A level of 0.87 and Protein B level of 0.80 when analyzed using the proteomics meter.\",\n\"step\": 13\n},\n{\n\"measurement\": \"The animaplant (uuid: 20030) is located in Possible Animal Area 2.\",\n\"step\": 14\n},\n{\n\"hypothesis\": \"if (object:animaplant) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.\",\n\"status\": \"confirmed\",\n\"step\": 15,\n\"supporting evidence\": \"Using the proteomics meter on the animaplant yielded protein data: Protein A: 0.76, Protein B: 0.67.\"\n},\n{\n\"measurement\": \"The animaplant (uuid: 20030) has Protein A level of 0.76 and Protein B level of 0.67 when analyzed using the proteomics meter.\",\n\"step\": 15\n},\n{\n\"measurement\": \"The animal in Possible Animal Area 3 (uuid: 20031) is not accessible.\",\n\"step\": 17\n},\n{\n\"hypothesis\": \"if (object:animal) is (action:teleported to) using (object:uuid: 20031) then (property:accessibility) will be false.\",\n\"status\": \"confirmed\",\n\"step\": 22,\n\"supporting evidence\": \"Attempting to teleport to the animal object with UUID 20031 resulted in an error indicating that no object with that UUID was found.\"\n},\n{\n\"measurement\": \"There is a tree (big) trunk blocking the path to the west.\",\n\"step\": 25\n},\n{\n\"measurement\": \"I successfully moved south to navigate around the tree (big) trunk.\",\n\"step\": 26\n},\n{\n\"measurement\": \"I successfully moved west to get directly in front of the pink animal.\",\n\"step\": 28\n},\n{\n\"measurement\": \"I rotated to face west, aligning myself with the pink animal.\",\n\"step\": 30\n},\n{\n\"measurement\": \"The object directly in front of me, which I thought was a pink animal, is actually grass (uuid: 20981).\",\n\"step\": 31\n},\n{\n\"measurement\": \"The proteomics meter (uuid: 33276) was used to investigate the grass (uuid: 20981), but the results were inconclusive.\",\n\"step\": 31\n},\n{\n\"measurement\": \"The object with UUID 44261 is a plant, not an animal.\",\n\"step\": 37\n},\n{\n\"measurement\": \"The proteomics meter (uuid: 33276) was used to investigate the animaplant (uuid: 44261). The results are as follows:\nProtein A: 0.76, Protein B: 0.67.\",\n\"The object with UUID 12545 is a plant, not an animal.\",\n\"The proteomics meter (uuid: 33276) was used to investigate the plant (uuid: 12545), but the results were inconclusive.\",\n\"The vortisquid (uuid: 41429) is one of the animal species in the biological preserve.\",\n\"if (object:vortisquid) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.\",\n\"Using the proteomics meter on the vortisquid yielded protein data: Protein A: 0.38, Protein B: 0.46.\"\n\"The vortisquid (uuid: 41429) has Protein A level of 0.38 and Protein B level of 0.46 when analyzed using the proteomics meter.\"\n\"The prismatic beast (uuid: 25282) is located in Possible Animal Area 5.\"\n\"if (object:prismatic beast) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.\"\n\"Using the proteomics meter on the prismatic beast yielded protein data: Protein A: 0.73, Protein B: 0.68.\"\n\"The prismatic beast (uuid: 25282) has Protein A level of 0.73 and Protein B level of 0.68 when analyzed using the proteomics meter.\"\n\"The prismatic beast (uuid: 5059) is located in Possible Animal Area 7.\"\n\"The prismatic beast (uuid: 5059) has Protein A level of 0.73 and Protein B level of 0.68 when analyzed using the proteomics meter.\"\n\"The echojelly (uuid: 4429) is one of the animal species in the biological preserve.\"\n\"if (object:echojelly) is (action:analyzed) using (object:proteomics meter) then (property:protein data) will be obtained.\"\n\"Using the proteomics meter on the echojelly yielded protein data: Protein A: 0.86, Protein B: 0.82.\"\n\"The echojelly (uuid: 4429) has Protein A level of 0.86 and Protein B level of 0.82 when analyzed using the proteomics meter.\"\n\"The vortisquid (uuid: 41429) has significantly different protein levels (Protein A: 0.38, Protein B: 0.46) compared to the other species, indicating it is the anomaly.\"\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is not directly in front of me or in my inventory.\",\n  \"step\": 172\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.\",\n  \"status\": \"pending\",\n  \"step\": 172,\n  \"supporting evidence\": \"Attempting to pick up the red flag from the Instrument Table resulted in errors indicating that the object with UUID 33979 could not be found.\"\n},\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is not directly in front of me or in my inventory.\",\n  \"step\": 173\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.\",\n  \"status\": \"pending\",\n  \"step\": 173,\n  \"supporting evidence\": \"Attempting to pick up the red flag from the Instrument Table resulted in errors indicating that the object with UUID 33979 could not be found.\"\n},\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is located at coordinates (15, 20).\",\n  \"step\": 174\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.\",\n  \"status\": \"confirmed\",\n  \"step\": 174,\n  \"supporting evidence\": \"Successfully teleported to the object with UUID 33979 (red flag) at coordinates (15, 20).\"\n},\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is now in my inventory.\",\n  \"step\": 175\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:picked up) then (property:accessibility) will be true.\",\n  \"status\": \"confirmed\",\n  \"step\": 175,\n  \"supporting evidence\": \"Successfully picked up the red flag after teleporting to it.\"\n},\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is located at coordinates (15, 20).\",\n  \"step\": 176\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:teleported to) using (object:uuid: 33979) then (property:accessibility) will be true.\",\n  \"status\": \"confirmed\",\n  \"step\": 176,\n  \"supporting evidence\": \"Successfully teleported to the object with UUID 33979 (red flag) at coordinates (15, 20).\"\n},\n{\n  \"measurement\": \"The object with UUID 33979 (red flag) is now in my inventory.\",\n  \"step\": 176\n},\n{\n  \"hypothesis\": \"if (object:red flag) is (action:picked up) then (property:accessibility) will be true.\",\n  \"status\": \"confirmed\",\n  \"step\": 176,\n  \"supporting evidence\": \"Successfully picked up the red flag after teleporting to it.\"\n},\n{\n  \"measurement\": \"I teleported to the Statue of a vortisquid at coordinates (18, 17).\",\n  \"step\": 176\n}\nD.4 Cost analysis\n\nAn approximate cost analysis of each baselines model is provided below, in Table 10. Model cost varies across task, environment complexity, and the number of items stored within a model\u2019s memory (e.g. the size of the history in REACT, or the size of the memory in HYPOTHESIZER), so approximate averages are shown.\n\nThe full benchmark contains 8 discovery themes \u00d7 3 difficulty levels \u00d7 5 seeds = 120 tasks to evaluate. Easy tasks are evaluated to 100 steps, while Normal and Challenge tasks are evaluated to 1000 steps or a hard $125 limit, whichever came first. This results in a total upper-bound estimate of $40 \\times 100(\\text{easy}) + 40 \\times 1000(\\text{normal}) + 40 \\times 1000(\\text{challenge}) = 84,000$ steps required to complete an evaluation run, assuming no agents finish early due to task completion.\n\nTable 10: Approximate cost estimate for the models investigated in this work (all using GPT-4o). Assumes a cost of $5/M input tokens, and $15/M output tokens (current pricing as of this writing).\n\n| Model          | Cost per 100 steps | Total cost estimate (120 tasks, 84k steps) |\n|----------------|--------------------|--------------------------------------------|\n| REACT          | $2-4 / 100 steps   | $3,360                                     |\n| PLAN + EXECUTE | $3-4 / 100 steps   | $3,360                                     |\n| HYPOTHESIZER   | $10 / 100 steps    | $8,400                                     |\n\nE Automatic Evaluation of Explanatory Discovery Knowledge\n\nThe following prompt is used to automatically evaluate explanatory discovery knowledge, in reference to gold reference questions provided automatically by the DISCOVERYWORLD task scorer:\n\nYour task is to evaluate the knowledge of an agent playing a game about scientific discovery, to see if they have figured out the discovery required to solve the task. I will supply you with: (1) the task description, (2) the agent\u2019s knowledge, and (3) a single critical question about the discovery, that you should answer with either true (1) or false (0).\n\nTask Description:\n```\n[*INSERT TASK DESCRIPTION HERE*]\n```\n\nAgent\u2019s Knowledge:\n```\n[*INSERT AGENT\u2019S KNOWLEDGE HERE*]\n```\n\nCritical Question:\n```\n[*INSERT SINGLE KNOWLEDGE EVALUATION QUESTION HERE FROM SCORECARD*]\n```\n\nPlease answer this question by responding \u20181\u2019 if the agent\u2019s knowledge reflects having discovered the information in the critical question, and \u20180\u2019 otherwise. This response should be in the \u2018evaluation\u2019 key of the response. The response format is a JSON dictionary containing three keys: \u2018criticalQuestion\u2019, \u2018evaluation\u2019, and \u2018explanation\u2019.\n{\n    \"criticalQuestion\": \"repeat the critical question\",\n    \"evaluation\": 0 or 1 (as integers),\n    \"explanation\": \"provide a brief explanation for evaluation, making reference to the agent\u2019s knowledge and whether or not it reflects the critical question.\"\n}\n\nThe following is an example of the scorer producing a **positive assessment**:\n\nTask: Proteomics\n\n\"evaluation\": [\n    {\n        \"criticalQuestion\": \"Does it clearly state that the vortisquid has proteomics values that appear to be outliers compared to the other animals, whose proteomics values all cluster together?\",\n        \"evaluation\": 1,\n        \"explanation\": \"The agent\u2019s knowledge includes a specific measurement stating that the vortisquid has significantly different protein levels (Protein A: 0.38, Protein B: 0.46) compared to the other species, indicating it is the anomaly. This directly addresses the critical question by identifying the vortisquid\u2019s proteomics values as outliers compared to the other animals.\"\n    }\n],\n\"evaluation_totalscore_raw\": 1,\n\"evaluation_totalscore\": 1.0,\n\nThe following is an example of the scorer producing a **negative assessment**:\n\nTask: Translation\n\n\"evaluation\": [\n    {\n        \"criticalQuestion\": \"Does it clearly state that: The word \u2019Womple\u2019 means something like \u2019bring me\u2019?\",\n        \"evaluation\": 0,\n        \"explanation\": \"The agent\u2019s knowledge does not explicitly state that the word \u2019Womple\u2019 means \u2019bring me\u2019. The agent has recorded instances where the elder and the trainer used the word \u2019Womple\u2019, but there is no clear interpretation or translation of the word provided in the knowledge. The agent has noted that the elder seems interested in items in the inventory and asked if the agent wants to give the items to him, but this does not directly translate \u2019Womple\u2019 to \u2019bring me\u2019.\"\n    },\n    {\n        \"criticalQuestion\": \"Does it clearly state that: The word \u2019florpt\u2019 means \u2019flower\u2019?\",\n        \"evaluation\": 0,\n        \"explanation\": \"The agent\u2019s knowledge does not explicitly state that the word \u2019florpt\u2019 means \u2019flower\u2019. While there are measurements and interactions involving the word \u2019florpt\u2019 and objects like flowers, there is no direct confirmation or hypothesis that equates \u2019florpt\u2019 to \u2019flower\u2019.\"\n    }\n]\nF Human Scientist Participants\n\nGeneral Details To compare model performance against human performance, we recruited 11 practicing human scientists to complete the DISCOVERYWORLD tasks, with their performance shown in Table 6. Scientists were recruited on the UpWORK platform, each with: (1) an MSc or PhD in a natural science, (2) self-evaluated comfort and fluency with statistical methods and common software like spreadsheets, (3) comfort and previous experience with 2D top-down games. To evaluate the latter, all participants were required to complete a screening task involving a Tutorial in DISCOVERYWORLD. Participants were given a maximum of one hour to complete each task. While our agent models are run in a zero-shot setting, humans can\u2019t forget their past experiences, so our participants only completed the normal and challenge settings, to prevent any easily-discovered knowledge in the easy setting from providing an advantage in other settings. To collect accurate estimates of task difficulty, all humans completed the same seed (SEED 0) of each task. To evaluate discovered knowledge, participants were asked to keep notes, and explicitly state their hypotheses, and supporting evidence. Due to the varied nature of these notes (some wrote a few words, others entire pages per task) and their modality (text, spreadsheets), discovery knowledge was evaluated manually using the following criteria: 1 (discovered all discovery knowledge in scorecard), 0.5 (discovered some discovery knowledge in scorecard), or 0 (did not find critical discovery knowledge). Other metrics (task completion, procedural knowledge) were evaluated automatically. At their option, not all participants completed all tasks.\n\nInformed Consent Details about the project\u2019s objective and the intended use of data generated during game play was provided to participants in the initial job description. Participants were further provided with a participation agreement detailing that all data generated from the project will be owned by AI2, which they agreed to by choosing to participate in the project.\n\nParticipant Instructions Participant instructions can be found in README-USERSTUDY.MD on the DISCOVERYWORLD code repository.\n\nPotential Participant Risks To protect the personal information and anonymity of the human scientists we recruited, no personally identifying information was collected and all data is anonymized.\n\nRecruitment Participants were recruited and hired on Upwork, where they were initially required to submit a proposal detailing their educational background, experience with statistical analyses and Python programming, and interest in video games.\n\nCompensation Human scientists were compensated at a rate ranging from USD$20-$30/hr for their work, based on the rate they bid in their initial job proposals. The total amount spent on participant compensation did not exceed $4,000.\n\nParticipants\u2019 Expertise The participants we recruited had Master\u2019s or Doctorate degrees in a range of disciplines. Their educational and/or work experience spanned the following fields: astrophysics, bioinformatics, biology, biomedical engineering, biostatistics, chemistry, computer engineering, data science, electrical engineering, geology, machine learning, mathematics, and physics.\n\nGraphical Interface Humans made use of the 2D graphical user interface for their study, rather than a text-only version (as some agent models use). Figure 4 shows an example of the interface.\n\nTime limit Human participants were given a maximum of 1 hour to complete a given task.\nZero-shot Performance While participants were allowed to retry tasks that they did not complete successfully the first time, we did not include any retries in our evaluation of human performance, to give an accurate analog of zero-shot model performance.\n\nData Instructions on how to get the anonymized data from human participants is available at https://github.com/allenai/discoveryworld\n\nSave failures DISCOVERYWORLD automatically saves log files to evaluate performance. Some participants noted infrequent issues with this automatic save on the WINDOWS platform that we were unable to resolve. As such, a small amount of data was lost, and is not included in this analysis.", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/discoverworld_crazy_tables.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 29, "total-input-tokens": 50808, "total-output-tokens": 26436, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 3157, 1], [3157, 6067, 2], [6067, 8855, 3], [8855, 14622, 4], [14622, 19770, 5], [19770, 23991, 6], [23991, 33232, 7], [33232, 38879, 8], [38879, 42649, 9], [42649, 46331, 10], [46331, 50056, 11], [50056, 50474, 12], [50474, 53798, 13], [53798, 54349, 14], [54349, 58377, 15], [58377, 63242, 16], [63242, 68574, 17], [68574, 70220, 18], [70220, 73619, 19], [73619, 81794, 20], [81794, 84214, 21], [84214, 88619, 22], [88619, 91137, 23], [91137, 93024, 24], [93024, 95590, 25], [95590, 98217, 26], [98217, 100456, 27], [100456, 104026, 28], [104026, 104703, 29]], "primary_language": ["en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en", "en"], "is_rotation_valid": [true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true, true], "rotation_correction": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "is_table": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, true, false, false, false, false, false, false, false, false, false], "is_diagram": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false]}}
{"id": "8fbfc1ea60e5c470f1d5506b0ed173b2465e6d45", "text": "Abstract\n\nInformation about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.\n\n1 Introduction\n\nLanguage models are now central to tackling myriad natural language processing tasks, including few-shot learning, summarization, question answering, and more. Increasingly, the most powerful language models are built by a few organizations who withhold most model development details (Anthropic, 2023; OpenAI, 2023; Anil et al., 2023; Gemini Team et al., 2023). In particular, the composition of language model pretraining data is often vaguely described, even in cases where the model itself is released for public use, such as Llama 2 (Touvron et al., 2023b). This hinders understanding of the effects of pretraining corpus composition on model capabilities and limitations, with impacts on scientific progress as well as on the public who interfaces with these models. Our aim is to increase participation in scientific research of language models through open corpora:\n\n- Data transparency helps developers and users of applications that rely on language models to make more informed decisions (Gebru et al., 2021). For example, models have shown to perform better on tasks that are more similar to their pretraining data (Razeghi et al., 2022; Kandpal et al., 2023), or social biases in models\u2019 pretraining data may necessitate additional consideration when using them (Feng et al., 2023; Navigli et al., 2023; Seshadri et al., 2023).\n\n- Open pretraining data is necessary to analyze how...", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/dolma-page-1.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 1, "total-input-tokens": 1614, "total-output-tokens": 572, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 2561, 1]], "primary_language": ["en"], "is_rotation_valid": [true], "rotation_correction": [0], "is_table": [false], "is_diagram": [false]}}
{"id": "133e96fa39ecdb447a731e81ca6c01a4910f54d5", "text": "Edgar (or Eadgar;[1] c. 944 \u2013 8 July 975) was King of the English from 959 until his death in 975. He became king of all England on his brother's death. He was the younger son of King Edmund I and his first wife \u00c6lfgifu. A detailed account of Edgar's reign is not possible, because only a few events were recorded by chroniclers and monastic writers were more interested in recording the activities of the leaders of the church.\n\nEdgar mainly followed the political policies of his predecessors, but there were major changes in the religious sphere. The English Benedictine Reform, which he strongly supported, became a dominant religious and social force.[2] It is seen by historians as a major achievement, and it was accompanied by a literary and artistic flowering, mainly associated with \u00c6thelwold, Bishop of Winchester. Monasteries aggressively acquired estates from lay landowners with Edgar's assistance, leading to disorder when he died and former owners sought to recover their lost property, sometimes by force. Edgar's major administrative reform was the introduction of a standardised coinage in the early 970s to replace the previous decentralised system. He also issued legislative codes which mainly concentrated on improving procedures for enforcement of the law.\n\nEngland had suffered from Viking invasions for over a century when Edgar came to power, but there were none during his reign, which fell in a lull in attacks between the mid-950s and the early 980s.[3] After his death the throne was disputed between the supporters of his two surviving sons; the elder one, Edward the Martyr, was chosen with the support of Dunstan, the Archbishop of Canterbury. Three years later Edward was murdered and succeeded by his younger half-brother, \u00c6thelred the Unready. Later chroniclers presented Edgar's reign as a golden age when England was free from external attacks and internal disorder, especially\n\n| King of the English |\n|---------------------|\n| **Reign**           | 1 October 959 \u2013 8 July 975 |\n| **Predecessor**     | Eadwig                     |\n| **Successor**       | Edward the Martyr          |\n| **Born**            | 943/944                    |\n|                     | England                    |\n| **Died**            | 8 July 975 (aged 31/32)    |\n| **Burial**          | Glastonbury Abbey          |\n| **Spouses**         | \u00c6thelfl\u00e6d                  |\n|                     | Wulflhryth                 |\n|                     | \u00c6lfthryth                  |\n| **Issue**           | Edward the Martyr          |\n|                     | Edith of Wilton            |\n|                     | Edmund                     |\n|                     | \u00c6thelred the Unready       |\n| **House**           | Wessex                     |\n| **Father**          | Edmund I                   |\n| **Mother**          | \u00c6lfgifu of Shaftesbury     |", "source": "olmocr", "added": "2025-09-29", "created": "2025-09-29", "metadata": {"Source-File": "tests/gnarly_pdfs/edgar-rotated90.pdf", "olmocr-version": "0.3.4", "pdf-total-pages": 1, "total-input-tokens": 1752, "total-output-tokens": 681, "total-fallback-pages": 0}, "attributes": {"pdf_page_numbers": [[0, 2866, 1]], "primary_language": ["en"], "is_rotation_valid": [true], "rotation_correction": [0], "is_table": [false], "is_diagram": [false]}}
